"""
ë©”ì¸ í–¥ìˆ˜ AI API - ìš°ë¦¬ê°€ ë§Œë“  ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, Dict, Any
import uvicorn
import time
import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ìš°ë¦¬ê°€ ë§Œë“  ì‹¤ì œ í–¥ìˆ˜ AI ëª¨ë¸ import
from fragrance_ai.models.advanced_generator import AdvancedFragranceGenerator

app = FastAPI(
    title="Main Fragrance AI API",
    version="1.0.0",
    description="ìš°ë¦¬ê°€ ë§Œë“  ì‹¤ì œ í–¥ìˆ˜ AI ëª¨ë¸ ì‚¬ìš©"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì‹¤ì œ AI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
print("Loading Advanced Fragrance Generator...")
try:
    ai_generator = AdvancedFragranceGenerator()
    print("AI Model loaded successfully!")
    MODEL_LOADED = True
except Exception as e:
    print(f"Error loading AI model: {e}")
    MODEL_LOADED = False
    ai_generator = None

class ChatRequest(BaseModel):
    query: str
    context: Optional[str] = None
    temperature: float = 0.8
    enable_reasoning: bool = False

@app.get("/")
async def root():
    return {
        "message": "Main Fragrance AI API",
        "model_status": "AdvancedFragranceGenerator loaded" if MODEL_LOADED else "Model loading failed",
        "version": "1.0.0"
    }

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "model_loaded": MODEL_LOADED,
        "model_name": "AdvancedFragranceGenerator" if MODEL_LOADED else None
    }

@app.post("/api/v2/rag-chat")
async def chat(request: ChatRequest):
    """ì‹¤ì œ í–¥ìˆ˜ AI ëª¨ë¸ì„ ì‚¬ìš©í•œ ì±„íŒ…"""

    try:
        # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±
        if not MODEL_LOADED:
            return {
                "response": generate_simple_response(request.query),
                "confidence_score": 0.7,
                "model_used": "SimpleAI",
                "timestamp": time.time()
            }

        # ì‚¬ìš©ì ì¿¼ë¦¬ ë¶„ì„
        query_lower = request.query.lower()

        # ì¡°ê±´ ì¶”ì¶œ (ì¿¼ë¦¬ì—ì„œ ìë™ ë¶„ì„)
        conditions = extract_conditions_from_query(query_lower)

        try:
            # AI ëª¨ë¸ë¡œ ë ˆì‹œí”¼ ìƒì„±
            result = ai_generator.generate_recipe(
                prompt=request.query,
                conditions=conditions,
                temperature=request.temperature
            )

            # ì„±ëŠ¥ ì˜ˆì¸¡
            performance = ai_generator.predict_performance(conditions)

            # ì‘ë‹µ í¬ë§·íŒ…
            response_text = format_ai_response(result, performance)
        except Exception as model_error:
            print(f"Model generation error: {model_error}")
            # ëª¨ë¸ ì—ëŸ¬ ì‹œ ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±
            response_text = generate_simple_response(request.query)
            performance = None

        return {
            "response": response_text,
            "confidence_score": 0.85 if MODEL_LOADED else 0.7,
            "predicted_performance": performance if performance else {},
            "conditions_analyzed": conditions,
            "model_used": "AdvancedFragranceGenerator" if MODEL_LOADED else "SimpleAI",
            "timestamp": time.time()
        }

    except Exception as e:
        print(f"Error in chat endpoint: {e}")
        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
        return {
            "response": generate_simple_response(request.query),
            "confidence_score": 0.6,
            "model_used": "Fallback",
            "timestamp": time.time()
        }

def extract_conditions_from_query(query: str) -> Dict[str, str]:
    """ì¿¼ë¦¬ì—ì„œ ì¡°ê±´ ìë™ ì¶”ì¶œ"""
    conditions = {
        'weather': 'sunny',
        'season': 'spring',
        'time': 'afternoon',
        'mood': 'calm',
        'age_group': '20s',
        'gender': 'unisex',
        'intensity': 'moderate',
        'budget': 'mid_range'
    }

    # ê³„ì ˆ ê°ì§€
    if 'ì—¬ë¦„' in query or 'summer' in query:
        conditions['season'] = 'summer'
        conditions['weather'] = 'hot'
    elif 'ê²¨ìš¸' in query or 'winter' in query:
        conditions['season'] = 'winter'
        conditions['weather'] = 'cold'
    elif 'ë´„' in query or 'spring' in query:
        conditions['season'] = 'spring'
    elif 'ê°€ì„' in query or 'autumn' in query or 'fall' in query:
        conditions['season'] = 'autumn'

    # ì‹œê°„ ê°ì§€
    if 'ì•„ì¹¨' in query or 'morning' in query:
        conditions['time'] = 'morning'
    elif 'ì €ë…' in query or 'evening' in query:
        conditions['time'] = 'evening'
    elif 'ë°¤' in query or 'night' in query:
        conditions['time'] = 'night'

    # ë¬´ë“œ ê°ì§€
    if 'ë¡œë§¨í‹±' in query or 'romantic' in query:
        conditions['mood'] = 'romantic'
    elif 'í™œê¸°' in query or 'energetic' in query:
        conditions['mood'] = 'energetic'
    elif 'ì°¨ë¶„' in query or 'calm' in query:
        conditions['mood'] = 'calm'
    elif 'ì‹ ë¹„' in query or 'mysterious' in query:
        conditions['mood'] = 'mysterious'

    # ê°•ë„ ê°ì§€
    if 'ê°€ë²¼ìš´' in query or 'light' in query:
        conditions['intensity'] = 'light'
    elif 'ê°•í•œ' in query or 'strong' in query:
        conditions['intensity'] = 'strong'

    # ì„±ë³„ ê°ì§€
    if 'ì—¬ì„±' in query or 'female' in query or 'ì—¬ì' in query:
        conditions['gender'] = 'female'
    elif 'ë‚¨ì„±' in query or 'male' in query or 'ë‚¨ì' in query:
        conditions['gender'] = 'male'

    return conditions

def format_ai_response(result: Dict[str, Any], performance: Dict[str, float]) -> str:
    """AI ê²°ê³¼ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""

    response = result.get('generated_recipe', '')

    # ë ˆì‹œí”¼ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ ì‘ë‹µ ìƒì„±
    if not response:
        conditions = result.get('conditions_used', {})
        response = f"ì¡°ê±´ì— ë§ëŠ” í–¥ìˆ˜ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\n\n"
        response += "ğŸŒ¸ í–¥ìˆ˜ íŠ¹ì„±:\n"
        response += f"- ê³„ì ˆ: {conditions.get('season', 'all seasons')}\n"
        response += f"- ì‹œê°„ëŒ€: {conditions.get('time', 'anytime')}\n"
        response += f"- ë¶„ìœ„ê¸°: {conditions.get('mood', 'versatile')}\n"
        response += f"- ê°•ë„: {conditions.get('intensity', 'moderate')}\n"

    # ì„±ëŠ¥ ì •ë³´ ì¶”ê°€
    if performance:
        response += "\n\nğŸ“Š ì˜ˆìƒ ì„±ëŠ¥:\n"
        response += f"- ì§€ì†ë ¥: {performance.get('longevity', 0):.1f}/10\n"
        response += f"- í™•ì‚°ë ¥: {performance.get('sillage', 0):.1f}/10\n"
        response += f"- íˆ¬ì‚¬ë ¥: {performance.get('projection', 0):.1f}/10\n"
        response += f"- ë‚ ì”¨ ì €í•­ì„±: {performance.get('weather_resistance', 0):.1f}/10"

    return response

def generate_simple_response(query: str) -> str:
    """ê°„ë‹¨í•œ AI ì‘ë‹µ ìƒì„± (í´ë°±ìš©)"""
    query_lower = query.lower()

    if 'ì•ˆë…•' in query_lower or 'hello' in query_lower:
        return "ì•ˆë…•í•˜ì„¸ìš”! í–¥ìˆ˜ AIì…ë‹ˆë‹¤. ì–´ë–¤ í–¥ìˆ˜ë¥¼ ì°¾ê³  ê³„ì‹ ê°€ìš”?"

    elif 'ì¶”ì²œ' in query_lower:
        if 'ì—¬ë¦„' in query_lower:
            return "ì—¬ë¦„ì—ëŠ” ìƒì¾Œí•œ ì‹œíŠ¸ëŸ¬ìŠ¤ ê³„ì—´ í–¥ìˆ˜ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤. ë ˆëª¬, ë² ë¥´ê°€ëª», ìëª½ì´ ë“¤ì–´ê°„ í–¥ìˆ˜ê°€ ì¢‹ìŠµë‹ˆë‹¤."
        elif 'ê²¨ìš¸' in query_lower:
            return "ê²¨ìš¸ì—ëŠ” ë”°ëœ»í•œ ìš°ë””, ì˜¤ë¦¬ì—”íƒˆ ê³„ì—´ì„ ì¶”ì²œí•©ë‹ˆë‹¤. ìƒŒë‹¬ìš°ë“œ, ë°”ë‹ë¼, ì•°ë²„ê°€ í¬í•¨ëœ í–¥ìˆ˜ê°€ ì–´ìš¸ë¦½ë‹ˆë‹¤."
        else:
            return "ì–´ë–¤ ê³„ì ˆì´ë‚˜ ìƒí™©ì— ë§ëŠ” í–¥ìˆ˜ë¥¼ ì°¾ìœ¼ì‹œë‚˜ìš”? êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ë§ì¶¤ ì¶”ì²œì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."

    elif 'ë§Œë“¤' in query_lower or 'ë ˆì‹œí”¼' in query_lower:
        return "í–¥ìˆ˜ ì œì‘ì€ íƒ‘ë…¸íŠ¸(20-30%), ë¯¸ë“¤ë…¸íŠ¸(30-50%), ë² ì´ìŠ¤ë…¸íŠ¸(20-30%)ì˜ ê· í˜•ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì›í•˜ì‹œëŠ” í–¥ì˜ ìŠ¤íƒ€ì¼ì„ ë§ì”€í•´ì£¼ì„¸ìš”."

    else:
        return "í–¥ìˆ˜ì— ëŒ€í•œ ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì„¸ìš”. ì¶”ì²œ, ë…¸íŠ¸ ì„¤ëª…, ì¡°í•© ë°©ë²• ë“±ì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

@app.get("/api/v2/model-stats")
async def get_model_stats():
    """ëª¨ë¸ í†µê³„ ì •ë³´ ë°˜í™˜"""
    if not MODEL_LOADED:
        return {"error": "Model not loaded"}

    stats = ai_generator.get_model_stats()
    return {
        "model_name": "AdvancedFragranceGenerator",
        "statistics": stats,
        "timestamp": time.time()
    }

if __name__ == "__main__":
    print("Starting Main Fragrance AI Server...")
    print("This server uses our REAL AI model (AdvancedFragranceGenerator)")
    print("URL: http://localhost:8000")

    uvicorn.run(app, host="0.0.0.0", port=8000, reload=False)