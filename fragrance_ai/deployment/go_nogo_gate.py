"""
Go / No-Go ë°°í¬ ê²Œì´íŠ¸
ë°°í¬ ì „ ìë™ ì²´í¬ë¡œ ì•ˆì „ì„± ë³´ì¥
"""

import logging
import subprocess
import json
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
import time

logger = logging.getLogger(__name__)


class DeploymentDecision(Enum):
    """ë°°í¬ ê²°ì •"""
    GO = "GO"
    NO_GO = "NO_GO"


@dataclass
class GoNoGoResult:
    """Go/No-Go ê²°ê³¼"""
    decision: DeploymentDecision
    reasons: List[str]
    metrics: Dict[str, any]
    timestamp: str


# =============================================================================
# 1. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘
# =============================================================================

class TestRunner:
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""

    def __init__(self, test_suites: List[str]):
        self.test_suites = test_suites

    def run_all_tests(self) -> Tuple[bool, Dict[str, any]]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ§ª Running all test suites...")

        all_passed = True
        results = {}

        for suite in self.test_suites:
            passed, stats = self._run_test_suite(suite)
            results[suite] = stats

            if not passed:
                all_passed = False
                logger.error(f"âŒ Test suite failed: {suite}")
            else:
                logger.info(f"âœ“ Test suite passed: {suite}")

        return all_passed, results

    def _run_test_suite(self, suite: str) -> Tuple[bool, Dict[str, any]]:
        """ê°œë³„ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"""
        try:
            # pytest ì‹¤í–‰
            result = subprocess.run(
                ["pytest", suite, "-v", "--tb=short", "--json-report", "--json-report-file=.test_report.json"],
                capture_output=True,
                text=True,
                timeout=300
            )

            # ê²°ê³¼ íŒŒì‹±
            report_path = Path(".test_report.json")
            if report_path.exists():
                with open(report_path, 'r', encoding='utf-8') as f:
                    report = json.load(f)

                stats = {
                    "passed": report.get("summary", {}).get("passed", 0),
                    "failed": report.get("summary", {}).get("failed", 0),
                    "total": report.get("summary", {}).get("total", 0),
                    "duration": report.get("duration", 0.0)
                }

                report_path.unlink()  # Clean up
                return stats["failed"] == 0, stats

            # Fallback: return code ê¸°ë°˜
            return result.returncode == 0, {
                "passed": "unknown",
                "failed": 0 if result.returncode == 0 else 1,
                "total": 1,
                "duration": 0.0
            }

        except subprocess.TimeoutExpired:
            logger.error(f"Test suite timed out: {suite}")
            return False, {"failed": 1, "reason": "timeout"}
        except Exception as e:
            logger.error(f"Test suite error: {suite} - {e}")
            return False, {"failed": 1, "reason": str(e)}


# =============================================================================
# 2. KPI ë©”íŠ¸ë¦­ ì²´í¬
# =============================================================================

@dataclass
class KPIThresholds:
    """KPI ì„ê³„ê°’"""

    # LLM ë©”íŠ¸ë¦­
    llm_p95_fast: float = 2.5  # seconds
    llm_p95_balanced: float = 3.2  # seconds
    llm_p95_creative: float = 4.5  # seconds

    # API ë©”íŠ¸ë¦­
    api_p95_latency: float = 2.5  # seconds
    api_p99_latency: float = 5.0  # seconds
    api_error_rate: float = 0.01  # 1%

    # ìŠ¤í‚¤ë§ˆ ê²€ì¦
    schema_failure_rate: float = 0.0  # 0% (ë¬´ê´€ìš©)

    # RL ë©”íŠ¸ë¦­
    rl_reward_min: float = 10.0
    rl_kl_divergence_max: float = 0.03


class KPIChecker:
    """KPI ì²´í¬"""

    def __init__(self, thresholds: KPIThresholds):
        self.thresholds = thresholds

    def check_all_kpis(self, metrics: Dict[str, float]) -> Tuple[bool, List[str]]:
        """ëª¨ë“  KPI ì²´í¬"""
        logger.info("ğŸ“Š Checking KPIs...")

        violations = []

        # LLM p95 ì²´í¬
        if metrics.get("llm_p95_fast", 0) > self.thresholds.llm_p95_fast:
            violations.append(
                f"LLM p95 (fast) exceeded: {metrics['llm_p95_fast']:.2f}s > {self.thresholds.llm_p95_fast}s"
            )

        if metrics.get("llm_p95_balanced", 0) > self.thresholds.llm_p95_balanced:
            violations.append(
                f"LLM p95 (balanced) exceeded: {metrics['llm_p95_balanced']:.2f}s > {self.thresholds.llm_p95_balanced}s"
            )

        if metrics.get("llm_p95_creative", 0) > self.thresholds.llm_p95_creative:
            violations.append(
                f"LLM p95 (creative) exceeded: {metrics['llm_p95_creative']:.2f}s > {self.thresholds.llm_p95_creative}s"
            )

        # API ì§€ì—° ì²´í¬
        if metrics.get("api_p95_latency", 0) > self.thresholds.api_p95_latency:
            violations.append(
                f"API p95 latency exceeded: {metrics['api_p95_latency']:.2f}s > {self.thresholds.api_p95_latency}s"
            )

        if metrics.get("api_p99_latency", 0) > self.thresholds.api_p99_latency:
            violations.append(
                f"API p99 latency exceeded: {metrics['api_p99_latency']:.2f}s > {self.thresholds.api_p99_latency}s"
            )

        # API ì—ëŸ¬ìœ¨ ì²´í¬
        if metrics.get("api_error_rate", 0) > self.thresholds.api_error_rate:
            violations.append(
                f"API error rate exceeded: {metrics['api_error_rate']:.2%} > {self.thresholds.api_error_rate:.2%}"
            )

        # ìŠ¤í‚¤ë§ˆ ì‹¤íŒ¨ìœ¨ ì²´í¬ (ë¬´ê´€ìš©)
        if metrics.get("schema_failure_rate", 0) > self.thresholds.schema_failure_rate:
            violations.append(
                f"ğŸš¨ Schema failure rate > 0%: {metrics['schema_failure_rate']:.2%} (ZERO TOLERANCE)"
            )

        # RL ë©”íŠ¸ë¦­ ì²´í¬
        if metrics.get("rl_reward", 0) < self.thresholds.rl_reward_min:
            violations.append(
                f"RL reward too low: {metrics['rl_reward']:.2f} < {self.thresholds.rl_reward_min}"
            )

        if metrics.get("rl_kl_divergence", 0) > self.thresholds.rl_kl_divergence_max:
            violations.append(
                f"RL KL divergence too high: {metrics['rl_kl_divergence']:.4f} > {self.thresholds.rl_kl_divergence_max}"
            )

        if violations:
            for v in violations:
                logger.error(f"âŒ KPI violation: {v}")
            return False, violations
        else:
            logger.info("âœ“ All KPIs passed")
            return True, []


# =============================================================================
# 3. ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (Prometheus ì¿¼ë¦¬)
# =============================================================================

class MetricsCollector:
    """Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""

    def __init__(self, prometheus_url: str = "http://localhost:9090"):
        self.prometheus_url = prometheus_url

    def collect_metrics(self) -> Dict[str, float]:
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        logger.info("ğŸ“ˆ Collecting metrics from Prometheus...")

        metrics = {}

        # LLM p95 ì§€ì—° (fast/balanced/creative)
        metrics["llm_p95_fast"] = self._query_prometheus(
            'histogram_quantile(0.95, rate(llm_brief_latency_seconds_bucket{mode="fast"}[5m]))'
        )
        metrics["llm_p95_balanced"] = self._query_prometheus(
            'histogram_quantile(0.95, rate(llm_brief_latency_seconds_bucket{mode="balanced"}[5m]))'
        )
        metrics["llm_p95_creative"] = self._query_prometheus(
            'histogram_quantile(0.95, rate(llm_brief_latency_seconds_bucket{mode="creative"}[5m]))'
        )

        # API ì§€ì—°
        metrics["api_p95_latency"] = self._query_prometheus(
            'histogram_quantile(0.95, rate(api_request_latency_seconds_bucket[5m]))'
        )
        metrics["api_p99_latency"] = self._query_prometheus(
            'histogram_quantile(0.99, rate(api_request_latency_seconds_bucket[5m]))'
        )

        # API ì—ëŸ¬ìœ¨
        total_requests = self._query_prometheus('sum(rate(api_request_total[5m]))')
        error_requests = self._query_prometheus('sum(rate(api_request_total{status=~"4xx|5xx"}[5m]))')
        metrics["api_error_rate"] = error_requests / total_requests if total_requests > 0 else 0

        # ìŠ¤í‚¤ë§ˆ ì‹¤íŒ¨ìœ¨
        total_briefs = self._query_prometheus('sum(rate(llm_brief_total[5m]))')
        failed_briefs = self._query_prometheus('sum(rate(llm_brief_total{status="failure"}[5m]))')
        metrics["schema_failure_rate"] = failed_briefs / total_briefs if total_briefs > 0 else 0

        # RL ë©”íŠ¸ë¦­
        metrics["rl_reward"] = self._query_prometheus('rl_reward')
        metrics["rl_kl_divergence"] = self._query_prometheus('rl_kl_divergence')

        return metrics

    def _query_prometheus(self, query: str) -> float:
        """Prometheus ì¿¼ë¦¬ (ì‹¤ì œ êµ¬í˜„ ì‹œ requests ì‚¬ìš©)"""
        # Mock implementation - ì‹¤ì œë¡œëŠ” requests.get() ì‚¬ìš©
        try:
            import requests
            response = requests.get(
                f"{self.prometheus_url}/api/v1/query",
                params={"query": query},
                timeout=5
            )
            data = response.json()
            if data.get("status") == "success":
                result = data.get("data", {}).get("result", [])
                if result:
                    return float(result[0]["value"][1])
        except Exception as e:
            logger.debug(f"Prometheus query failed: {query} - {e}")

        # Fallback to mock data
        return 0.0


# =============================================================================
# 4. Go / No-Go ê²Œì´íŠ¸
# =============================================================================

class GoNoGoGate:
    """Go / No-Go ë°°í¬ ê²Œì´íŠ¸"""

    def __init__(
        self,
        test_suites: Optional[List[str]] = None,
        kpi_thresholds: Optional[KPIThresholds] = None,
        prometheus_url: str = "http://localhost:9090"
    ):
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸
        if test_suites is None:
            test_suites = [
                "tests/test_llm_ensemble_operation.py",
                "tests/test_ga.py",
                "tests/test_ifra.py",
                "tests/test_end_to_end_evolution.py"
            ]

        self.test_runner = TestRunner(test_suites)
        self.kpi_checker = KPIChecker(kpi_thresholds or KPIThresholds())
        self.metrics_collector = MetricsCollector(prometheus_url)

    def evaluate(self) -> GoNoGoResult:
        """Go / No-Go í‰ê°€"""
        logger.info("=" * 60)
        logger.info("ğŸš¦ GO / NO-GO DEPLOYMENT GATE")
        logger.info("=" * 60)

        reasons = []
        metrics = {}
        decision = DeploymentDecision.GO

        # 1. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        tests_passed, test_results = self.test_runner.run_all_tests()
        metrics["test_results"] = test_results

        if not tests_passed:
            decision = DeploymentDecision.NO_GO
            reasons.append("âŒ Tests failed")
            logger.error("NO-GO: Tests failed")
        else:
            logger.info("âœ“ All tests passed")

        # 2. KPI ì²´í¬
        current_metrics = self.metrics_collector.collect_metrics()
        metrics.update(current_metrics)

        kpis_passed, kpi_violations = self.kpi_checker.check_all_kpis(current_metrics)

        if not kpis_passed:
            decision = DeploymentDecision.NO_GO
            reasons.extend(kpi_violations)
            logger.error("NO-GO: KPI violations detected")
        else:
            logger.info("âœ“ All KPIs passed")

        # 3. íŠ¹ë³„ ì²´í¬: ìŠ¤í‚¤ë§ˆ ì‹¤íŒ¨ìœ¨ ë¬´ê´€ìš©
        if current_metrics.get("schema_failure_rate", 0) > 0:
            decision = DeploymentDecision.NO_GO
            reasons.append("ğŸš¨ CRITICAL: Schema failure rate > 0% (ZERO TOLERANCE)")
            logger.error("NO-GO: Schema failures detected (zero tolerance)")

        # 4. íŠ¹ë³„ ì²´í¬: p95 ì§€ì† ì´ˆê³¼
        p95_violations = [
            r for r in reasons
            if "p95" in r.lower() or "p99" in r.lower()
        ]
        if p95_violations:
            decision = DeploymentDecision.NO_GO
            logger.error("NO-GO: p95/p99 thresholds exceeded")

        # ê²°ê³¼ ìš”ì•½
        logger.info("=" * 60)
        if decision == DeploymentDecision.GO:
            logger.info("âœ… DECISION: GO - Safe to deploy")
        else:
            logger.error("â›” DECISION: NO-GO - Do not deploy")
            logger.error("Reasons:")
            for reason in reasons:
                logger.error(f"  - {reason}")

        logger.info("=" * 60)

        return GoNoGoResult(
            decision=decision,
            reasons=reasons,
            metrics=metrics,
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
        )

    def generate_report(self, result: GoNoGoResult) -> str:
        """ë°°í¬ ê²Œì´íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 60)
        report.append("GO / NO-GO DEPLOYMENT GATE REPORT")
        report.append("=" * 60)
        report.append(f"Timestamp: {result.timestamp}")
        report.append(f"Decision: {result.decision.value}")
        report.append("")

        if result.decision == DeploymentDecision.GO:
            report.append("âœ… SAFE TO DEPLOY")
            report.append("")
            report.append("All checks passed:")
            report.append("  âœ“ Tests: PASSED")
            report.append("  âœ“ KPIs: PASSED")
            report.append("  âœ“ Schema: PASSED (0% failure)")
            report.append("  âœ“ Latency: PASSED (p95/p99 within limits)")
        else:
            report.append("â›” DO NOT DEPLOY")
            report.append("")
            report.append("Failures detected:")
            for i, reason in enumerate(result.reasons, 1):
                report.append(f"  {i}. {reason}")

        report.append("")
        report.append("Metrics:")
        for key, value in result.metrics.items():
            if isinstance(value, dict):
                report.append(f"  {key}:")
                for k, v in value.items():
                    report.append(f"    {k}: {v}")
            else:
                report.append(f"  {key}: {value}")

        report.append("=" * 60)

        return "\n".join(report)


# =============================================================================
# CLI ì‹¤í–‰
# =============================================================================

def main():
    """CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸"""
    import argparse

    parser = argparse.ArgumentParser(description="Go / No-Go Deployment Gate")
    parser.add_argument(
        "--prometheus-url",
        default="http://localhost:9090",
        help="Prometheus server URL"
    )
    parser.add_argument(
        "--report-file",
        help="Save report to file"
    )
    parser.add_argument(
        "--exit-code",
        action="store_true",
        help="Exit with code 1 on NO-GO"
    )

    args = parser.parse_args()

    # Logging ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    # Go/No-Go ê²Œì´íŠ¸ ì‹¤í–‰
    gate = GoNoGoGate(prometheus_url=args.prometheus_url)
    result = gate.evaluate()

    # ë¦¬í¬íŠ¸ ìƒì„±
    report = gate.generate_report(result)
    print(report)

    # íŒŒì¼ ì €ì¥
    if args.report_file:
        with open(args.report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        logger.info(f"Report saved to: {args.report_file}")

    # Exit code
    if args.exit_code and result.decision == DeploymentDecision.NO_GO:
        return 1

    return 0


# =============================================================================
# ì‚¬ìš© ì˜ˆì‹œ
# =============================================================================

if __name__ == "__main__":
    import sys
    sys.exit(main())
