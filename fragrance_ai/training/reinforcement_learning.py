"""
'ì§„í™”' ì—”ì§„: EpigeneticVariationAI
PyTorchë¥¼ ì‚¬ìš©í•œ ê°•í™”í•™ìŠµ(RLHF) ëª¨ë¸ êµ¬í˜„
ëª©í‘œ: ì‚¬ìš©ìì˜ ì£¼ê´€ì ì¸ ì„ íƒ(í”¼ë“œë°±)ì„ 'ë³´ìƒ'ìœ¼ë¡œ ì‚¼ì•„, ì–´ë–¤ ì¢…ë¥˜ì˜ 'ë³€í˜•'ì´ ì‚¬ìš©ìë¥¼ ë§Œì¡±ì‹œí‚¤ëŠ”ì§€ í•™ìŠµ
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import json
from collections import deque
import random
from datetime import datetime
import logging

# í”„ë¡œì íŠ¸ ë‚´ë¶€ ëª¨ë“ˆ imports
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

# OlfactoryDNAì™€ CreativeBrief import
from fragrance_ai.training.moga_optimizer import OlfactoryDNA, CreativeBrief

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ScentPhenotype:
    """í–¥ìˆ˜ í‘œí˜„í˜• - ì‚¬ìš©ìì—ê²Œ ì œì‹œë  ë³€í˜•ëœ í–¥ìˆ˜"""
    dna: OlfactoryDNA
    variation_applied: str  # ì ìš©ëœ ë³€í˜• ì¢…ë¥˜
    user_rating: Optional[float] = None  # ì‚¬ìš©ì í‰ê°€ (1-10)


class PolicyNetwork(nn.Module):
    """
    1ë‹¨ê³„: ì •ì±… ì‹ ê²½ë§(Policy Network) ëª¨ë¸ ì •ì˜
    torch.nn.Moduleì„ ìƒì†ë°›ëŠ” ê°„ë‹¨í•œ MLP(Multi-Layer Perceptron) ëª¨ë¸

    ì…ë ¥(State): í˜„ì¬ í–¥ìˆ˜ì˜ OlfactoryDNA ë²¡í„°ì™€ ì‚¬ìš©ìì˜ í”¼ë“œë°±(CreativeBrief) ë²¡í„°ë¥¼ í•©ì¹œ ë²¡í„°
    ì¶œë ¥(Action): ê°€ëŠ¥í•œ ëª¨ë“  'ë³€í˜•' ë°©ë²•ì— ëŒ€í•œ í™•ë¥  ë¶„í¬
    """

    def __init__(self, input_dim: int = 100, hidden_dim: int = 256):
        super(PolicyNetwork, self).__init__()

        # MLP ë ˆì´ì–´ë“¤
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 128)

        # ì¶œë ¥ ë ˆì´ì–´ - ë³€í˜• ë°©ë²•ì— ëŒ€í•œ í™•ë¥ 
        # ë³€í˜•ì˜ ì¢…ë¥˜: ['Amplify_Note_A', 'Silence_Note_B', 'Add_New_Note_C', ...]
        self.action_head = nn.Linear(128, 30)  # 10ê°œ ë…¸íŠ¸ x 3ê°œ í–‰ë™ = 30ê°œ ì•¡ì…˜

        self.dropout = nn.Dropout(0.1)

    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """
        ìˆœë°©í–¥ ì „íŒŒ
        ë§ˆì§€ë§‰ ë ˆì´ì–´ëŠ” Softmax í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í–‰ë™ì— ëŒ€í•œ í™•ë¥  ê°’ì„ ì¶œë ¥
        """
        x = F.relu(self.fc1(state))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = F.relu(self.fc3(x))

        # Softmaxë¥¼ í†µí•œ í™•ë¥  ë¶„í¬ ìƒì„±
        action_probs = F.softmax(self.action_head(x), dim=-1)

        return action_probs


class EpigeneticVariationAI:
    """
    ì§„í™” ì—”ì§„: ì‚¬ìš©ì í”¼ë“œë°±ì„ í†µí•´ í•™ìŠµí•˜ëŠ” ê°•í™”í•™ìŠµ ëª¨ë¸
    REINFORCE ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ ì •ì±… ê²½ì‚¬(Policy Gradient) êµ¬í˜„
    """

    def __init__(self,
                 state_dim: int = 100,
                 learning_rate: float = 0.001,
                 gamma: float = 0.99):

        self.state_dim = state_dim
        self.learning_rate = learning_rate
        self.gamma = gamma  # í• ì¸ìœ¨

        # ì •ì±… ì‹ ê²½ë§ ì´ˆê¸°í™”
        self.policy_network = PolicyNetwork(input_dim=state_dim)
        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)

        # ê²½í—˜ ë²„í¼ (ì—í”¼ì†Œë“œ ê¸°ë¡)
        self.episode_log_probs = []
        self.episode_rewards = []

        # ë³€í˜• ì•¡ì…˜ ì •ì˜
        self.action_space = self._define_action_space()

        # í•™ìŠµ íˆìŠ¤í† ë¦¬
        self.training_history = []

    def _define_action_space(self) -> List[str]:
        """
        ê°€ëŠ¥í•œ ë³€í˜• í–‰ë™ë“¤ ì •ì˜
        ì¶œë ¥(Action): ê°€ëŠ¥í•œ ëª¨ë“  'ë³€í˜•' ë°©ë²•
        ['Amplify_Note_A', 'Silence_Note_B', 'Add_New_Note_C', ...]
        """
        actions = []
        note_names = ['Bergamot', 'Lemon', 'Rose', 'Jasmine', 'Sandalwood',
                     'Cedar', 'Vanilla', 'Musk', 'Amber', 'Patchouli']

        for note in note_names:
            actions.append(f"Amplify_{note}")
            actions.append(f"Silence_{note}")
            actions.append(f"Add_{note}")

        return actions

    def encode_state(self, dna: OlfactoryDNA, brief: CreativeBrief) -> torch.Tensor:
        """
        ì…ë ¥(State): í˜„ì¬ í–¥ìˆ˜ì˜ OlfactoryDNA ë²¡í„°ì™€ ì‚¬ìš©ìì˜ í”¼ë“œë°±(CreativeBrief) ë²¡í„°ë¥¼ í•©ì¹œ ë²¡í„°
        """
        # DNA ì¸ì½”ë”© (ë…¸íŠ¸ì™€ ë†ë„)
        dna_vector = np.zeros(50)  # 10 ë…¸íŠ¸ x 5 íŠ¹ì§•
        for i, (note_id, percentage) in enumerate(dna.genes[:10]):
            if i < 10:
                dna_vector[i*5] = note_id / 10.0  # ì •ê·œí™”
                dna_vector[i*5 + 1] = percentage / 30.0  # ì •ê·œí™”
                dna_vector[i*5 + 2] = dna.fitness_scores[0] if dna.fitness_scores else 0
                dna_vector[i*5 + 3] = dna.fitness_scores[1] if dna.fitness_scores else 0
                dna_vector[i*5 + 4] = dna.fitness_scores[2] if dna.fitness_scores else 0

        # CreativeBrief ì¸ì½”ë”©
        brief_vector = np.zeros(50)
        brief_vector[:3] = brief.emotional_palette[:3]
        brief_vector[3] = brief.intensity
        # ì¶”ê°€ íŠ¹ì§•ë“¤ì„ ì¸ì½”ë”©í•  ìˆ˜ ìˆìŒ

        # ìƒíƒœ ë²¡í„° í•©ì¹˜ê¸° (concatenate)
        state_vector = np.concatenate([dna_vector, brief_vector])

        return torch.FloatTensor(state_vector).unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€

    def sample_action(self, state: torch.Tensor) -> Tuple[int, torch.Tensor]:
        """
        2ë‹¨ê³„: í–‰ë™ ì‹¤í–‰ ë° ë³´ìƒ íšë“
        a. í–‰ë™ ìƒ˜í”Œë§: í˜„ì¬ Stateë¥¼ ì •ì±… ì‹ ê²½ë§ì— ì…ë ¥í•˜ì—¬,
        ì¶œë ¥ëœ í™•ë¥  ë¶„í¬ì— ë”°ë¼ ì—¬ëŸ¬ ê°œì˜ 'ë³€í˜•' í–‰ë™(Action)ì„ ìƒ˜í”Œë§
        """
        # ì •ì±… ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•œ í™•ë¥  ë¶„í¬ ê³„ì‚°
        action_probs = self.policy_network(state)

        # í™•ë¥  ë¶„í¬ì— ë”°ë¼ í–‰ë™ ìƒ˜í”Œë§
        action_distribution = torch.distributions.Categorical(action_probs)
        action = action_distribution.sample()

        # ë¡œê·¸ í™•ë¥  ê³„ì‚° (ë‚˜ì¤‘ì— í•™ìŠµì— ì‚¬ìš©)
        log_prob = action_distribution.log_prob(action)

        return action.item(), log_prob

    def apply_variation(self, dna: OlfactoryDNA, action_idx: int) -> OlfactoryDNA:
        """
        ìƒ˜í”Œë§ëœ í–‰ë™ì„ DNAì— ì ìš©í•˜ì—¬ ë³€í˜•ëœ DNA ìƒì„±
        """
        # í–‰ë™ í•´ì„
        action_name = self.action_space[action_idx]
        action_type, note_name = action_name.split('_', 1)

        # DNA ë³µì‚¬
        new_genes = list(dna.genes)

        # ë…¸íŠ¸ ID ë§¤í•‘ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
        note_mapping = {
            'Bergamot': 1, 'Lemon': 2, 'Rose': 3, 'Jasmine': 4,
            'Sandalwood': 5, 'Cedar': 6, 'Vanilla': 7, 'Musk': 8,
            'Amber': 9, 'Patchouli': 10
        }

        note_id = note_mapping.get(note_name, 1)

        if action_type == "Amplify":
            # í•´ë‹¹ ë…¸íŠ¸ì˜ ë†ë„ ì¦ê°€
            for i, (nid, percentage) in enumerate(new_genes):
                if nid == note_id:
                    new_genes[i] = (nid, min(percentage * 1.5, 30.0))
                    break

        elif action_type == "Silence":
            # í•´ë‹¹ ë…¸íŠ¸ ì œê±° ë˜ëŠ” ê°ì†Œ
            for i, (nid, percentage) in enumerate(new_genes):
                if nid == note_id:
                    new_genes[i] = (nid, percentage * 0.3)
                    break

        elif action_type == "Add":
            # ìƒˆë¡œìš´ ë…¸íŠ¸ ì¶”ê°€ (ë¹ˆ ìŠ¬ë¡¯ì´ ìˆìœ¼ë©´)
            added = False
            for i, (nid, percentage) in enumerate(new_genes):
                if percentage < 0.1:  # ê±°ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ìŠ¬ë¡¯
                    new_genes[i] = (note_id, random.uniform(1.0, 5.0))
                    added = True
                    break

            if not added and len(new_genes) < 15:
                # ìŠ¬ë¡¯ ì¶”ê°€
                new_genes.append((note_id, random.uniform(1.0, 5.0)))

        # ìƒˆë¡œìš´ DNA ê°ì²´ ìƒì„±
        return OlfactoryDNA(
            genes=new_genes,
            fitness_scores=dna.fitness_scores  # ì¼ë‹¨ ë™ì¼í•˜ê²Œ ìœ ì§€
        )

    def generate_variations(self, dna: OlfactoryDNA, brief: CreativeBrief, num_variations: int = 3) -> List[ScentPhenotype]:
        """
        b. ì‚¬ìš©ìì—ê²Œ ì œì‹œ: ì´ í–‰ë™ë“¤ì„ ì ìš©í•˜ì—¬ ìƒì„±ëœ ì—¬ëŸ¬ ê°œì˜ ScentPhenotype í›„ë³´ A, B, Cë¥¼ ìƒì„±
        """
        variations = []
        state = self.encode_state(dna, brief)

        for _ in range(num_variations):
            # í–‰ë™ ìƒ˜í”Œë§
            action_idx, log_prob = self.sample_action(state)

            # ë³€í˜• ì ìš©
            varied_dna = self.apply_variation(dna, action_idx)

            # ScentPhenotype ìƒì„±
            phenotype = ScentPhenotype(
                dna=varied_dna,
                variation_applied=self.action_space[action_idx]
            )

            variations.append(phenotype)

            # ë¡œê·¸ í™•ë¥  ì €ì¥ (í•™ìŠµìš©)
            self.episode_log_probs.append(log_prob)

        return variations

    def update_policy_with_feedback(self, variations: List[ScentPhenotype], selected_idx: int):
        """
        3ë‹¨ê³„: ì •ì±… ì—…ë°ì´íŠ¸ (í•™ìŠµ)
        REINFORCE ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì •ì±… ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸

        c. ë³´ìƒ ì •ì˜: ì‚¬ìš©ìê°€ í›„ë³´ Bë¥¼ ì„ íƒí•˜ë©´, í–‰ë™ Bì— ëŒ€í•œ ë³´ìƒ(reward)ì€ +1,
        ì„ íƒë°›ì§€ ëª»í•œ Aì™€ Cì— ëŒ€í•œ ë³´ìƒì€ -1 (ë˜ëŠ” 0)ë¡œ ì„¤ì •
        """

        # ë³´ìƒ ì„¤ì •
        rewards = []
        for i, phenotype in enumerate(variations):
            if i == selected_idx:
                # ì„ íƒëœ ë³€í˜•ì— ëŒ€í•œ ê¸ì •ì  ë³´ìƒ
                reward = 1.0
                logger.info(f"âœ¨ ì‚¬ìš©ìê°€ ì„ íƒí•œ ë³€í˜•: {phenotype.variation_applied}")
            else:
                # ì„ íƒë˜ì§€ ì•Šì€ ë³€í˜•ì— ëŒ€í•œ ë¶€ì •ì /ì¤‘ë¦½ì  ë³´ìƒ
                reward = -0.5  # ë˜ëŠ” 0

            rewards.append(reward)
            phenotype.user_rating = reward  # ê¸°ë¡ìš©

        self.episode_rewards.extend(rewards)

        # REINFORCE ì•Œê³ ë¦¬ì¦˜ ì ìš©
        self._update_policy()

    def _update_policy(self):
        """
        REINFORCE ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ ì •ì±… ì—…ë°ì´íŠ¸
        í•µì‹¬ ìˆ˜ì‹: Loss = -log(P(action_chosen)) * reward

        - P(action_chosen): ì •ì±… ì‹ ê²½ë§ì´ 'ì‚¬ìš©ìê°€ ì„ íƒí•œ í–‰ë™'ì„ ì˜ˆì¸¡í–ˆë˜ í™•ë¥ 
        - reward: ìœ„ì—ì„œ ì •ì˜í•œ ë³´ìƒ ê°’(+1 ë˜ëŠ” -0.5)

        ë§Œì•½ ë³´ìƒì´ ê¸ì •ì (+1)ì´ë©´, ì†ì‹¤ í•¨ìˆ˜ëŠ” log(P(action_chosen))ë¥¼ ìµœëŒ€í™”
        ë§Œì•½ ë³´ìƒì´ ë¶€ì •ì (-1)ì´ë©´, ì†ì‹¤ í•¨ìˆ˜ëŠ” log(P(action_chosen))ë¥¼ ìµœì†Œí™”
        """

        if len(self.episode_rewards) == 0:
            return

        # ë¦¬í„´ ê³„ì‚° (í• ì¸ëœ ëˆ„ì  ë³´ìƒ)
        returns = []
        G = 0
        for reward in reversed(self.episode_rewards):
            G = reward + self.gamma * G
            returns.insert(0, G)

        returns = torch.FloatTensor(returns)

        # ì •ê·œí™” (ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•´)
        if len(returns) > 1:
            returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        # ì •ì±… ê²½ì‚¬ ì†ì‹¤ ê³„ì‚°
        policy_loss = []
        for log_prob, G in zip(self.episode_log_probs, returns):
            # Loss = -log(P(action)) * G
            # ì—¬ê¸°ì„œ GëŠ” í•´ë‹¹ í–‰ë™ì˜ ë¦¬í„´(í• ì¸ëœ ëˆ„ì  ë³´ìƒ)
            policy_loss.append(-log_prob * G)

        # ì „ì²´ ì†ì‹¤
        loss = torch.stack(policy_loss).sum()

        # ì—­ì „íŒŒ ë° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # í•™ìŠµ ê¸°ë¡
        self.training_history.append({
            'loss': loss.item(),
            'mean_reward': np.mean(self.episode_rewards),
            'timestamp': datetime.now().isoformat()
        })

        logger.info(f"ğŸ“ˆ ì •ì±… ì—…ë°ì´íŠ¸ ì™„ë£Œ: Loss={loss.item():.4f}, "
                   f"í‰ê·  ë³´ìƒ={np.mean(self.episode_rewards):.2f}")

        # ì—í”¼ì†Œë“œ ë²„í¼ ì´ˆê¸°í™”
        self.episode_log_probs = []
        self.episode_rewards = []

    def evolve_with_feedback(self,
                            initial_dna: OlfactoryDNA,
                            brief: CreativeBrief,
                            num_iterations: int = 10) -> OlfactoryDNA:
        """
        ì‚¬ìš©ì í”¼ë“œë°±ì„ í†µí•œ ì§„í™” ì‹œë®¬ë ˆì´ì…˜
        ì‹¤ì œë¡œëŠ” ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ì™€ ì—°ë™ë˜ì–´ì•¼ í•¨
        """

        logger.info("ğŸ§¬ ì§„í™” ì—”ì§„ ì‹œì‘: ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ í–¥ìˆ˜ ì§„í™”")

        current_dna = initial_dna

        for iteration in range(num_iterations):
            logger.info(f"\nğŸ“ ì§„í™” ë¼ìš´ë“œ {iteration + 1}/{num_iterations}")

            # ë³€í˜• ìƒì„±
            variations = self.generate_variations(current_dna, brief, num_variations=3)

            # ì‚¬ìš©ì ì„ íƒ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” UIì—ì„œ ë°›ì•„ì•¼ í•¨)
            # ì—¬ê¸°ì„œëŠ” ëœë¤í•˜ê²Œ ì„ íƒ (ì‹¤ì œ êµ¬í˜„ì‹œ ì‚¬ìš©ì ì…ë ¥ í•„ìš”)
            selected_idx = random.randint(0, len(variations) - 1)

            # ì •ì±… ì—…ë°ì´íŠ¸
            self.update_policy_with_feedback(variations, selected_idx)

            # ì„ íƒëœ ë³€í˜•ì„ ìƒˆë¡œìš´ í˜„ì¬ DNAë¡œ ì„¤ì •
            current_dna = variations[selected_idx].dna

        logger.info("âœ¨ ì§„í™” ì™„ë£Œ! ìµœì¢… í–¥ìˆ˜ DNA ìƒì„±")

        return current_dna

    def save_model(self, path: str):
        """ëª¨ë¸ ì €ì¥"""
        torch.save({
            'model_state_dict': self.policy_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'training_history': self.training_history
        }, path)
        logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")

    def load_model(self, path: str):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(path)
        self.policy_network.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.training_history = checkpoint.get('training_history', [])
        logger.info(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")


def example_usage():
    """ì‚¬ìš© ì˜ˆì‹œ"""

    # ì´ˆê¸° DNA ìƒì„± (MOGA ì—”ì§„ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ)
    initial_dna = OlfactoryDNA(
        genes=[(1, 5.0), (3, 8.0), (5, 12.0), (7, 3.0), (9, 6.0)],
        fitness_scores=(0.8, 0.7, 0.9)
    )

    # ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­
    brief = CreativeBrief(
        emotional_palette=[0.4, 0.6, 0.2],  # í™œê¸°, ìš°ì•„í•¨, ë”°ëœ»í•¨
        fragrance_family="oriental",
        mood="sophisticated",
        intensity=0.8,
        season="autumn",
        gender="unisex"
    )

    # ì§„í™” ì—”ì§„ ì´ˆê¸°í™”
    engine = EpigeneticVariationAI(
        state_dim=100,
        learning_rate=0.001,
        gamma=0.99
    )

    # ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ì§„í™” ì‹¤í–‰
    print("ğŸ§¬ ì§„í™” ì—”ì§„: ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ í–¥ìˆ˜ ì§„í™” ì‹œì‘...")
    evolved_dna = engine.evolve_with_feedback(
        initial_dna=initial_dna,
        brief=brief,
        num_iterations=5
    )

    print("\nâœ¨ ì§„í™” ì™„ë£Œ!")
    print(f"ìµœì¢… DNA: {evolved_dna.genes[:5]}")  # ì²˜ìŒ 5ê°œ ìœ ì „ìë§Œ ì¶œë ¥
    print(f"í•™ìŠµ íˆìŠ¤í† ë¦¬ ê¸¸ì´: {len(engine.training_history)}")

    # ëª¨ë¸ ì €ì¥
    engine.save_model("fragrance_rlhf_model.pth")


if __name__ == "__main__":
    example_usage()