"""
ìš´ì˜ ì„¤ì • (Operations Config)
ì‹¤ë¬´ í™˜ê²½ì—ì„œ ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ ìš´ì˜ íŒ êµ¬í˜„
"""

import numpy as np
import logging
from typing import Dict, Any, Optional, List
from collections import deque
from dataclasses import dataclass
import json
from pathlib import Path

logger = logging.getLogger(__name__)


# =============================================================================
# 1. í•™ìŠµ ìŠ¤ì¼€ì¤„ ì„¤ì •
# =============================================================================

@dataclass
class LearningScheduleConfig:
    """í•™ìŠµ ìŠ¤ì¼€ì¤„ ì„¤ì •"""

    # Entropy coefficient decay
    entropy_coef_start: float = 0.01  # ì´ˆê¸° ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜
    entropy_coef_end: float = 0.001   # ìµœì¢… ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜
    entropy_decay_type: str = "cosine"  # linear or cosine
    entropy_decay_steps: int = 10000   # Decay ê¸°ê°„

    # Reward normalization
    reward_norm_window: int = 1000  # 1k step ì°½
    reward_norm_epsilon: float = 1e-8

    # Learning rate decay
    lr_decay_type: str = "cosine"
    lr_warmup_steps: int = 100


class EntropyCoefficientScheduler:
    """Entropy coefficient scheduler with linear/cosine decay"""

    def __init__(self, config: LearningScheduleConfig):
        self.config = config
        self.current_step = 0

    def get_entropy_coef(self, step: Optional[int] = None) -> float:
        """í˜„ì¬ ìŠ¤í…ì˜ entropy coefficient ë°˜í™˜"""
        if step is None:
            step = self.current_step
        else:
            self.current_step = step

        if step >= self.config.entropy_decay_steps:
            return self.config.entropy_coef_end

        progress = step / self.config.entropy_decay_steps

        if self.config.entropy_decay_type == "linear":
            # Linear decay
            coef = self.config.entropy_coef_start - \
                   (self.config.entropy_coef_start - self.config.entropy_coef_end) * progress
        else:
            # Cosine decay
            coef = self.config.entropy_coef_end + \
                   0.5 * (self.config.entropy_coef_start - self.config.entropy_coef_end) * \
                   (1 + np.cos(np.pi * progress))

        return coef

    def step(self) -> float:
        """ìŠ¤í… ì§„í–‰ ë° í˜„ì¬ coefficient ë°˜í™˜"""
        self.current_step += 1
        return self.get_entropy_coef()


class RewardNormalizer:
    """Reward normalization with sliding window"""

    def __init__(self, window_size: int = 1000, epsilon: float = 1e-8):
        self.window_size = window_size
        self.epsilon = epsilon
        self.rewards = deque(maxlen=window_size)

    def normalize(self, reward: float) -> float:
        """Reward ì •ê·œí™”"""
        # ìœˆë„ìš°ì— ì¶”ê°€
        self.rewards.append(reward)

        # í†µê³„ ê³„ì‚°
        if len(self.rewards) < 2:
            return reward

        mean = np.mean(self.rewards)
        std = np.std(self.rewards)

        # ì •ê·œí™”
        normalized = (reward - mean) / (std + self.epsilon)
        return normalized

    def get_stats(self) -> Dict[str, float]:
        """í˜„ì¬ í†µê³„ ë°˜í™˜"""
        if len(self.rewards) < 2:
            return {"mean": 0.0, "std": 1.0, "count": len(self.rewards)}

        return {
            "mean": float(np.mean(self.rewards)),
            "std": float(np.std(self.rewards)),
            "count": len(self.rewards)
        }


# =============================================================================
# 2. ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
# =============================================================================

@dataclass
class CheckpointConfig:
    """ì²´í¬í¬ì¸íŠ¸ ì„¤ì •"""

    # ì €ì¥ ì£¼ê¸°
    save_interval_steps: int = 500  # 500 stepë§ˆë‹¤ ì €ì¥
    max_checkpoints: int = 5  # ìµœëŒ€ ë³´ê´€ ê°œìˆ˜

    # ì´ìƒ ê°ì§€ ì„ê³„ê°’
    loss_spike_threshold: float = 2.0  # ì´ì „ ëŒ€ë¹„ 2ë°° ì´ìƒ
    reward_drop_threshold: float = 0.5  # ì´ì „ ëŒ€ë¹„ 50% ì´í•˜

    # ë¡¤ë°± ì„¤ì •
    enable_auto_rollback: bool = True
    rollback_lookback: int = 3  # ìµœê·¼ 3ê°œ ì²´í¬í¬ì¸íŠ¸ ì¤‘ ì„ íƒ


class CheckpointManager:
    """ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ with ìë™ ë¡¤ë°±"""

    def __init__(self, checkpoint_dir: str, config: CheckpointConfig):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.config = config

        # ì´ë ¥ ì¶”ì 
        self.checkpoint_history: List[Dict[str, Any]] = []
        self.loss_history = deque(maxlen=20)
        self.reward_history = deque(maxlen=20)

    def should_save(self, current_step: int) -> bool:
        """ì €ì¥ í•„ìš” ì—¬ë¶€ í™•ì¸"""
        return current_step % self.config.save_interval_steps == 0

    def save_checkpoint(
        self,
        step: int,
        model_state: Dict[str, Any],
        metrics: Dict[str, float]
    ) -> str:
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_path = self.checkpoint_dir / f"checkpoint_step_{step}.pth"

        checkpoint_data = {
            "step": step,
            "model_state": model_state,
            "metrics": metrics,
            "timestamp": str(Path(__file__).stat().st_mtime)
        }

        import torch
        torch.save(checkpoint_data, checkpoint_path)

        # ì´ë ¥ ì¶”ê°€
        self.checkpoint_history.append({
            "step": step,
            "path": str(checkpoint_path),
            "loss": metrics.get("loss", 0.0),
            "reward": metrics.get("reward", 0.0)
        })

        # ë©”íŠ¸ë¦­ ì´ë ¥ ì¶”ê°€
        self.loss_history.append(metrics.get("loss", 0.0))
        self.reward_history.append(metrics.get("reward", 0.0))

        # ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬
        self._cleanup_old_checkpoints()

        logger.info(f"âœ“ Checkpoint saved: {checkpoint_path} (step={step})")
        return str(checkpoint_path)

    def detect_anomaly(self, current_metrics: Dict[str, float]) -> Optional[str]:
        """ì´ìƒ ê°ì§€ - ê¸‰ê²©í•œ loss/reward ë³€í™”"""
        if len(self.loss_history) < 3 or len(self.reward_history) < 3:
            return None

        current_loss = current_metrics.get("loss", 0.0)
        current_reward = current_metrics.get("reward", 0.0)

        # ìµœê·¼ í‰ê· 
        recent_loss_avg = np.mean(list(self.loss_history)[-5:])
        recent_reward_avg = np.mean(list(self.reward_history)[-5:])

        # Loss spike ê°ì§€
        if current_loss > recent_loss_avg * self.config.loss_spike_threshold:
            return f"loss_spike: {current_loss:.4f} > {recent_loss_avg:.4f} * {self.config.loss_spike_threshold}"

        # Reward drop ê°ì§€
        if current_reward < recent_reward_avg * self.config.reward_drop_threshold:
            return f"reward_drop: {current_reward:.4f} < {recent_reward_avg:.4f} * {self.config.reward_drop_threshold}"

        return None

    def rollback_to_best(self) -> Optional[str]:
        """ìµœì  ì²´í¬í¬ì¸íŠ¸ë¡œ ë¡¤ë°±"""
        if len(self.checkpoint_history) < 2:
            logger.warning("Not enough checkpoints for rollback")
            return None

        # ìµœê·¼ Nê°œ ì¤‘ ìµœê³  reward ì„ íƒ
        recent_checkpoints = self.checkpoint_history[-self.config.rollback_lookback:]
        best_checkpoint = max(recent_checkpoints, key=lambda x: x["reward"])

        logger.warning(f"ğŸ”„ Rolling back to step {best_checkpoint['step']} (reward={best_checkpoint['reward']:.3f})")
        return best_checkpoint["path"]

    def _cleanup_old_checkpoints(self):
        """ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ"""
        if len(self.checkpoint_history) > self.config.max_checkpoints:
            # ì˜¤ë˜ëœ ê²ƒ ì‚­ì œ
            old_checkpoint = self.checkpoint_history.pop(0)
            old_path = Path(old_checkpoint["path"])
            if old_path.exists():
                old_path.unlink()
                logger.debug(f"Deleted old checkpoint: {old_path}")


# =============================================================================
# 3. ì¥ì•  ëŒ€ì²˜ (Circuit Breaker)
# =============================================================================

@dataclass
class CircuitBreakerConfig:
    """ì„œí‚·ë¸Œë ˆì´ì»¤ ì„¤ì •"""

    # Qwen ì¥ì•  ê°ì§€
    qwen_failure_threshold: int = 3  # 3ë²ˆ ì‹¤íŒ¨ ì‹œ ë‹¤ìš´ì‹œí”„íŠ¸
    qwen_timeout_seconds: float = 5.0

    # ë‹¤ìš´ì‹œí”„íŠ¸ ì „ëµ
    enable_auto_downshift: bool = True
    downshift_duration_seconds: int = 300  # 5ë¶„ê°„ ë‹¤ìš´ì‹œí”„íŠ¸ ìœ ì§€

    # TTL ë‹¨ì¶•
    enable_ttl_reduction: bool = True
    normal_ttl_seconds: int = 3600  # ì •ìƒ: 1ì‹œê°„
    reduced_ttl_seconds: int = 300   # ë‹¨ì¶•: 5ë¶„


class CircuitBreaker:
    """ì„œí‚·ë¸Œë ˆì´ì»¤ with ìë™ ë‹¤ìš´ì‹œí”„íŠ¸"""

    def __init__(self, config: CircuitBreakerConfig):
        self.config = config

        # ì‹¤íŒ¨ ì¹´ìš´í„°
        self.qwen_failures = 0
        self.qwen_last_failure_time = 0

        # ë‹¤ìš´ì‹œí”„íŠ¸ ìƒíƒœ
        self.is_downshifted = False
        self.downshift_start_time = 0

    def record_qwen_failure(self):
        """Qwen ì‹¤íŒ¨ ê¸°ë¡"""
        import time
        self.qwen_failures += 1
        self.qwen_last_failure_time = time.time()

        logger.warning(f"âš ï¸ Qwen failure recorded: {self.qwen_failures}/{self.config.qwen_failure_threshold}")

        # ì„ê³„ê°’ ë„ë‹¬ ì‹œ ë‹¤ìš´ì‹œí”„íŠ¸
        if self.qwen_failures >= self.config.qwen_failure_threshold:
            if self.config.enable_auto_downshift:
                self._trigger_downshift()

    def record_qwen_success(self):
        """Qwen ì„±ê³µ ê¸°ë¡ - ì‹¤íŒ¨ ì¹´ìš´í„° ë¦¬ì…‹"""
        self.qwen_failures = max(0, self.qwen_failures - 1)

    def _trigger_downshift(self):
        """ë‹¤ìš´ì‹œí”„íŠ¸ íŠ¸ë¦¬ê±°"""
        import time
        self.is_downshifted = True
        self.downshift_start_time = time.time()

        logger.warning("ğŸ”» Auto downshift triggered: creative -> balanced/fast")

        # ë©”íŠ¸ë¦­ ê¸°ë¡
        try:
            from fragrance_ai.monitoring.operations_metrics import OperationsMetricsCollector
            collector = OperationsMetricsCollector()
            collector.record_circuit_breaker_downgrade("llm", "creative", "balanced")
        except Exception as e:
            logger.debug(f"Failed to record downshift metric: {e}")

    def should_use_downshift(self) -> bool:
        """ë‹¤ìš´ì‹œí”„íŠ¸ ì‚¬ìš© ì—¬ë¶€"""
        if not self.is_downshifted:
            return False

        import time
        elapsed = time.time() - self.downshift_start_time

        # ë‹¤ìš´ì‹œí”„íŠ¸ ê¸°ê°„ ì¢…ë£Œ
        if elapsed > self.config.downshift_duration_seconds:
            self.is_downshifted = False
            self.qwen_failures = 0
            logger.info("âœ“ Downshift period ended, returning to normal")
            return False

        return True

    def get_current_ttl(self) -> int:
        """í˜„ì¬ TTL ë°˜í™˜"""
        if self.config.enable_ttl_reduction and self.is_downshifted:
            return self.config.reduced_ttl_seconds
        return self.config.normal_ttl_seconds


# =============================================================================
# 4. ë°ì´í„° ê¸°ë°˜ ì¬íŠœë‹
# =============================================================================

@dataclass
class RetuningConfig:
    """ì¬íŠœë‹ ì„¤ì •"""

    # í”¼ë“œë°± ìƒ˜í”Œ ì„ê³„ê°’
    min_samples_for_retune: int = 100
    optimal_samples_for_retune: int = 300

    # ì¬íŠœë‹ ëŒ€ìƒ
    retune_learning_rate: bool = True
    retune_entropy_coef: bool = True
    retune_clip_epsilon: bool = True


class HyperparameterRetuner:
    """ë°ì´í„° ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¬íŠœë‹"""

    def __init__(self, config: RetuningConfig):
        self.config = config
        self.feedback_samples: List[Dict[str, Any]] = []

    def add_feedback(self, feedback: Dict[str, Any]):
        """í”¼ë“œë°± ì¶”ê°€"""
        self.feedback_samples.append(feedback)
        logger.debug(f"Feedback added: {len(self.feedback_samples)}/{self.config.optimal_samples_for_retune}")

    def should_retune(self) -> bool:
        """ì¬íŠœë‹ í•„ìš” ì—¬ë¶€"""
        return len(self.feedback_samples) >= self.config.min_samples_for_retune

    def suggest_hyperparameters(self) -> Dict[str, float]:
        """í”¼ë“œë°± ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì²œ"""
        if not self.should_retune():
            logger.warning(f"Not enough samples for retuning: {len(self.feedback_samples)}")
            return {}

        # í”¼ë“œë°± ë¶„ì„
        ratings = [f.get("rating", 0.0) for f in self.feedback_samples]
        avg_rating = np.mean(ratings)
        rating_std = np.std(ratings)

        suggestions = {}

        # í‰ê·  ratingì´ ë‚®ìœ¼ë©´ exploration ì¦ê°€ (entropy_coef ì¦ê°€)
        if avg_rating < 3.0 and self.config.retune_entropy_coef:
            suggestions["entropy_coef"] = 0.02  # ê¸°ë³¸ 0.01 -> 0.02
            logger.info("ğŸ“ˆ Suggesting higher entropy_coef for more exploration")

        # Rating ë³€ë™ì´ í¬ë©´ learning rate ê°ì†Œ
        if rating_std > 1.5 and self.config.retune_learning_rate:
            suggestions["learning_rate"] = 1e-4  # ê¸°ë³¸ 3e-4 -> 1e-4
            logger.info("ğŸ“‰ Suggesting lower learning_rate for stability")

        # í‰ê·  ratingì´ ë†’ìœ¼ë©´ exploitation ì¦ê°€ (clip_epsilon ê°ì†Œ)
        if avg_rating >= 4.0 and self.config.retune_clip_epsilon:
            suggestions["clip_epsilon"] = 0.1  # ê¸°ë³¸ 0.2 -> 0.1
            logger.info("ğŸ¯ Suggesting lower clip_epsilon for exploitation")

        logger.info(f"Retuning suggestions based on {len(self.feedback_samples)} samples:")
        for key, value in suggestions.items():
            logger.info(f"  {key}: {value}")

        return suggestions

    def clear_feedback(self):
        """í”¼ë“œë°± ìƒ˜í”Œ ì´ˆê¸°í™”"""
        self.feedback_samples.clear()
        logger.info("Feedback samples cleared")


# =============================================================================
# í†µí•© ìš´ì˜ ì„¤ì •
# =============================================================================

class OperationsManager:
    """í†µí•© ìš´ì˜ ê´€ë¦¬ì"""

    def __init__(
        self,
        checkpoint_dir: str = "./checkpoints",
        enable_all_features: bool = True
    ):
        # ì„¤ì • ì´ˆê¸°í™”
        self.schedule_config = LearningScheduleConfig()
        self.checkpoint_config = CheckpointConfig()
        self.circuit_breaker_config = CircuitBreakerConfig()
        self.retuning_config = RetuningConfig()

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.entropy_scheduler = EntropyCoefficientScheduler(self.schedule_config)
        self.reward_normalizer = RewardNormalizer(
            window_size=self.schedule_config.reward_norm_window
        )
        self.checkpoint_manager = CheckpointManager(checkpoint_dir, self.checkpoint_config)
        self.circuit_breaker = CircuitBreaker(self.circuit_breaker_config)
        self.retuner = HyperparameterRetuner(self.retuning_config)

        self.enabled = enable_all_features

    def on_training_step(
        self,
        step: int,
        metrics: Dict[str, float],
        model_state: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """í•™ìŠµ ìŠ¤í…ë§ˆë‹¤ í˜¸ì¶œ"""
        if not self.enabled:
            return {}

        # 1. Entropy coefficient ì—…ë°ì´íŠ¸
        current_entropy_coef = self.entropy_scheduler.get_entropy_coef(step)

        # 2. Reward normalization
        if "reward" in metrics:
            normalized_reward = self.reward_normalizer.normalize(metrics["reward"])
            metrics["normalized_reward"] = normalized_reward

        # 3. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = None
        if self.checkpoint_manager.should_save(step) and model_state:
            checkpoint_path = self.checkpoint_manager.save_checkpoint(step, model_state, metrics)

        # 4. ì´ìƒ ê°ì§€ ë° ë¡¤ë°±
        anomaly = self.checkpoint_manager.detect_anomaly(metrics)
        rollback_path = None
        if anomaly and self.checkpoint_config.enable_auto_rollback:
            logger.error(f"âŒ Anomaly detected: {anomaly}")
            rollback_path = self.checkpoint_manager.rollback_to_best()

        return {
            "entropy_coef": current_entropy_coef,
            "checkpoint_path": checkpoint_path,
            "anomaly": anomaly,
            "rollback_path": rollback_path,
            "should_downshift": self.circuit_breaker.should_use_downshift(),
            "current_ttl": self.circuit_breaker.get_current_ttl()
        }

    def on_llm_failure(self, model: str):
        """LLM ì‹¤íŒ¨ ì‹œ í˜¸ì¶œ"""
        if model == "qwen":
            self.circuit_breaker.record_qwen_failure()

    def on_llm_success(self, model: str):
        """LLM ì„±ê³µ ì‹œ í˜¸ì¶œ"""
        if model == "qwen":
            self.circuit_breaker.record_qwen_success()

    def on_user_feedback(self, feedback: Dict[str, Any]):
        """ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘"""
        self.retuner.add_feedback(feedback)

        # ì¬íŠœë‹ ì œì•ˆ
        if self.retuner.should_retune():
            suggestions = self.retuner.suggest_hyperparameters()
            return suggestions
        return None


# =============================================================================
# ì‚¬ìš© ì˜ˆì‹œ
# =============================================================================

if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)

    print("=== ìš´ì˜ ì„¤ì • ì‹œë®¬ë ˆì´ì…˜ ===\n")

    # ìš´ì˜ ê´€ë¦¬ì ì´ˆê¸°í™”
    ops_manager = OperationsManager(checkpoint_dir="./test_checkpoints")

    # 1. í•™ìŠµ ìŠ¤í… ì‹œë®¬ë ˆì´ì…˜
    print("1. í•™ìŠµ ìŠ¤í… ì‹œë®¬ë ˆì´ì…˜")
    for step in range(0, 1500, 100):
        metrics = {
            "loss": 0.5 - step * 0.0001 + np.random.randn() * 0.01,
            "reward": 10.0 + step * 0.01 + np.random.randn() * 0.5
        }

        model_state = {"dummy": "state"}  # ì‹¤ì œë¡œëŠ” torch ëª¨ë¸ state

        result = ops_manager.on_training_step(step, metrics, model_state)

        if result.get("checkpoint_path"):
            print(f"  Step {step}: Checkpoint saved")
        if result.get("anomaly"):
            print(f"  Step {step}: âš ï¸ Anomaly - {result['anomaly']}")

    # 2. Qwen ì¥ì•  ì‹œë®¬ë ˆì´ì…˜
    print("\n2. Qwen ì¥ì•  ì‹œë®¬ë ˆì´ì…˜")
    for i in range(5):
        ops_manager.on_llm_failure("qwen")
        if ops_manager.circuit_breaker.should_use_downshift():
            print(f"  Failure {i+1}: Downshift activated!")
            break

    # 3. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘
    print("\n3. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ë° ì¬íŠœë‹")
    for i in range(150):
        feedback = {
            "rating": np.random.uniform(2.5, 4.5),
            "user_id": f"user_{i}"
        }
        suggestions = ops_manager.on_user_feedback(feedback)

        if suggestions:
            print(f"  âœ“ Retuning triggered after {i+1} samples")
            print(f"  Suggestions: {suggestions}")
            break

    print("\nì™„ë£Œ!")
