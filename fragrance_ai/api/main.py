"""
ì—…ê·¸ë ˆì´ë“œëœ Fragrance AI API ë©”ì¸ íŒŒì¼
- Pydantic v2
- ê³ ê¸‰ RAG ì‹œìŠ¤í…œ
- ì„±ëŠ¥ ìµœì í™”
- ë¶„ì‚° ìºì‹±
"""

from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks, Request
import numpy as np
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.responses import JSONResponse
import uvicorn
from typing import List, Dict, Any, Optional, Tuple
import time
import asyncio
from contextlib import asynccontextmanager

from ..core.config import settings
# from ..core.production_logging import get_logger, LogCategory
# from ..core.logging_middleware import RequestLoggingMiddleware, SecurityLoggingMiddleware
import logging
logger = logging.getLogger(__name__)
from ..core.exceptions import FragranceAIException, SystemException, ErrorCode
# from ..core.exceptions_unified import (
#     FragranceAIException as UnifiedException,
#     APIException,
#     ModelException,
#     global_error_handler,
#     handle_exceptions_async
# )
from ..core.advanced_caching import FragranceCacheManager, CachePolicy
from ..core.performance_optimizer import global_performance_optimizer
from ..core.error_handling import (
    global_error_handler,
    error_context,
    ErrorCategory,
    ErrorSeverity,
    ErrorContext
)
from ..models.embedding import AdvancedKoreanFragranceEmbedding
from ..models.rag_system import FragranceRAGSystem, RAGMode
from ..database.connection import initialize_database, shutdown_database
from .schemas import (
    SemanticSearchRequest,
    SemanticSearchResponse,
    SearchResult,
    RecipeGenerationRequest,
    RecipeGenerationResponse,
    BatchGenerationRequest,
    BatchGenerationResponse,
    SystemStatus,
    ErrorResponse
)
from .middleware import LoggingMiddleware, RateLimitMiddleware
from ..core.security_middleware import SecurityMiddleware, validate_input
from ..core.real_monitoring import get_monitoring_dashboard
from .dependencies import get_current_user, verify_api_key, require_permission
from .auth import get_current_user as auth_get_current_user, require_access_level
from .user_auth import router as user_auth_router
from .external_services import router as external_services_router
from .error_handlers import setup_error_handlers

# í”„ë¡œë•ì…˜ ë¡œê¹… ì´ˆê¸°í™”
logger = get_logger("fragrance_ai_api")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬ (ì—…ê·¸ë ˆì´ë“œëœ ë²„ì „)"""
    startup_time = time.time()
    
    async with error_context(
        category=ErrorCategory.SYSTEM,
        severity=ErrorSeverity.CRITICAL,
        attempt_recovery=False
    ):
        logger.info("ğŸš€ Starting Advanced Fragrance AI System...")

        # 1. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        logger.info("ğŸ“Š Initializing database...")
        initialize_database()

        # 2. ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        logger.info("ğŸ—„ï¸  Initializing advanced caching system...")
        app.state.cache_manager = FragranceCacheManager(
            max_size=50000,
            policy=CachePolicy.ADAPTIVE,
            redis_url=getattr(settings, 'redis_url', None),
            enable_metrics=True,
            warmup_enabled=True
        )

        # 3. ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        logger.info("âš¡ Starting performance optimization system...")
        await global_performance_optimizer.start()

        # 4. AI ëª¨ë¸ ì´ˆê¸°í™”
        logger.info("ğŸ¤– Loading advanced AI models...")

        # ê³ ê¸‰ ì„ë² ë”© ëª¨ë¸
        app.state.embedding_model = AdvancedKoreanFragranceEmbedding(
            use_adapter=True,
            enable_multi_aspect=True,
            cache_size=10000
        )

        # RAG ì‹œìŠ¤í…œ
        app.state.rag_system = FragranceRAGSystem(
            embedding_model=app.state.embedding_model,
            rag_mode=RAGMode.ADAPTIVE_RAG,
            max_retrieved_docs=15
        )

        # 5. ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì„¤ì •
        logger.info("ğŸ“¦ Setting up batch processors...")

        # ì„ë² ë”© ë°°ì¹˜ í”„ë¡œì„¸ì„œ
        embedding_processor = global_performance_optimizer.create_batch_processor(
            name="embedding_batch",
            batch_size=64,
            max_wait_time=0.05,
            processor_func=batch_embedding_processor
        )

        # ê²€ìƒ‰ ë°°ì¹˜ í”„ë¡œì„¸ì„œ
        search_processor = global_performance_optimizer.create_batch_processor(
            name="search_batch",
            batch_size=32,
            max_wait_time=0.1,
            processor_func=batch_search_processor
        )

        app.state.embedding_batch_processor = embedding_processor
        app.state.search_batch_processor = search_processor

        # 6. ìºì‹œ ì›Œë°ì—…
        logger.info("ğŸ”¥ Warming up caches...")
        await app.state.cache_manager.warmup(generate_warmup_data)

        # 7. í—¬ìŠ¤ì²´í¬ ì¤€ë¹„
        app.state.startup_time = startup_time
        app.state.ready = True

        total_startup_time = time.time() - startup_time
        logger.info(f"âœ… Advanced Fragrance AI System ready in {total_startup_time:.2f}s")
    
    yield
    
    # Shutdown
    logger.info("ğŸ›‘ Shutting down Advanced Fragrance AI System...")
    
    try:
        # Performance optimizer ì¤‘ì§€
        await global_performance_optimizer.stop()
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì¢…ë£Œ
        shutdown_database()
        
        logger.info("âœ… Graceful shutdown completed")
        
    except Exception as e:
        logger.error(f"âŒ Shutdown error: {e}")


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Advanced Fragrance AI",
    version="2.0.0",
    description="""
    ğŸŒ¸ **ê³ ê¸‰ í–¥ìˆ˜ AI ì‹œìŠ¤í…œ**
    
    ìµœì‹  AI ê¸°ìˆ ì„ ì ìš©í•œ í–¥ìˆ˜ ì¶”ì²œ ë° ìƒì„± ì‹œìŠ¤í…œ
    - ğŸ¤– Advanced RAG (Retrieval-Augmented Generation)
    - âš¡ ê³ ì„±ëŠ¥ ë¹„ë™ê¸° ì²˜ë¦¬
    - ğŸ—„ï¸ ì§€ëŠ¥í˜• ë¶„ì‚° ìºì‹±
    - ğŸ“Š ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    - ğŸ¯ í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”©
    """,
    docs_url="/api/v2/docs",
    redoc_url="/api/v2/redoc",
    openapi_url="/api/v2/openapi.json",
    lifespan=lifespan
)

# Security
security = HTTPBearer()

# CORS - í”„ë¡œë•ì…˜ ë³´ì•ˆ ì„¤ì •
allowed_origins = [
    "http://localhost:3000",  # ê°œë°œìš©
    "http://127.0.0.1:3000",  # ê°œë°œìš©
]

# í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ í—ˆìš© ë„ë©”ì¸ ê°€ì ¸ì˜¤ê¸°
if hasattr(settings, 'cors_origins') and settings.cors_origins:
    if isinstance(settings.cors_origins, str):
        # ë¬¸ìì—´ì¸ ê²½ìš° JSON íŒŒì‹±
        import json
        try:
            allowed_origins = json.loads(settings.cors_origins)
        except json.JSONDecodeError:
            allowed_origins = [settings.cors_origins]
    else:
        allowed_origins = settings.cors_origins

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
    max_age=86400,  # 24ì‹œê°„ ìºì‹œ
)

# ì»¤ìŠ¤í…€ ë¯¸ë“¤ì›¨ì–´ (ìˆœì„œ ì¤‘ìš” - ë¡œê¹…ì´ ê°€ì¥ ë°”ê¹¥ìª½)
app.add_middleware(RequestLoggingMiddleware)
app.add_middleware(SecurityLoggingMiddleware)
app.add_middleware(SecurityMiddleware,
                  rate_limit_per_minute=100,  # ë¶„ë‹¹ 100 ìš”ì²­
                  max_request_size=2*1024*1024)  # 2MB
app.add_middleware(RateLimitMiddleware)

# ì—ëŸ¬ í•¸ë“¤ëŸ¬
setup_error_handlers(app)

# í†µí•© ì˜ˆì™¸ í•¸ë“¤ëŸ¬ ì¶”ê°€
@app.exception_handler(UnifiedException)
async def fragrance_ai_exception_handler(request: Request, exc: UnifiedException):
    """FragranceAI ì˜ˆì™¸ í•¸ë“¤ëŸ¬"""
    error_info = global_error_handler.handle_error(exc)

    return JSONResponse(
        status_code=getattr(exc, 'status_code', 500),
        content={
            "error": True,
            "error_code": exc.error_code.value,
            "message": exc.message,
            "details": exc.details,
            "timestamp": exc.timestamp.isoformat()
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """ì¼ë°˜ ì˜ˆì™¸ í•¸ë“¤ëŸ¬"""
    # ì˜ˆìƒì¹˜ ëª»í•œ ì˜ˆì™¸ë¥¼ FragranceAI ì˜ˆì™¸ë¡œ ë³€í™˜
    fragrance_exc = UnifiedException(
        message=f"Unexpected server error: {str(exc)}",
        cause=exc
    )

    error_info = global_error_handler.handle_error(fragrance_exc)

    return JSONResponse(
        status_code=500,
        content={
            "error": True,
            "error_code": fragrance_exc.error_code.value,
            "message": "Internal server error",
            "timestamp": fragrance_exc.timestamp.isoformat()
        }
    )

# ë¼ìš°í„° ë“±ë¡
app.include_router(user_auth_router, prefix="/api/v2")
app.include_router(external_services_router, prefix="/api/v2")

# ìƒˆë¡œìš´ ì¸ì¦ ë° ë ˆì‹œí”¼ ë¼ìš°í„° ì¶”ê°€
from .routes.auth import router as auth_router
from .routes.public_recipes import router as public_recipes_router
from .routes.generation import router as generation_router
from .routes.agentic import router as agentic_router
from .routes.customer_service import router as customer_service_router
from .routes.admin_auth import router as admin_auth_router  # ê´€ë¦¬ì ì¸ì¦ ë¼ìš°í„°
from .routes.generation_with_ai import router as ai_generation_router  # AI ìƒì„± ë¼ìš°í„° ì¶”ê°€

app.include_router(auth_router, prefix="/api/v2")
app.include_router(public_recipes_router, prefix="/api/v2")
app.include_router(generation_router, prefix="/api/v2/admin", tags=["ê´€ë¦¬ì ì „ìš©"])
app.include_router(agentic_router, prefix="/api/v2", tags=["AI Orchestrator"])  # New agentic system
app.include_router(customer_service_router, tags=["Customer Service"])  # Customer service routes
app.include_router(admin_auth_router, tags=["Admin Authentication"])  # ê´€ë¦¬ì ì„¸ì…˜ ê¸°ë°˜ ì¸ì¦
app.include_router(ai_generation_router, prefix="/api/v1/ai", tags=["AI Generation"])  # DEAP/RLHF AI ë¼ìš°í„°


# ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
async def batch_embedding_processor(items: List[Tuple]) -> List[np.ndarray]:
    """ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬"""
    texts = [item[0] for item in items]
    embedding_model = app.state.embedding_model
    
    result = await embedding_model.encode_async(texts, enable_caching=True)
    return [result.embeddings[i] for i in range(len(texts))]


async def batch_search_processor(items: List[Tuple]) -> List[List[SearchResult]]:
    """ê²€ìƒ‰ ë°°ì¹˜ ì²˜ë¦¬"""
    results = []
    for query, search_params in items:
        # ì‹¤ì œ ê²€ìƒ‰ ë¡œì§ êµ¬í˜„
        search_results = await perform_single_search(query, search_params)
        results.append(search_results)
    return results


async def perform_single_search(query: str, params: Dict[str, Any]) -> List[SearchResult]:
    """ë‹¨ì¼ ê²€ìƒ‰ ìˆ˜í–‰"""
    # ìºì‹œ í™•ì¸
    cached_result = await app.state.cache_manager.get_cached_embedding(query)
    
    # RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•œ ê²€ìƒ‰
    rag_result = await app.state.rag_system.generate_with_rag(
        query=query,
        temperature=0.7,
        enable_reasoning=True
    )
    
    # ê²°ê³¼ë¥¼ SearchResult í˜•íƒœë¡œ ë³€í™˜
    results = []
    for i, doc in enumerate(rag_result.source_documents[:10]):
        results.append(SearchResult(
            id=f"doc_{i}",
            document=doc,
            metadata={"source": "rag_system"},
            distance=0.0,
            similarity=rag_result.confidence_score,
            collection="knowledge_base",
            rank=i + 1
        ))
    
    return results


def generate_warmup_data() -> Dict[str, Any]:
    """ìºì‹œ ì›Œë°ì—… ë°ì´í„° ìƒì„±"""
    warmup_data = {}
    
    # ì¸ê¸° ê²€ìƒ‰ì–´ë“¤
    popular_queries = [
        "ì‹œíŠ¸ëŸ¬ìŠ¤ í–¥ìˆ˜", "í”Œë¡œëŸ´ í–¥ìˆ˜", "ìš°ë”” í–¥ìˆ˜", "ì—¬ë¦„ í–¥ìˆ˜",
        "ê²¨ìš¸ í–¥ìˆ˜", "ë¡œë§¨í‹±í•œ í–¥ìˆ˜", "í”„ë ˆì‹œí•œ í–¥ìˆ˜", "ì˜¤ë¦¬ì—”íƒˆ í–¥ìˆ˜"
    ]
    
    # ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚°
    for query in popular_queries:
        cache_key = f"popular_query:{query}"
        warmup_data[cache_key] = {"query": query, "popularity": 1.0}
    
    return warmup_data


# API ì—”ë“œí¬ì¸íŠ¸ë“¤

@app.get("/")
async def root():
    """API ë£¨íŠ¸"""
    return {
        "message": "ğŸŒ¸ Advanced Fragrance AI v2.0",
        "version": "2.0.0",
        "features": [
            "ğŸ¤– Advanced RAG System",
            "âš¡ High-Performance Async Processing", 
            "ğŸ—„ï¸ Intelligent Distributed Caching",
            "ğŸ“Š Real-time Performance Monitoring",
            "ğŸ¯ Korean-Specialized Embeddings"
        ],
        "endpoints": {
            "docs": "/api/v2/docs",
            "semantic_search": "/api/v2/semantic-search",
            "rag_chat": "/api/v2/rag-chat",
            "auth_login": "/api/v2/auth/login",
            "public_recipes": "/api/v2/public/recipes/generate",
            "admin_recipes": "/api/v2/admin/recipe",
            "performance": "/api/v2/performance"
        }
    }


@app.get("/health")
async def advanced_health_check(request: Request):
    """ê³ ê¸‰ í—¬ìŠ¤ì²´í¬"""
    # ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    error_ctx = ErrorContext(
        endpoint="/health",
        method="GET",
        user_agent=request.headers.get("user-agent"),
        ip_address=getattr(request.client, 'host', None)
    )

    async with error_context(
        category=ErrorCategory.SYSTEM,
        severity=ErrorSeverity.HIGH,
        context=error_ctx,
        attempt_recovery=False,
        raise_on_error=False
    ):
        health_status = {
            "status": "healthy",
            "timestamp": time.time(),
            "uptime": time.time() - getattr(app.state, 'startup_time', time.time()),
            "version": "2.0.0"
        }

        # ê° ì»´í¬ë„ŒíŠ¸ ìƒíƒœ í™•ì¸
        components = {
            "embedding_model": hasattr(app.state, 'embedding_model'),
            "rag_system": hasattr(app.state, 'rag_system'),
            "cache_manager": hasattr(app.state, 'cache_manager'),
            "performance_optimizer": global_performance_optimizer.running,
            "batch_processors": len(global_performance_optimizer.batch_processors) > 0
        }

        health_status["components"] = components
        health_status["all_systems_ready"] = all(components.values())

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ê°€
        if hasattr(app.state, 'cache_manager'):
            cache_stats = app.state.cache_manager.get_stats()
            health_status["cache_stats"] = {
                "hit_rate": cache_stats.get("hit_rate", 0),
                "cache_size": cache_stats.get("cache_size", 0)
            }

        # ì—ëŸ¬ í†µê³„ ì¶”ê°€
        error_stats = global_error_handler.get_error_statistics(hours=1)
        health_status["error_stats"] = {
            "total_errors_last_hour": error_stats["total_errors"],
            "recovery_success_rate": error_stats["recovery_success_rate"],
            "critical_errors": error_stats["by_severity"].get("critical", 0)
        }

        if not health_status["all_systems_ready"]:
            return JSONResponse(status_code=503, content=health_status)

        return health_status


@app.get("/api/v2/monitoring")
async def get_monitoring_dashboard():
    """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ë°ì´í„°"""
    try:
        dashboard = get_monitoring_dashboard()
        dashboard_data = dashboard.get_dashboard_data()

        return JSONResponse(
            status_code=200,
            content={
                "success": True,
                "data": dashboard_data,
                "timestamp": time.time()
            }
        )
    except Exception as e:
        logger.error(f"Failed to get monitoring data: {e}")
        raise HTTPException(
            status_code=500,
            detail="Failed to retrieve monitoring data"
        )


@app.get("/api/v2/monitoring/alerts")
async def get_recent_alerts():
    """ìµœê·¼ ì•Œë¦¼ ì¡°íšŒ"""
    try:
        dashboard = get_monitoring_dashboard()
        alerts = dashboard.alert_manager.get_recent_alerts(limit=20)

        return JSONResponse(
            status_code=200,
            content={
                "success": True,
                "alerts": alerts,
                "total_count": len(alerts),
                "timestamp": time.time()
            }
        )
    except Exception as e:
        logger.error(f"Failed to get alerts: {e}")
        raise HTTPException(
            status_code=500,
            detail="Failed to retrieve alerts"
        )


@app.post("/api/v2/semantic-search")
@validate_input({
    "query": {"type": "text", "max_length": 500},
    "top_k": {"type": "numeric", "min": 1, "max": 100},
    "min_similarity": {"type": "numeric", "min": 0.0, "max": 1.0}
})
async def advanced_semantic_search(
    request: SemanticSearchRequest,
    background_tasks: BackgroundTasks,
    req: Request
) -> SemanticSearchResponse:
    """ê³ ê¸‰ ì‹œë§¨í‹± ê²€ìƒ‰"""
    start_time = time.time()

    # ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    error_ctx = ErrorContext(
        endpoint="/api/v2/semantic-search",
        method="POST",
        user_agent=req.headers.get("user-agent"),
        ip_address=getattr(req.client, 'host', None),
        additional_data={"query_length": len(request.query), "top_k": request.top_k}
    )

    async with error_context(
        category=ErrorCategory.MODEL_INFERENCE,
        severity=ErrorSeverity.MEDIUM,
        context=error_ctx,
        attempt_recovery=True
    ):
        # ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        use_batch = len(request.query) > 100 or request.top_k > 20

        if use_batch and hasattr(app.state, 'search_batch_processor'):
            # ë°°ì¹˜ ì²˜ë¦¬
            search_params = {
                "search_type": request.search_type,
                "collections": request.collections,
                "top_k": request.top_k,
                "min_similarity": request.min_similarity
            }

            results = await app.state.search_batch_processor.add_item(
                (request.query, search_params)
            )
        else:
            # ë‹¨ì¼ ì²˜ë¦¬
            results = await perform_single_search(request.query, request.model_dump())

        # ì¬ìˆœìœ„í™” (í™œì„±í™”ëœ ê²½ìš°)
        if request.enable_reranking and len(results) > 5:
            results = await rerank_results(results, request.query)

        # ìºì‹œì— ê²°ê³¼ ì €ì¥
        if request.use_cache:
            background_tasks.add_task(
                app.state.cache_manager.cache_search_result,
                request.query,
                [result.model_dump_optimized() for result in results]
            )

        search_time = time.time() - start_time

        return SemanticSearchResponse(
            results=results,
            total_results=len(results),
            query=request.query,
            search_time=search_time,
            cached=False,
            reranked=request.enable_reranking
        )


@app.post("/api/v2/rag-chat")
async def rag_chat(
    request: Request,
    query: str,
    context: Optional[str] = None,
    temperature: float = 0.7,
    enable_reasoning: bool = True,
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Dict[str, Any]:
    """RAG ê¸°ë°˜ ì±„íŒ…"""
    start_time = time.time()

    # ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    error_ctx = ErrorContext(
        endpoint="/api/v2/rag-chat",
        method="POST",
        user_id=current_user.get("user_id") if current_user else None,
        user_agent=request.headers.get("user-agent"),
        ip_address=getattr(request.client, 'host', None),
        additional_data={
            "query_length": len(query),
            "temperature": temperature,
            "enable_reasoning": enable_reasoning
        }
    )

    async with error_context(
        category=ErrorCategory.MODEL_INFERENCE,
        severity=ErrorSeverity.MEDIUM,
        context=error_ctx,
        attempt_recovery=True
    ):
        # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ìƒì„± (ì„±ëŠ¥ ë¡œê¹… í¬í•¨)
        with logger.log_performance("rag_generation"):
            result = await app.state.rag_system.generate_with_rag(
                query=query,
                context=context,
                temperature=temperature,
                enable_reasoning=enable_reasoning
            )

        # AI ëª¨ë¸ ì„±ëŠ¥ ë¡œê·¸
        logger.info(
            f"RAG generation completed for query length {len(query)}",
            category=LogCategory.AI_MODEL,
            query_length=len(query),
            confidence_score=result.confidence_score,
            documents_retrieved=len(result.source_documents),
            temperature=temperature,
            enable_reasoning=enable_reasoning
        )

        response_time = time.time() - start_time

        return {
            "response": result.generated_text,
            "confidence_score": result.confidence_score,
            "source_documents": result.source_documents[:3],  # Top 3ë§Œ ë°˜í™˜
            "reasoning_steps": result.reasoning_steps,
            "retrieval_info": {
                "retrieval_time": result.retrieval_context.retrieval_time,
                "documents_retrieved": len(result.retrieval_context.retrieved_documents),
                "avg_similarity": float(np.mean(result.retrieval_context.similarity_scores)) if result.retrieval_context.similarity_scores else 0.0
            },
            "response_time": response_time,
            "timestamp": time.time()
        }


@app.get("/api/v2/performance")
async def get_performance_metrics(
    request: Request,
    current_user: Dict[str, Any] = Depends(require_permission("system.metrics"))
):
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    error_ctx = ErrorContext(
        endpoint="/api/v2/performance",
        method="GET",
        user_id=current_user.get("user_id") if current_user else None,
        user_agent=request.headers.get("user-agent"),
        ip_address=getattr(request.client, 'host', None)
    )

    async with error_context(
        category=ErrorCategory.SYSTEM,
        severity=ErrorSeverity.LOW,
        context=error_ctx,
        attempt_recovery=True
    ):
        # ì„±ëŠ¥ ìµœì í™” ë³´ê³ ì„œ
        perf_report = global_performance_optimizer.get_performance_report()

        # ìºì‹œ í†µê³„
        cache_stats = {}
        if hasattr(app.state, 'cache_manager'):
            cache_stats = app.state.cache_manager.get_stats()
            cache_stats["hot_keys"] = app.state.cache_manager.get_hot_keys(10)

        # AI ëª¨ë¸ ìƒíƒœ
        model_stats = {}
        if hasattr(app.state, 'rag_system'):
            model_stats["knowledge_base"] = app.state.rag_system.get_knowledge_base_stats()

        return {
            "timestamp": time.time(),
            "performance": perf_report,
            "cache": cache_stats,
            "models": model_stats,
            "system_info": {
                "version": "2.0.0",
                "uptime": time.time() - getattr(app.state, 'startup_time', time.time())
            }
        }


@app.get("/api/v2/error-statistics")
async def get_error_statistics(
    request: Request,
    hours: int = 24,
    current_user: Dict[str, Any] = Depends(require_permission("system.admin"))
):
    """ì—ëŸ¬ í†µê³„ ì¡°íšŒ"""
    error_ctx = ErrorContext(
        endpoint="/api/v2/error-statistics",
        method="GET",
        user_id=current_user.get("user_id") if current_user else None,
        user_agent=request.headers.get("user-agent"),
        ip_address=getattr(request.client, 'host', None),
        additional_data={"hours": hours}
    )

    async with error_context(
        category=ErrorCategory.SYSTEM,
        severity=ErrorSeverity.LOW,
        context=error_ctx,
        attempt_recovery=True
    ):
        # ì—ëŸ¬ í†µê³„ ì¡°íšŒ
        error_stats = global_error_handler.get_error_statistics(hours=hours)

        # ìµœê·¼ ì—ëŸ¬ ìƒ˜í”Œ (ë¯¼ê°í•œ ì •ë³´ ì œì™¸)
        recent_errors = []
        for record in global_error_handler.error_records[-10:]:
            recent_errors.append({
                "error_id": record.error_id,
                "category": record.category.value,
                "severity": record.severity.value,
                "timestamp": record.context.timestamp.isoformat(),
                "endpoint": record.context.endpoint,
                "is_handled": record.is_handled,
                "recovery_successful": record.recovery_successful,
                "user_message": record.user_message
            })

        return {
            "timestamp": time.time(),
            "period_hours": hours,
            "statistics": error_stats,
            "recent_errors": recent_errors,
            "system_health": {
                "error_rate": error_stats["total_errors"] / max(hours, 1),
                "recovery_rate": error_stats["recovery_success_rate"],
                "critical_issues": error_stats["by_severity"].get("critical", 0) > 0
            }
        }


async def rerank_results(results: List[SearchResult], query: str) -> List[SearchResult]:
    """ê²€ìƒ‰ ê²°ê³¼ ì¬ìˆœìœ„í™”"""
    # ê°„ë‹¨í•œ ì¬ìˆœìœ„í™” ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ëª¨ë¸ ì‚¬ìš©)
    query_words = set(query.lower().split())
    
    for result in results:
        doc_words = set(result.document.lower().split())
        keyword_overlap = len(query_words.intersection(doc_words))
        
        # ì¬ìˆœìœ„í™” ì ìˆ˜ ê³„ì‚°
        rerank_score = (
            result.effective_score * 0.7 +  # ê¸°ì¡´ ì ìˆ˜
            (keyword_overlap / len(query_words)) * 0.3  # í‚¤ì›Œë“œ ê²¹ì¹¨
        )
        result.rerank_score = rerank_score
    
    # ì¬ìˆœìœ„í™” ì ìˆ˜ë¡œ ì •ë ¬
    results.sort(key=lambda x: x.rerank_score or x.effective_score, reverse=True)
    
    # ìˆœìœ„ ì—…ë°ì´íŠ¸
    for i, result in enumerate(results):
        result.rank = i + 1
    
    return results


if __name__ == "__main__":
    uvicorn.run(
        "fragrance_ai.api.main:app",
        host=getattr(settings, 'api_host', '0.0.0.0'),
        port=getattr(settings, 'api_port', 8000),
        reload=getattr(settings, 'debug', False),
        workers=1 if getattr(settings, 'debug', False) else getattr(settings, 'max_workers', 4)
    )