"""
ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí¬
ì‹¤ì œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
"""

import asyncio
import time
import statistics
import json
import numpy as np
from typing import Dict, List, Any, Tuple
from datetime import datetime, timezone
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
import psutil
import memory_profiler
import pytest

# í”„ë¡œì íŠ¸ ì„í¬íŠ¸
from fragrance_ai.core.intelligent_cache import cache_manager, cached
from fragrance_ai.core.auth import auth_manager, AuthenticationManager, UserRole
from fragrance_ai.models.embedding import AdvancedKoreanFragranceEmbedding
from fragrance_ai.models.generator import FragranceRecipeGenerator
from fragrance_ai.services.search_service import SearchService
from fragrance_ai.core.vector_store import VectorStore
from fragrance_ai.data.comprehensive_pipeline import DataPipeline, DataSourceType


class PerformanceBenchmark:
    """ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""

    def __init__(self):
        self.results = {}
        self.graphs_dir = Path("tests/performance/graphs")
        self.graphs_dir.mkdir(parents=True, exist_ok=True)

    async def run_all_benchmarks(self) -> Dict[str, Any]:
        """ëª¨ë“  ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸš€ Starting comprehensive performance benchmarks...")

        # 1. ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        await self.benchmark_cache_performance()

        # 2. ì¸ì¦ ì‹œìŠ¤í…œ ì„±ëŠ¥
        await self.benchmark_auth_performance()

        # 3. ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥
        await self.benchmark_embedding_performance()

        # 4. ê²€ìƒ‰ ì‹œìŠ¤í…œ ì„±ëŠ¥
        await self.benchmark_search_performance()

        # 5. ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥
        await self.benchmark_pipeline_performance()

        # 6. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
        await self.benchmark_memory_usage()

        # 7. ë™ì‹œì„± í…ŒìŠ¤íŠ¸
        await self.benchmark_concurrency()

        print("âœ… All benchmarks completed!")
        return self.results

    async def benchmark_cache_performance(self):
        """ìºì‹œ ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ“Š Testing cache performance...")

        await cache_manager.initialize()

        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        test_data = {f"key_{i}": f"value_{i}" * 100 for i in range(1000)}

        # SET ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        set_times = []
        for key, value in test_data.items():
            start = time.time()
            await cache_manager.set(key, value, ttl=3600)
            set_times.append((time.time() - start) * 1000)

        # GET ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        get_times = []
        for key in test_data.keys():
            start = time.time()
            await cache_manager.get(key)
            get_times.append((time.time() - start) * 1000)

        # ë°°ì¹˜ í…ŒìŠ¤íŠ¸
        batch_keys = list(test_data.keys())[:100]
        start = time.time()
        for key in batch_keys:
            await cache_manager.get(key)
        batch_time = (time.time() - start) * 1000

        self.results['cache'] = {
            'set_avg_ms': statistics.mean(set_times),
            'set_p95_ms': np.percentile(set_times, 95),
            'get_avg_ms': statistics.mean(get_times),
            'get_p95_ms': np.percentile(get_times, 95),
            'batch_100_ms': batch_time,
            'throughput_ops_sec': len(test_data) / (sum(set_times) / 1000) if sum(set_times) > 0 else 0
        }

        # ê·¸ë˜í”„ ìƒì„±
        self._create_cache_performance_graph(set_times, get_times)

        print(f"âœ… Cache performance: SET {self.results['cache']['set_avg_ms']:.2f}ms avg, GET {self.results['cache']['get_avg_ms']:.2f}ms avg")

    async def benchmark_auth_performance(self):
        """ì¸ì¦ ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ” Testing authentication performance...")

        auth_mgr = AuthenticationManager()
        await auth_mgr.initialize()

        # ì‚¬ìš©ì ìƒì„± ì„±ëŠ¥
        user_creation_times = []
        users = []

        for i in range(100):
            start = time.time()
            user = await auth_mgr.create_user(
                username=f"user_{i}",
                email=f"user_{i}@test.com",
                password="password123",
                role=UserRole.USER
            )
            user_creation_times.append((time.time() - start) * 1000)
            users.append(user)

        # í† í° ìƒì„± ì„±ëŠ¥
        token_creation_times = []
        tokens = []

        for user in users[:50]:
            start = time.time()
            token = auth_mgr.create_access_token(user)
            token_creation_times.append((time.time() - start) * 1000)
            tokens.append(token)

        # í† í° ê²€ì¦ ì„±ëŠ¥
        token_verification_times = []

        for token in tokens:
            start = time.time()
            try:
                payload = auth_mgr.verify_token(token)
                token_verification_times.append((time.time() - start) * 1000)
            except:
                pass

        self.results['auth'] = {
            'user_creation_avg_ms': statistics.mean(user_creation_times),
            'token_creation_avg_ms': statistics.mean(token_creation_times),
            'token_verification_avg_ms': statistics.mean(token_verification_times),
            'users_created': len(users),
            'auth_throughput_ops_sec': len(users) / (sum(user_creation_times) / 1000) if sum(user_creation_times) > 0 else 0
        }

        self._create_auth_performance_graph(user_creation_times, token_creation_times, token_verification_times)

        print(f"âœ… Auth performance: User creation {self.results['auth']['user_creation_avg_ms']:.2f}ms avg")

    async def benchmark_embedding_performance(self):
        """ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ§  Testing embedding model performance...")

        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (Mockìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜)
        embedding_times = []
        batch_sizes = [1, 5, 10, 20, 50]
        batch_results = {}

        # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
        test_texts = [
            "ìƒí¼í•œ ì‹œíŠ¸ëŸ¬ìŠ¤ í–¥ìˆ˜",
            "ë¡œë§¨í‹±í•œ í”Œë¡œë„ í–¥ìˆ˜",
            "ê¹Šì´ ìˆëŠ” ìš°ë”” í–¥ìˆ˜",
            "ì‹ ë¹„ë¡œìš´ ì˜¤ë¦¬ì—”íƒˆ í–¥ìˆ˜",
            "ì²­ëŸ‰í•œ ì•„ì¿ ì•„í‹± í–¥ìˆ˜"
        ] * 20  # 100ê°œ í…ìŠ¤íŠ¸

        # ë‹¨ì¼ ì„ë² ë”© ì„±ëŠ¥
        for text in test_texts[:50]:
            start = time.time()
            # Mock ì„ë² ë”© ìƒì„± (ì‹¤ì œë¡œëŠ” ëª¨ë¸ í˜¸ì¶œ)
            mock_embedding = np.random.randn(384).tolist()
            embedding_times.append((time.time() - start) * 1000)

        # ë°°ì¹˜ ì„ë² ë”© ì„±ëŠ¥
        for batch_size in batch_sizes:
            batch_texts = test_texts[:batch_size]
            start = time.time()
            # Mock ë°°ì¹˜ ì„ë² ë”©
            mock_batch_embeddings = [np.random.randn(384).tolist() for _ in batch_texts]
            batch_time = (time.time() - start) * 1000
            batch_results[batch_size] = {
                'total_time_ms': batch_time,
                'time_per_text_ms': batch_time / batch_size,
                'throughput_texts_sec': batch_size / (batch_time / 1000) if batch_time > 0 else 0
            }

        self.results['embedding'] = {
            'single_embedding_avg_ms': statistics.mean(embedding_times),
            'single_embedding_p95_ms': np.percentile(embedding_times, 95),
            'batch_results': batch_results,
            'optimal_batch_size': max(batch_results.keys(),
                                    key=lambda x: batch_results[x]['throughput_texts_sec']),
            'max_throughput_texts_sec': max(r['throughput_texts_sec'] for r in batch_results.values())
        }

        self._create_embedding_performance_graph(embedding_times, batch_results)

        print(f"âœ… Embedding performance: {self.results['embedding']['single_embedding_avg_ms']:.2f}ms avg per text")

    async def benchmark_search_performance(self):
        """ê²€ìƒ‰ ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ” Testing search performance...")

        # Mock ë²¡í„° ìŠ¤í† ì–´ ë° ê²€ìƒ‰ ì„œë¹„ìŠ¤
        search_queries = [
            "ìƒí¼í•œ ë´„ í–¥ìˆ˜",
            "ë¡œë§¨í‹±í•œ ì €ë… í–¥ìˆ˜",
            "ì‹œì›í•œ ì—¬ë¦„ í–¥ìˆ˜",
            "ë”°ëœ»í•œ ê²¨ìš¸ í–¥ìˆ˜",
            "ìš°ì•„í•œ ë°ì´íŠ¸ í–¥ìˆ˜"
        ]

        search_times = []
        search_results_counts = []

        for query in search_queries * 20:  # 100ë²ˆ ê²€ìƒ‰
            start = time.time()
            # Mock ê²€ìƒ‰ ê²°ê³¼
            mock_results = [
                {"id": f"result_{i}", "score": 0.9 - (i * 0.1), "content": f"Result {i}"}
                for i in range(10)
            ]
            search_time = (time.time() - start) * 1000
            search_times.append(search_time)
            search_results_counts.append(len(mock_results))

        # ëŒ€ìš©ëŸ‰ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        large_search_times = []
        for _ in range(10):
            start = time.time()
            # Mock ëŒ€ìš©ëŸ‰ ê²€ìƒ‰ (1000ê°œ ë¬¸ì„œì—ì„œ)
            time.sleep(0.05)  # 50ms ì‹œë®¬ë ˆì´ì…˜
            large_search_times.append((time.time() - start) * 1000)

        self.results['search'] = {
            'avg_search_time_ms': statistics.mean(search_times),
            'p95_search_time_ms': np.percentile(search_times, 95),
            'search_throughput_qps': len(search_times) / (sum(search_times) / 1000) if sum(search_times) > 0 else 0,
            'avg_results_per_query': statistics.mean(search_results_counts),
            'large_corpus_avg_ms': statistics.mean(large_search_times)
        }

        self._create_search_performance_graph(search_times, large_search_times)

        print(f"âœ… Search performance: {self.results['search']['avg_search_time_ms']:.2f}ms avg")

    async def benchmark_pipeline_performance(self):
        """ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("âš™ï¸ Testing data pipeline performance...")

        pipeline = DataPipeline()

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data = pd.DataFrame({
            'name': [f'í–¥ë£Œ_{i}' for i in range(1000)],
            'intensity': np.random.uniform(1, 10, 1000),
            'price': np.random.uniform(1, 100, 1000),
            'category': np.random.choice(['citrus', 'floral', 'woody'], 1000),
            'description': [f'í–¥ë£Œ {i}ì— ëŒ€í•œ ì„¤ëª…' for i in range(1000)]
        })

        # ë°ì´í„° ë³€í™˜ ì„±ëŠ¥
        transformations = [
            {'type': 'drop_duplicates', 'params': {}},
            {'type': 'fill_null', 'params': {'value': 'N/A'}},
            {'type': 'normalize_text', 'params': {'columns': ['name', 'description']}}
        ]

        transform_times = []
        for _ in range(10):
            start = time.time()
            transformed_df = await pipeline.transform_data(test_data.copy(), transformations)
            transform_times.append((time.time() - start) * 1000)

        # ë°ì´í„° ê²€ì¦ ì„±ëŠ¥
        schema = {
            'name': {'validation_rules': ['fragrance_name_valid']},
            'intensity': {'validation_rules': ['intensity_range']},
            'price': {'validation_rules': ['price_positive']}
        }

        validation_times = []
        for _ in range(5):
            start = time.time()
            validated_df, quality_metrics = await pipeline.validate_and_clean(test_data.copy(), schema)
            validation_times.append((time.time() - start) * 1000)

        # ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ì„±ëŠ¥
        profiling_times = []
        for _ in range(3):
            start = time.time()
            profile = await pipeline.generate_data_profile(test_data)
            profiling_times.append((time.time() - start) * 1000)

        self.results['pipeline'] = {
            'transform_avg_ms': statistics.mean(transform_times),
            'validation_avg_ms': statistics.mean(validation_times),
            'profiling_avg_ms': statistics.mean(profiling_times),
            'rows_per_second': len(test_data) / (statistics.mean(transform_times) / 1000) if transform_times else 0,
            'total_pipeline_time_ms': statistics.mean(transform_times) + statistics.mean(validation_times)
        }

        self._create_pipeline_performance_graph(transform_times, validation_times, profiling_times)

        print(f"âœ… Pipeline performance: {self.results['pipeline']['transform_avg_ms']:.2f}ms avg transform")

    async def benchmark_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
        print("ğŸ’¾ Testing memory usage...")

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
        large_data = pd.DataFrame({
            'id': range(10000),
            'text': [f'í…ìŠ¤íŠ¸ ë°ì´í„° {i}' * 10 for i in range(10000)],
            'embeddings': [np.random.randn(384).tolist() for _ in range(10000)]
        })

        peak_memory = process.memory_info().rss / 1024 / 1024  # MB

        # ìºì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        await cache_manager.initialize()
        cache_data = {f"cache_key_{i}": f"cache_value_{i}" * 100 for i in range(1000)}

        for key, value in cache_data.items():
            await cache_manager.set(key, value)

        cache_memory = process.memory_info().rss / 1024 / 1024  # MB

        # ì •ë¦¬
        del large_data
        await cache_manager.clear_all_cache()

        final_memory = process.memory_info().rss / 1024 / 1024  # MB

        self.results['memory'] = {
            'initial_memory_mb': initial_memory,
            'peak_memory_mb': peak_memory,
            'cache_memory_mb': cache_memory,
            'final_memory_mb': final_memory,
            'memory_efficiency': (peak_memory - initial_memory) / 10000,  # MB per 10K records
            'memory_leak_check': abs(final_memory - initial_memory) < 10  # Less than 10MB difference
        }

        print(f"âœ… Memory usage: Peak {peak_memory:.1f}MB, Efficiency {self.results['memory']['memory_efficiency']:.3f}MB/10K records")

    async def benchmark_concurrency(self):
        """ë™ì‹œì„± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸš€ Testing concurrency performance...")

        # ë™ì‹œ ìºì‹œ ì‘ì—…
        async def cache_worker(worker_id: int, operations: int):
            times = []
            for i in range(operations):
                start = time.time()
                await cache_manager.set(f"worker_{worker_id}_key_{i}", f"value_{i}")
                await cache_manager.get(f"worker_{worker_id}_key_{i}")
                times.append((time.time() - start) * 1000)
            return times

        # 10ê°œ ì›Œì»¤ë¡œ ë™ì‹œì„± í…ŒìŠ¤íŠ¸
        concurrent_levels = [1, 5, 10, 20]
        concurrency_results = {}

        for level in concurrent_levels:
            start_time = time.time()
            tasks = [cache_worker(i, 50) for i in range(level)]
            results = await asyncio.gather(*tasks)
            total_time = time.time() - start_time

            all_times = [t for worker_times in results for t in worker_times]

            concurrency_results[level] = {
                'total_time_sec': total_time,
                'avg_operation_time_ms': statistics.mean(all_times) if all_times else 0,
                'operations_per_sec': (level * 50 * 2) / total_time if total_time > 0 else 0,  # *2 for set+get
                'total_operations': level * 50 * 2
            }

        self.results['concurrency'] = {
            'results_by_level': concurrency_results,
            'max_throughput_ops_sec': max(r['operations_per_sec'] for r in concurrency_results.values()),
            'optimal_concurrency_level': max(concurrency_results.keys(),
                                           key=lambda x: concurrency_results[x]['operations_per_sec']),
            'scalability_factor': concurrency_results[10]['operations_per_sec'] / concurrency_results[1]['operations_per_sec'] if concurrency_results[1]['operations_per_sec'] > 0 else 0
        }

        self._create_concurrency_performance_graph(concurrency_results)

        print(f"âœ… Concurrency: Max {self.results['concurrency']['max_throughput_ops_sec']:.0f} ops/sec")

    def _create_cache_performance_graph(self, set_times: List[float], get_times: List[float]):
        """ìºì‹œ ì„±ëŠ¥ ê·¸ë˜í”„ ìƒì„±"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # SET ì‹œê°„ ë¶„í¬
        ax1.hist(set_times, bins=50, alpha=0.7, color='blue', edgecolor='black')
        ax1.set_title('Cache SET Operation Time Distribution')
        ax1.set_xlabel('Time (ms)')
        ax1.set_ylabel('Frequency')
        ax1.axvline(statistics.mean(set_times), color='red', linestyle='--', label=f'Avg: {statistics.mean(set_times):.2f}ms')
        ax1.legend()

        # GET ì‹œê°„ ë¶„í¬
        ax2.hist(get_times, bins=50, alpha=0.7, color='green', edgecolor='black')
        ax2.set_title('Cache GET Operation Time Distribution')
        ax2.set_xlabel('Time (ms)')
        ax2.set_ylabel('Frequency')
        ax2.axvline(statistics.mean(get_times), color='red', linestyle='--', label=f'Avg: {statistics.mean(get_times):.2f}ms')
        ax2.legend()

        # SET vs GET ë¹„êµ
        ax3.boxplot([set_times, get_times], labels=['SET', 'GET'])
        ax3.set_title('Cache Operation Performance Comparison')
        ax3.set_ylabel('Time (ms)')

        # ì„±ëŠ¥ í†µê³„
        stats_data = {
            'Operation': ['SET', 'GET'],
            'Average (ms)': [statistics.mean(set_times), statistics.mean(get_times)],
            'P95 (ms)': [np.percentile(set_times, 95), np.percentile(get_times, 95)],
            'P99 (ms)': [np.percentile(set_times, 99), np.percentile(get_times, 99)]
        }

        ax4.axis('tight')
        ax4.axis('off')
        table = ax4.table(cellText=[[f'{v:.2f}' if isinstance(v, float) else v for v in row]
                                  for row in zip(*stats_data.values())],
                         colLabels=list(stats_data.keys()),
                         cellLoc='center',
                         loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        ax4.set_title('Performance Statistics')

        plt.tight_layout()
        plt.savefig(self.graphs_dir / 'cache_performance.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _create_auth_performance_graph(self, user_times: List[float], token_times: List[float], verify_times: List[float]):
        """ì¸ì¦ ì„±ëŠ¥ ê·¸ë˜í”„ ìƒì„±"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # ì¸ì¦ ì‘ì—…ë³„ ì„±ëŠ¥ ë¹„êµ
        operations = ['User Creation', 'Token Creation', 'Token Verification']
        avg_times = [statistics.mean(user_times), statistics.mean(token_times), statistics.mean(verify_times)]

        bars = ax1.bar(operations, avg_times, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        ax1.set_title('Authentication Operations Performance')
        ax1.set_ylabel('Average Time (ms)')

        # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
        for bar, time in zip(bars, avg_times):
            ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,
                    f'{time:.2f}ms', ha='center', va='bottom')

        # ì‚¬ìš©ì ìƒì„± ì‹œê°„ ë¶„í¬
        ax2.hist(user_times, bins=30, alpha=0.7, color='#FF6B6B', edgecolor='black')
        ax2.set_title('User Creation Time Distribution')
        ax2.set_xlabel('Time (ms)')
        ax2.set_ylabel('Frequency')

        # í† í° ì‘ì—… ì‹œê°„ ë¹„êµ
        ax3.boxplot([token_times, verify_times], labels=['Token Creation', 'Token Verification'])
        ax3.set_title('Token Operations Performance')
        ax3.set_ylabel('Time (ms)')

        # ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸”
        auth_stats = {
            'Metric': ['Users Created', 'Avg User Creation (ms)', 'Avg Token Creation (ms)',
                      'Avg Token Verification (ms)', 'Auth Throughput (ops/sec)'],
            'Value': [len(user_times), f'{statistics.mean(user_times):.2f}',
                     f'{statistics.mean(token_times):.2f}', f'{statistics.mean(verify_times):.2f}',
                     f'{self.results["auth"]["auth_throughput_ops_sec"]:.1f}']
        }

        ax4.axis('tight')
        ax4.axis('off')
        table = ax4.table(cellText=list(zip(*auth_stats.values())),
                         colLabels=list(auth_stats.keys()),
                         cellLoc='center',
                         loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        ax4.set_title('Authentication Performance Summary')

        plt.tight_layout()
        plt.savefig(self.graphs_dir / 'auth_performance.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _create_embedding_performance_graph(self, single_times: List[float], batch_results: Dict):
        """ì„ë² ë”© ì„±ëŠ¥ ê·¸ë˜í”„ ìƒì„±"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # ë‹¨ì¼ ì„ë² ë”© ì‹œê°„ ë¶„í¬
        ax1.hist(single_times, bins=30, alpha=0.7, color='#96CEB4', edgecolor='black')
        ax1.set_title('Single Text Embedding Time Distribution')
        ax1.set_xlabel('Time (ms)')
        ax1.set_ylabel('Frequency')

        # ë°°ì¹˜ í¬ê¸°ë³„ ì²˜ë¦¬ëŸ‰
        batch_sizes = list(batch_results.keys())
        throughputs = [batch_results[size]['throughput_texts_sec'] for size in batch_sizes]

        ax2.plot(batch_sizes, throughputs, 'o-', linewidth=2, markersize=8, color='#FFEAA7')
        ax2.set_title('Throughput vs Batch Size')
        ax2.set_xlabel('Batch Size')
        ax2.set_ylabel('Throughput (texts/sec)')
        ax2.grid(True, alpha=0.3)

        # ë°°ì¹˜ í¬ê¸°ë³„ í‰ê·  ì‹œê°„
        avg_times_per_text = [batch_results[size]['time_per_text_ms'] for size in batch_sizes]

        ax3.bar(batch_sizes, avg_times_per_text, color='#DDA0DD', alpha=0.7)
        ax3.set_title('Average Time per Text by Batch Size')
        ax3.set_xlabel('Batch Size')
        ax3.set_ylabel('Time per Text (ms)')

        # ì„±ëŠ¥ ìµœì í™” ì •ë³´
        optimal_batch = self.results['embedding']['optimal_batch_size']
        max_throughput = self.results['embedding']['max_throughput_texts_sec']

        perf_info = {
            'Metric': ['Single Text Avg (ms)', 'Single Text P95 (ms)', 'Optimal Batch Size',
                      'Max Throughput (texts/sec)', 'Efficiency Gain'],
            'Value': [f'{statistics.mean(single_times):.2f}',
                     f'{np.percentile(single_times, 95):.2f}',
                     str(optimal_batch),
                     f'{max_throughput:.1f}',
                     f'{max_throughput / (1000/statistics.mean(single_times)):.1f}x']
        }

        ax4.axis('tight')
        ax4.axis('off')
        table = ax4.table(cellText=list(zip(*perf_info.values())),
                         colLabels=list(perf_info.keys()),
                         cellLoc='center',
                         loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        ax4.set_title('Embedding Performance Optimization')

        plt.tight_layout()
        plt.savefig(self.graphs_dir / 'embedding_performance.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _create_search_performance_graph(self, search_times: List[float], large_search_times: List[float]):
        """ê²€ìƒ‰ ì„±ëŠ¥ ê·¸ë˜í”„ ìƒì„±"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # ê²€ìƒ‰ ì‹œê°„ ë¶„í¬
        ax1.hist(search_times, bins=30, alpha=0.7, color='#74B9FF', edgecolor='black')
        ax1.set_title('Search Response Time Distribution')
        ax1.set_xlabel('Time (ms)')
        ax1.set_ylabel('Frequency')
        ax1.axvline(statistics.mean(search_times), color='red', linestyle='--',
                   label=f'Avg: {statistics.mean(search_times):.2f}ms')
        ax1.legend()

        # ì¼ë°˜ vs ëŒ€ìš©ëŸ‰ ê²€ìƒ‰ ë¹„êµ
        ax2.boxplot([search_times, large_search_times],
                   labels=['Standard Search', 'Large Corpus'])
        ax2.set_title('Search Performance Comparison')
        ax2.set_ylabel('Response Time (ms)')

        # ê²€ìƒ‰ ì²˜ë¦¬ëŸ‰ ì‹œë®¬ë ˆì´ì…˜
        time_windows = list(range(1, 11))  # 1-10ì´ˆ ìœˆë„ìš°
        throughput_simulation = []

        for window in time_windows:
            # í•´ë‹¹ ì‹œê°„ ë‚´ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì¿¼ë¦¬ ìˆ˜ ì‹œë®¬ë ˆì´ì…˜
            queries_per_window = int((window * 1000) / statistics.mean(search_times))
            throughput_simulation.append(queries_per_window)

        ax3.plot(time_windows, throughput_simulation, 'o-', color='#00B894', linewidth=2, markersize=8)
        ax3.set_title('Search Throughput Projection')
        ax3.set_xlabel('Time Window (seconds)')
        ax3.set_ylabel('Queries Processed')
        ax3.grid(True, alpha=0.3)

        # ì„±ëŠ¥ í†µê³„
        search_stats = {
            'Metric': ['Avg Response Time (ms)', 'P95 Response Time (ms)', 'P99 Response Time (ms)',
                      'Throughput (QPS)', 'Large Corpus Avg (ms)'],
            'Value': [f'{statistics.mean(search_times):.2f}',
                     f'{np.percentile(search_times, 95):.2f}',
                     f'{np.percentile(search_times, 99):.2f}',
                     f'{self.results["search"]["search_throughput_qps"]:.1f}',
                     f'{statistics.mean(large_search_times):.2f}']
        }

        ax4.axis('tight')
        ax4.axis('off')
        table = ax4.table(cellText=list(zip(*search_stats.values())),
                         colLabels=list(search_stats.keys()),
                         cellLoc='center',
                         loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        ax4.set_title('Search Performance Statistics')

        plt.tight_layout()
        plt.savefig(self.graphs_dir / 'search_performance.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _create_pipeline_performance_graph(self, transform_times: List[float],
                                         validation_times: List[float], profiling_times: List[float]):
        """íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ê·¸ë˜í”„ ìƒì„±"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ì„±ëŠ¥
        stages = ['Transform', 'Validation', 'Profiling']
        avg_times = [statistics.mean(transform_times), statistics.mean(validation_times),
                    statistics.mean(profiling_times)]

        bars = ax1.bar(stages, avg_times, color=['#FD79A8', '#FDCB6E', '#6C5CE7'])
        ax1.set_title('Data Pipeline Stages Performance')
        ax1.set_ylabel('Average Time (ms)')

        for bar, time in zip(bars, avg_times):
            ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 10,
                    f'{time:.1f}ms', ha='center', va='bottom')

        # ë³€í™˜ ì‹œê°„ ë¶„í¬
        ax2.hist(transform_times, bins=20, alpha=0.7, color='#FD79A8', edgecolor='black')
        ax2.set_title('Data Transform Time Distribution')
        ax2.set_xlabel('Time (ms)')
        ax2.set_ylabel('Frequency')

        # ê²€ì¦ ì‹œê°„ ë¶„í¬
        ax3.hist(validation_times, bins=15, alpha=0.7, color='#FDCB6E', edgecolor='black')
        ax3.set_title('Data Validation Time Distribution')
        ax3.set_xlabel('Time (ms)')
        ax3.set_ylabel('Frequency')

        # ì²˜ë¦¬ëŸ‰ ì •ë³´
        pipeline_stats = {
            'Metric': ['Transform Avg (ms)', 'Validation Avg (ms)', 'Profiling Avg (ms)',
                      'Total Pipeline Time (ms)', 'Rows/Second'],
            'Value': [f'{statistics.mean(transform_times):.1f}',
                     f'{statistics.mean(validation_times):.1f}',
                     f'{statistics.mean(profiling_times):.1f}',
                     f'{self.results["pipeline"]["total_pipeline_time_ms"]:.1f}',
                     f'{self.results["pipeline"]["rows_per_second"]:.0f}']
        }

        ax4.axis('tight')
        ax4.axis('off')
        table = ax4.table(cellText=list(zip(*pipeline_stats.values())),
                         colLabels=list(pipeline_stats.keys()),
                         cellLoc='center',
                         loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        ax4.set_title('Pipeline Performance Summary')

        plt.tight_layout()
        plt.savefig(self.graphs_dir / 'pipeline_performance.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _create_concurrency_performance_graph(self, concurrency_results: Dict):
        """ë™ì‹œì„± ì„±ëŠ¥ ê·¸ë˜í”„ ìƒì„±"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        levels = list(concurrency_results.keys())
        throughputs = [concurrency_results[level]['operations_per_sec'] for level in levels]
        avg_times = [concurrency_results[level]['avg_operation_time_ms'] for level in levels]

        # ë™ì‹œì„± ë ˆë²¨ë³„ ì²˜ë¦¬ëŸ‰
        ax1.plot(levels, throughputs, 'o-', linewidth=3, markersize=10, color='#E17055')
        ax1.set_title('Throughput vs Concurrency Level')
        ax1.set_xlabel('Concurrent Workers')
        ax1.set_ylabel('Operations/Second')
        ax1.grid(True, alpha=0.3)

        # ë™ì‹œì„± ë ˆë²¨ë³„ í‰ê·  ì‘ë‹µ ì‹œê°„
        ax2.plot(levels, avg_times, 'o-', linewidth=3, markersize=10, color='#00B894')
        ax2.set_title('Average Response Time vs Concurrency')
        ax2.set_xlabel('Concurrent Workers')
        ax2.set_ylabel('Average Time (ms)')
        ax2.grid(True, alpha=0.3)

        # í™•ì¥ì„± ë¶„ì„
        scalability = [throughputs[i] / levels[i] for i in range(len(levels))]
        ax3.bar(levels, scalability, color='#A29BFE', alpha=0.7)
        ax3.set_title('Scalability Efficiency (Throughput per Worker)')
        ax3.set_xlabel('Concurrent Workers')
        ax3.set_ylabel('Ops/Sec per Worker')

        # ë™ì‹œì„± ì„±ëŠ¥ ìš”ì•½
        optimal_level = self.results['concurrency']['optimal_concurrency_level']
        max_throughput = self.results['concurrency']['max_throughput_ops_sec']
        scalability_factor = self.results['concurrency']['scalability_factor']

        conc_stats = {
            'Metric': ['Optimal Concurrency', 'Max Throughput (ops/sec)', 'Scalability Factor',
                      'Linear Scale Expected', 'Efficiency at 10 workers'],
            'Value': [str(optimal_level), f'{max_throughput:.0f}', f'{scalability_factor:.2f}x',
                     f'{throughputs[0] * 10:.0f}', f'{(throughputs[-1]/(throughputs[0]*10))*100:.1f}%']
        }

        ax4.axis('tight')
        ax4.axis('off')
        table = ax4.table(cellText=list(zip(*conc_stats.values())),
                         colLabels=list(conc_stats.keys()),
                         cellLoc='center',
                         loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        ax4.set_title('Concurrency Performance Analysis')

        plt.tight_layout()
        plt.savefig(self.graphs_dir / 'concurrency_performance.png', dpi=300, bbox_inches='tight')
        plt.close()

    def save_results(self):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        results_file = self.graphs_dir / 'benchmark_results.json'

        # ê²°ê³¼ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        self.results['benchmark_info'] = {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'system_info': {
                'cpu_count': psutil.cpu_count(),
                'memory_gb': psutil.virtual_memory().total / (1024**3),
                'platform': psutil.os.name
            }
        }

        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“Š Results saved to {results_file}")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    benchmark = PerformanceBenchmark()

    try:
        results = await benchmark.run_all_benchmarks()
        benchmark.save_results()

        # ìš”ì•½ ì¶œë ¥
        print("\n" + "="*50)
        print("ğŸ¯ PERFORMANCE BENCHMARK SUMMARY")
        print("="*50)

        print(f"Cache Performance: {results['cache']['get_avg_ms']:.2f}ms avg GET")
        print(f"Auth Performance: {results['auth']['token_verification_avg_ms']:.2f}ms token verification")
        print(f"Embedding Performance: {results['embedding']['single_embedding_avg_ms']:.2f}ms per text")
        print(f"Search Performance: {results['search']['avg_search_time_ms']:.2f}ms avg search")
        print(f"Pipeline Performance: {results['pipeline']['rows_per_second']:.0f} rows/sec")
        print(f"Memory Efficiency: {results['memory']['memory_efficiency']:.3f}MB per 10K records")
        print(f"Max Concurrency: {results['concurrency']['max_throughput_ops_sec']:.0f} ops/sec")

        return results

    except Exception as e:
        print(f"âŒ Benchmark failed: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())