# Artisan Observability Guide

## ê°œìš”

Artisanì˜ ê´€ì¸¡ì„±(Observability)ì€ **3ê°€ì§€ í•µì‹¬ ìš”ì†Œ**ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

1. **ë¡œê·¸ (Logs)** - êµ¬ì¡°í™”ëœ JSON ë¡œê·¸
2. **ë©”íŠ¸ë¦­ (Metrics)** - Prometheus + Grafana
3. **íŠ¸ë ˆì´ì‹± (Tracing)** - OpenTelemetry

---

## 1. êµ¬ì¡°í™”ëœ JSON ë¡œê¹…

### 1.1 í•µì‹¬ ë¡œê·¸ í‚¤

#### LLM ë¡œê·¸
```json
{
  "timestamp": "2025-10-14T10:30:00.123Z",
  "level": "INFO",
  "component": "LLM",
  "log_type": "llm_brief",
  "mode": "fast",
  "qwen_ok": true,
  "mistral_fix": false,
  "hints": "citrus notes recommended",
  "elapsed_ms": 1847.32,
  "cache_hit": false,
  "brief_style": "fresh",
  "brief_intensity": 0.7
}
```

**í•µì‹¬ í•„ë“œ:**
- `mode` - fast/balanced/creative
- `qwen_ok` - Qwen ëª¨ë¸ ì„±ê³µ ì—¬ë¶€
- `mistral_fix` - Mistral ëª¨ë¸ ë³´ì • ì—¬ë¶€
- `hints` - ìƒì„±ëœ íŒíŠ¸ (ìˆëŠ” ê²½ìš°)
- `elapsed_ms` - ì „ì²´ ì†Œìš” ì‹œê°„

#### RL ë¡œê·¸
```json
{
  "timestamp": "2025-10-14T10:30:00.123Z",
  "level": "INFO",
  "component": "RL",
  "log_type": "rl_update",
  "algo": "PPO",
  "loss": 0.002341,
  "reward": 15.67,
  "entropy": 0.0085,
  "clip_frac": 0.12
}
```

**í•µì‹¬ í•„ë“œ:**
- `algo` - PPO ë˜ëŠ” REINFORCE
- `loss` - ì •ì±… ì†ì‹¤
- `reward` - í‰ê·  ë³´ìƒ
- `entropy` - ì •ì±… ì—”íŠ¸ë¡œí”¼
- `clip_frac` - PPO í´ë¦¬í•‘ ë¹„ìœ¨

#### API ë¡œê·¸
```json
{
  "timestamp": "2025-10-14T10:30:00.123Z",
  "level": "INFO",
  "component": "API",
  "log_type": "api_request",
  "method": "POST",
  "endpoint": "/api/v1/evolve/options",
  "status_code": 200,
  "latency_ms": 2145.67
}
```

**í•µì‹¬ í•„ë“œ:**
- `method` - HTTP ë©”ì„œë“œ
- `endpoint` - API ê²½ë¡œ
- `status_code` - HTTP ìƒíƒœ ì½”ë“œ
- `latency_ms` - ì‘ë‹µ ì§€ì—°ì‹œê°„

### 1.2 ì‚¬ìš©ë²•

```python
from fragrance_ai.observability import llm_logger, rl_logger, orchestrator_logger

# LLM ë¡œê·¸
llm_logger.log_brief(
    user_text="I want a fresh summer fragrance",
    brief={"style": "fresh", "intensity": 0.7},
    model="qwen-2.5-72b",
    mode="fast",
    latency_ms=1847.32,
    qwen_ok=True,
    mistral_fix=False,
    hints="citrus notes recommended"
)

# RL ë¡œê·¸
rl_logger.log_update(
    algorithm="PPO",
    loss=0.002341,
    reward=15.67,
    entropy=0.0085,
    clip_frac=0.12
)

# API ë¡œê·¸
orchestrator_logger.log_api_request(
    method="POST",
    path="/api/v1/evolve/options",
    status_code=200,
    response_time_ms=2145.67
)
```

### 1.3 ë³´ì•ˆ - ë¡œê·¸ ë§ˆìŠ¤í‚¹

**ìë™ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ë˜ëŠ” ì •ë³´:**
- API í‚¤/í† í°
- ì´ë©”ì¼ ì£¼ì†Œ
- ì „í™”ë²ˆí˜¸
- ì‹ ìš©ì¹´ë“œ ë²ˆí˜¸
- AWS í‚¤
- ë°ì´í„°ë² ì´ìŠ¤ ìê²©ì¦ëª…

```python
from fragrance_ai.observability import LogMasker

# ì˜ˆì‹œ
text = "api_key=sk-1234567890abcdef email=user@example.com"
masked = LogMasker.mask_all(text)
# ê²°ê³¼: "api_key=***MASKED*** email=***EMAIL_MASKED***"
```

---

## 2. ë©”íŠ¸ë¦­ (Prometheus + Grafana)

### 2.1 Prometheus ë©”íŠ¸ë¦­

#### LLM ë©”íŠ¸ë¦­
```
# LLM ë¸Œë¦¬í”„ ìƒì„± ì§€ì—°ì‹œê°„
llm_brief_latency_seconds{mode="fast|balanced|creative"}

# LLM ëª¨ë¸ ìƒíƒœ
llm_model_status_total{model="qwen|mistral|llama", status="ok|error|timeout"}

# ìºì‹œ íˆíŠ¸ìœ¨
cache_hits_total{cache_type="llm_brief"}
cache_misses_total{cache_type="llm_brief"}
```

#### RL ë©”íŠ¸ë¦­
```
# RL ì—…ë°ì´íŠ¸ ì¹´ìš´íŠ¸
rl_updates_total{algo="PPO|REINFORCE"}

# RL ë³´ìƒ
rl_reward{algo="PPO|REINFORCE"}

# RL ì†ì‹¤
rl_loss{algo="PPO|REINFORCE"}

# RL ì—”íŠ¸ë¡œí”¼
rl_entropy{algo="PPO|REINFORCE"}
```

#### API ë©”íŠ¸ë¦­
```
# API ìš”ì²­ ì¹´ìš´íŠ¸
api_requests_total{method="GET|POST", endpoint="/api/...", status="200|400|500"}

# API ì‘ë‹µ ì‹œê°„ (íˆìŠ¤í† ê·¸ë¨)
api_response_seconds{method="GET|POST", endpoint="/api/..."}

# p95 ì§€ì—°ì‹œê°„
histogram_quantile(0.95, api_response_seconds)
```

#### ì„œí‚·ë¸Œë ˆì´ì»¤ ë©”íŠ¸ë¦­
```
# ì„œí‚·ë¸Œë ˆì´ì»¤ í´ë°± ì¹´ìš´íŠ¸
circuit_breaker_fallback_total{service="llm", fallback_type="cache|default"}

# ì„œí‚·ë¸Œë ˆì´ì»¤ ë‹¤ìš´ê·¸ë ˆì´ë“œ ì¹´ìš´íŠ¸
circuit_breaker_downgrade_total{service="llm", from_tier="creative", to_tier="balanced"}
```

### 2.2 Grafana ëŒ€ì‹œë³´ë“œ

#### íŒ¨ë„ 1: LLM ì§€ì—°ì‹œê°„ (Modeë³„)
**ì¿¼ë¦¬:**
```promql
rate(llm_brief_latency_seconds_sum{mode="fast"}[5m])
/ rate(llm_brief_latency_seconds_count{mode="fast"}[5m])
```

**ì‹œê°í™”:** Time Series Graph

#### íŒ¨ë„ 2: ìºì‹œ íˆíŠ¸ìœ¨
**ì¿¼ë¦¬:**
```promql
rate(cache_hits_total[5m])
/ (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m]))
```

**ì‹œê°í™”:** Gauge (0-100%)

#### íŒ¨ë„ 3: RL ë³´ìƒ ì¶”ì„¸ (ì•Œê³ ë¦¬ì¦˜ë³„)
**ì¿¼ë¦¬:**
```promql
rl_reward{algo="PPO"}
rl_reward{algo="REINFORCE"}
```

**ì‹œê°í™”:** Time Series Graph

#### íŒ¨ë„ 4: API p95 ì§€ì—°ì‹œê°„
**ì¿¼ë¦¬:**
```promql
histogram_quantile(0.95,
  rate(api_response_seconds_bucket[5m])
)
```

**ì‹œê°í™”:** Gauge + Threshold (2.5s)

#### íŒ¨ë„ 5: ì„œí‚·ë¸Œë ˆì´ì»¤ ì „í™˜ ì¹´ìš´íŠ¸
**ì¿¼ë¦¬:**
```promql
rate(circuit_breaker_fallback_total[5m])
rate(circuit_breaker_downgrade_total[5m])
```

**ì‹œê°í™”:** Bar Chart

### 2.3 Grafana + Loki í†µí•© (JSON ë¡œê·¸)

Lokië¥¼ ì‚¬ìš©í•˜ì—¬ JSON ë¡œê·¸ë¥¼ ì¿¼ë¦¬:

#### LLM ëª¨ë“œë³„ ì§€ì—°ì‹œê°„ (JSON ë¡œê·¸ ê¸°ë°˜)
```logql
{component="LLM", log_type="llm_brief"}
| json
| mode="fast"
| avg_over_time(elapsed_ms) by (mode)
```

#### RL ì•Œê³ ë¦¬ì¦˜ë³„ ë³´ìƒ (JSON ë¡œê·¸ ê¸°ë°˜)
```logql
{component="RL", log_type="rl_update"}
| json
| avg(reward) by (algo)
```

#### API ì—ëŸ¬ìœ¨ (JSON ë¡œê·¸ ê¸°ë°˜)
```logql
sum(rate({component="API", status_code=~"5.."} [5m]))
/ sum(rate({component="API"} [5m]))
```

---

## 3. íŠ¸ë ˆì´ì‹± (OpenTelemetry)

### 3.1 ì„¤ì •

#### ë°©ë²• 1: Jaeger ì‚¬ìš©
```python
from fragrance_ai.tracing import setup_tracing, TracingConfig

config = TracingConfig(
    service_name="fragrance-ai",
    jaeger_endpoint="localhost:6831",  # Jaeger agent
    sample_rate=1.0  # 100% sampling
)

tracer = setup_tracing(config)
```

#### ë°©ë²• 2: OTLP ì‚¬ìš© (Grafana Tempo)
```python
config = TracingConfig(
    service_name="fragrance-ai",
    otlp_endpoint="http://localhost:4317",  # Tempo endpoint
    sample_rate=0.1  # 10% sampling
)

tracer = setup_tracing(config)
```

### 3.2 ì „ì²´ ìš”ì²­ ì¶”ì 

**ì‹œë‚˜ë¦¬ì˜¤:** `/evolve/options` â†’ `/evolve/feedback` ì „ì²´ í”Œë¡œìš°

```python
from fragrance_ai.tracing import artisan_tracer

# Evolution request
with artisan_tracer.trace_evolution(
    experiment_id="exp_123",
    algorithm="PPO",
    num_options=3
):
    # LLM call
    with artisan_tracer.trace_llm_call(
        model="qwen-2.5-72b",
        mode="balanced",
        cache_hit=False
    ):
        brief = llm_service.generate_brief(user_text)

    # RL evolution
    with artisan_tracer.trace_rl_update(
        algorithm="PPO",
        iteration=1
    ):
        options = rl_service.evolve(dna, brief)

# Feedback submission
with artisan_tracer.trace_feedback(
    experiment_id="exp_123",
    chosen_id="pheno_1",
    rating=5
):
    rl_service.submit_feedback(feedback)
```

### 3.3 FastAPI í†µí•©

```python
from fastapi import FastAPI
from fragrance_ai.tracing import create_fastapi_middleware

app = FastAPI()

# Add tracing middleware
app.add_middleware(create_fastapi_middleware())

@app.post("/api/v1/evolve/options")
async def evolve_options(request: EvolutionRequest):
    # ìë™ìœ¼ë¡œ íŠ¸ë ˆì´ìŠ¤ë¨
    return evolution_service.create_options(request)
```

### 3.4 ì¶”ì  ê°€ëŠ¥í•œ ì •ë³´

**ìŠ¤íŒ¬(Span) ì†ì„±:**
- `experiment.id` - ì‹¤í—˜ ID
- `evolution.algorithm` - PPO/REINFORCE
- `llm.model` - LLM ëª¨ë¸ ì´ë¦„
- `llm.mode` - fast/balanced/creative
- `llm.latency_ms` - LLM í˜¸ì¶œ ì§€ì—°ì‹œê°„
- `rl.iteration` - RL ë°˜ë³µ íšŸìˆ˜
- `http.method` - HTTP ë©”ì„œë“œ
- `http.url` - ìš”ì²­ URL
- `http.status_code` - HTTP ìƒíƒœ ì½”ë“œ

**íŠ¸ë ˆì´ìŠ¤ ì˜ˆì‹œ:**
```
[Trace: exp_123]
  â””â”€ evolution.request (2500ms)
      â”œâ”€ llm.call (1800ms)
      â”‚   â””â”€ qwen-2.5-72b inference
      â”œâ”€ rl.update (600ms)
      â”‚   â”œâ”€ policy forward
      â”‚   â”œâ”€ loss calculation
      â”‚   â””â”€ optimizer step
      â””â”€ ga.generation (100ms)
          â”œâ”€ crossover
          â””â”€ mutation
```

---

## 4. ë°°í¬ ë° ì„¤ì •

### 4.1 Docker Compose

```yaml
version: '3.8'

services:
  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'

  # Grafana
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources

  # Loki (JSON ë¡œê·¸)
  loki:
    image: grafana/loki:latest
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml

  # Promtail (ë¡œê·¸ ìˆ˜ì§‘)
  promtail:
    image: grafana/promtail:latest
    volumes:
      - /var/log:/var/log
      - ./promtail-config.yml:/etc/promtail/config.yml
    command: -config.file=/etc/promtail/config.yml

  # Jaeger (íŠ¸ë ˆì´ì‹±)
  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "5775:5775/udp"  # Zipkin thrift compact
      - "6831:6831/udp"  # Jaeger agent
      - "6832:6832/udp"  # Jaeger agent
      - "5778:5778"      # Config
      - "16686:16686"    # UI
      - "14268:14268"    # Collector HTTP
      - "14250:14250"    # Collector gRPC

  # Tempo (íŠ¸ë ˆì´ì‹± - Grafana native)
  tempo:
    image: grafana/tempo:latest
    ports:
      - "3200:3200"   # Tempo
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - ./tempo.yaml:/etc/tempo.yaml

  # Artisan API
  artisan-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - JAEGER_ENDPOINT=jaeger:6831
      - PROMETHEUS_MULTIPROC_DIR=/tmp/prometheus
    depends_on:
      - prometheus
      - loki
      - jaeger
```

### 4.2 Prometheus ì„¤ì • (`prometheus.yml`)

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'fragrance-ai'
    static_configs:
      - targets: ['artisan-api:8000']
    metrics_path: '/metrics'
```

### 4.3 Promtail ì„¤ì • (JSON ë¡œê·¸ ìˆ˜ì§‘)

```yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: fragrance-ai-logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: fragrance-ai
          __path__: /var/log/fragrance-ai/*.log

    pipeline_stages:
      # JSON ë¡œê·¸ íŒŒì‹±
      - json:
          expressions:
            timestamp: timestamp
            level: level
            component: component
            log_type: log_type
            message: message

      # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
      - timestamp:
          source: timestamp
          format: RFC3339Nano

      # ë¼ë²¨ ì¶”ê°€
      - labels:
          level:
          component:
          log_type:
```

### 4.4 í™˜ê²½ ë³€ìˆ˜

```bash
# Tracing
export JAEGER_ENDPOINT=localhost:6831
export OTLP_ENDPOINT=http://localhost:4317

# Prometheus
export PROMETHEUS_MULTIPROC_DIR=/tmp/prometheus

# Artisan
export ARTISAN_ENV=prod
```

---

## 5. ì¿¼ë¦¬ ì˜ˆì‹œ

### 5.1 Grafana ëŒ€ì‹œë³´ë“œ ì¿¼ë¦¬

#### LLM ëª¨ë“œë³„ í‰ê·  ì§€ì—°ì‹œê°„ (Loki)
```logql
avg_over_time(
  {component="LLM", log_type="llm_brief"}
  | json
  | __error__ = ""
  | unwrap elapsed_ms [5m]
) by (mode)
```

#### RL ì•Œê³ ë¦¬ì¦˜ë³„ ì†ì‹¤ ì¶”ì„¸ (Loki)
```logql
{component="RL", log_type="rl_update"}
| json
| line_format "{{.algo}}: loss={{.loss}}"
```

#### API ì—ëŸ¬ìœ¨ (Prometheus)
```promql
sum(rate(api_requests_total{status=~"5.."}[5m]))
/ sum(rate(api_requests_total[5m]))
```

#### p95 ì§€ì—°ì‹œê°„ (Prometheus)
```promql
histogram_quantile(0.95,
  sum(rate(api_response_seconds_bucket[5m])) by (le, endpoint)
)
```

### 5.2 ì•ŒëŒ ê·œì¹™ (Prometheus)

```yaml
groups:
  - name: artisan_alerts
    interval: 30s
    rules:
      # p95 ì§€ì—°ì‹œê°„ ì´ˆê³¼
      - alert: HighAPILatency
        expr: |
          histogram_quantile(0.95,
            rate(api_response_seconds_bucket[5m])
          ) > 2.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "API p95 latency exceeded threshold"
          description: "p95 latency is {{ $value }}s (threshold: 2.5s)"

      # ì—ëŸ¬ìœ¨ ê¸‰ì¦
      - alert: HighErrorRate
        expr: |
          sum(rate(api_requests_total{status=~"5.."}[5m]))
          / sum(rate(api_requests_total[5m])) > 0.01
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High API error rate"
          description: "Error rate is {{ $value | humanizePercentage }}"

      # ì„œí‚·ë¸Œë ˆì´ì»¤ ë‹¤ìš´ê·¸ë ˆì´ë“œ
      - alert: CircuitBreakerDowngrade
        expr: rate(circuit_breaker_downgrade_total[5m]) > 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Circuit breaker downgrade detected"
          description: "LLM service downgraded due to failures"

      # RL ë³´ìƒ ê¸‰ë½
      - alert: RLRewardDrop
        expr: |
          rl_reward < 10.0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "RL reward dropped below threshold"
          description: "Current reward: {{ $value }}"
```

---

## 6. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 6.1 ë¡œê·¸ ë ˆë²¨

- **DEBUG** - ìƒì„¸í•œ ë””ë²„ê¹… ì •ë³´ (dev only)
- **INFO** - ì •ìƒ ì‘ë™ ë¡œê·¸ (ê¸°ë³¸ê°’)
- **WARNING** - ê²½ê³  (ì„±ëŠ¥ ì €í•˜, ì¬ì‹œë„)
- **ERROR** - ì—ëŸ¬ (ë³µêµ¬ ê°€ëŠ¥)
- **CRITICAL** - ì¹˜ëª…ì  ì—ëŸ¬ (ì„œë¹„ìŠ¤ ì¤‘ë‹¨)

### 6.2 ìƒ˜í”Œë§

**í”„ë¡œë•ì…˜ í™˜ê²½:**
- ë¡œê·¸: 100% (ëª¨ë“  ë¡œê·¸ ìˆ˜ì§‘)
- ë©”íŠ¸ë¦­: 100% (ëª¨ë“  ë©”íŠ¸ë¦­ ìˆ˜ì§‘)
- íŠ¸ë ˆì´ì‹±: 1-10% (ìƒ˜í”Œë§ìœ¼ë¡œ ë¶€í•˜ ê°ì†Œ)

### 6.3 ë³´ì•ˆ

- ëª¨ë“  ë¡œê·¸ì— ìë™ ë§ˆìŠ¤í‚¹ ì ìš©
- user_idëŠ” SHA256 í•´ì‹œë¡œ ìµëª…í™”
- API í‚¤/ë¹„ë°€ë²ˆí˜¸ëŠ” ì ˆëŒ€ ë¡œê·¸ì— ê¸°ë¡ ê¸ˆì§€
- PII(ê°œì¸ì‹ë³„ì •ë³´)ëŠ” ìë™ ë§ˆìŠ¤í‚¹

### 6.4 ì„±ëŠ¥

- ë¡œê·¸ëŠ” ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬
- ë©”íŠ¸ë¦­ì€ ì¸ë©”ëª¨ë¦¬ ì¹´ìš´í„° ì‚¬ìš©
- íŠ¸ë ˆì´ì‹±ì€ ìƒ˜í”Œë§ìœ¼ë¡œ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
- Prometheus ìŠ¤í¬ë ˆì´í”„ ê°„ê²©: 15ì´ˆ

---

## 7. ë¬¸ì œ í•´ê²°

### 7.1 ë¡œê·¸ê°€ Lokiì— ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ

**í™•ì¸ ì‚¬í•­:**
1. Promtailì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
2. ë¡œê·¸ íŒŒì¼ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
3. JSON í˜•ì‹ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
4. Loki URLì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸

```bash
# Promtail ë¡œê·¸ í™•ì¸
docker logs promtail

# Loki ìƒíƒœ í™•ì¸
curl http://localhost:3100/ready
```

### 7.2 ë©”íŠ¸ë¦­ì´ Prometheusì— ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ

**í™•ì¸ ì‚¬í•­:**
1. `/metrics` ì—”ë“œí¬ì¸íŠ¸ ì ‘ê·¼ ê°€ëŠ¥ í™•ì¸
2. Prometheus ìŠ¤í¬ë ˆì´í”„ ì„¤ì • í™•ì¸
3. ë©”íŠ¸ë¦­ ì´ë¦„ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸

```bash
# ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
curl http://localhost:8000/metrics

# Prometheus íƒ€ê²Ÿ ìƒíƒœ í™•ì¸
# http://localhost:9090/targets
```

### 7.3 íŠ¸ë ˆì´ìŠ¤ê°€ Jaegerì— ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ

**í™•ì¸ ì‚¬í•­:**
1. Jaeger ì—”ë“œí¬ì¸íŠ¸ ì„¤ì • í™•ì¸
2. OpenTelemetry ì„¤ì¹˜ í™•ì¸
3. íŠ¸ë ˆì´ì„œ ì´ˆê¸°í™” í™•ì¸

```bash
# Jaeger UI í™•ì¸
# http://localhost:16686

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
echo $JAEGER_ENDPOINT
```

---

## 8. ìš”ì•½

| ìš”ì†Œ | ë„êµ¬ | í¬íŠ¸ | ìš©ë„ |
|------|------|------|------|
| **ë¡œê·¸** | Loki + Promtail | 3100 | JSON ë¡œê·¸ ìˆ˜ì§‘/ì¿¼ë¦¬ |
| **ë©”íŠ¸ë¦­** | Prometheus | 9090 | ì‹œê³„ì—´ ë©”íŠ¸ë¦­ |
| **ëŒ€ì‹œë³´ë“œ** | Grafana | 3000 | ì‹œê°í™” |
| **íŠ¸ë ˆì´ì‹±** | Jaeger | 16686 | ë¶„ì‚° ì¶”ì  |
| **íŠ¸ë ˆì´ì‹±** | Tempo | 3200 | Grafana native ì¶”ì  |

**í•µì‹¬ ë¡œê·¸ í‚¤:**
- `llm_brief{mode, qwen_ok, mistral_fix, hints, elapsed_ms}`
- `rl_update{algo, loss, reward, entropy, clip_frac}`
- `api_request{status_code, latency_ms, endpoint}`

**ê´€ì¸¡ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- âœ… ëª¨ë“  ë¡œê·¸ëŠ” JSON í˜•ì‹
- âœ… ë¯¼ê° ì •ë³´ëŠ” ìë™ ë§ˆìŠ¤í‚¹
- âœ… ë©”íŠ¸ë¦­ì€ Prometheusë¡œ ìˆ˜ì§‘
- âœ… Grafana ëŒ€ì‹œë³´ë“œë¡œ ì‹œê°í™”
- âœ… OpenTelemetryë¡œ ì „ì²´ ìš”ì²­ ì¶”ì 
- âœ… ì•ŒëŒ ê·œì¹™ ì„¤ì • (p95, ì—ëŸ¬ìœ¨)

---

**Artisanì€ ì™„ì „í•œ ê´€ì¸¡ì„±ì„ í†µí•´ í”„ë¡œë•ì…˜ ì•ˆì •ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤!** ğŸ“Š
