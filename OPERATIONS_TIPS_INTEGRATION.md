# ìš´ì˜ íŒ ì ìš© ê°€ì´ë“œ

ì‹¤ë¬´ í™˜ê²½ì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ìš´ì˜ ì„¤ì •ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

## 1. í•™ìŠµ ìŠ¤ì¼€ì¤„

### Entropy Coefficient Decay (ì„ í˜•/ì½”ì‚¬ì¸)

```python
from fragrance_ai.config.operations_config import OperationsManager

# ìš´ì˜ ê´€ë¦¬ì ì´ˆê¸°í™”
ops_manager = OperationsManager(checkpoint_dir="./checkpoints")

# í•™ìŠµ ë£¨í”„ì—ì„œ ì‚¬ìš©
for step in range(10000):
    # í˜„ì¬ ìŠ¤í…ì˜ entropy coefficient ìë™ ê³„ì‚°
    result = ops_manager.on_training_step(
        step=step,
        metrics={"loss": loss, "reward": reward},
        model_state=trainer.network.state_dict()
    )

    # PPO ì—…ë°ì´íŠ¸ ì‹œ ì‚¬ìš©
    current_entropy_coef = result["entropy_coef"]  # 0.01 -> 0.001ë¡œ decay
    trainer.entropy_coef = current_entropy_coef
```

**ì„¤ì •:**
- ì´ˆê¸°: 0.01 (exploration)
- ìµœì¢…: 0.001 (exploitation)
- Decay: cosine (ê¸°ë³¸) ë˜ëŠ” linear
- ê¸°ê°„: 10,000 steps

### Reward Normalization (1k step ì°½)

```python
# ìë™ìœ¼ë¡œ 1k step ì°½ì—ì„œ reward normalize
result = ops_manager.on_training_step(step, metrics, model_state)

# Normalized reward ì‚¬ìš©
normalized_reward = result["metrics"]["normalized_reward"]

# í†µê³„ í™•ì¸
stats = ops_manager.reward_normalizer.get_stats()
print(f"Reward mean: {stats['mean']:.3f}, std: {stats['std']:.3f}")
```

**íš¨ê³¼:**
- ì•ˆì •ì ì¸ í•™ìŠµ
- Outlier ì™„í™”
- 1,000 step ìŠ¬ë¼ì´ë”© ìœˆë„ìš°


## 2. ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬

### 500 Stepë§ˆë‹¤ ìë™ ì €ì¥

```python
# ìë™ ì €ì¥ - ë³„ë„ ì½”ë“œ ë¶ˆí•„ìš”
result = ops_manager.on_training_step(step, metrics, model_state)

if result["checkpoint_path"]:
    print(f"âœ“ Checkpoint saved: {result['checkpoint_path']}")
```

**ì„¤ì •:**
- ì €ì¥ ì£¼ê¸°: 500 steps
- ìµœëŒ€ ë³´ê´€: 5ê°œ (ì˜¤ë˜ëœ ê²ƒ ìë™ ì‚­ì œ)

### ì´ìƒ ê°ì§€ & ìë™ ë¡¤ë°±

```python
# ì´ìƒ ê°ì§€
result = ops_manager.on_training_step(step, metrics, model_state)

if result["anomaly"]:
    print(f"âŒ Anomaly detected: {result['anomaly']}")

    # ìë™ ë¡¤ë°±
    if result["rollback_path"]:
        trainer.load_checkpoint(result["rollback_path"])
        print(f"ğŸ”„ Rolled back to: {result['rollback_path']}")
```

**ì„ê³„ê°’:**
- Loss spike: ì´ì „ ëŒ€ë¹„ 2ë°° ì´ìƒ
- Reward drop: ì´ì „ ëŒ€ë¹„ 50% ì´í•˜


## 3. ì¥ì•  ëŒ€ì²˜ (Circuit Breaker)

### Qwen ì¥ì•  â†’ ìë™ ë‹¤ìš´ì‹œí”„íŠ¸

```python
# LLM í˜¸ì¶œ í›„ ê²°ê³¼ ê¸°ë¡
try:
    result = qwen_model.generate(prompt)
    ops_manager.on_llm_success("qwen")
except Exception as e:
    ops_manager.on_llm_failure("qwen")

# ë‹¤ìš´ì‹œí”„íŠ¸ í™•ì¸
if ops_manager.circuit_breaker.should_use_downshift():
    # creative -> balanced ë˜ëŠ” fastë¡œ ìë™ ì „í™˜
    mode = "balanced"  # ë˜ëŠ” "fast"
    print(f"ğŸ”» Using downshift mode: {mode}")
```

**ì„¤ì •:**
- ì‹¤íŒ¨ ì„ê³„ê°’: 3íšŒ
- ë‹¤ìš´ì‹œí”„íŠ¸ ìœ ì§€: 5ë¶„
- ìë™ ë³µêµ¬: 5ë¶„ í›„

### TTL ë‹¨ì¶•ìœ¼ë¡œ ì¬ì‹œë„ í­ ì¤„ì´ê¸°

```python
# í˜„ì¬ TTL í™•ì¸
current_ttl = ops_manager.circuit_breaker.get_current_ttl()

# Cache ì„¤ì • ì‹œ ì‚¬ìš©
cache.set(key, value, ttl=current_ttl)
```

**ì„¤ì •:**
- ì •ìƒ TTL: 3600ì´ˆ (1ì‹œê°„)
- ë‹¨ì¶• TTL: 300ì´ˆ (5ë¶„)
- ìë™ ì „í™˜: ë‹¤ìš´ì‹œí”„íŠ¸ ì‹œ


## 4. ë°ì´í„° ê¸°ë°˜ ì¬íŠœë‹

### 100-300ê±´ í”¼ë“œë°± ìˆ˜ì§‘ í›„ ìë™ ì¬íŠœë‹

```python
# ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘
feedback = {
    "rating": 4.5,
    "user_id": "user_123",
    "formula_id": "F001"
}

suggestions = ops_manager.on_user_feedback(feedback)

# ì¬íŠœë‹ ì œì•ˆ
if suggestions:
    print("ğŸ“Š Hyperparameter retuning suggestions:")

    # Learning rate ì¡°ì •
    if "learning_rate" in suggestions:
        trainer.optimizer.param_groups[0]['lr'] = suggestions["learning_rate"]

    # Entropy coefficient ì¡°ì •
    if "entropy_coef" in suggestions:
        trainer.entropy_coef = suggestions["entropy_coef"]

    # Clip epsilon ì¡°ì •
    if "clip_epsilon" in suggestions:
        trainer.clip_epsilon = suggestions["clip_epsilon"]
```

**ì¬íŠœë‹ ë¡œì§:**
- í‰ê·  rating < 3.0 â†’ entropy_coef â†‘ (exploration)
- rating ë³€ë™ > 1.5 â†’ learning_rate â†“ (stability)
- í‰ê·  rating â‰¥ 4.0 â†’ clip_epsilon â†“ (exploitation)


## í†µí•© ì˜ˆì‹œ

### PPO í•™ìŠµ ë£¨í”„ì— ì ìš©

```python
from fragrance_ai.config.operations_config import OperationsManager
from fragrance_ai.training.ppo_engine import PPOTrainer

# ì´ˆê¸°í™”
ops_manager = OperationsManager(checkpoint_dir="./checkpoints")
trainer = PPOTrainer(state_dim=40, action_dim=60)

# í•™ìŠµ ë£¨í”„
for step in range(10000):
    # 1. ê²½í—˜ ìˆ˜ì§‘
    avg_reward = trainer.collect_rollout(env, n_steps=2048)

    # 2. PPO ì—…ë°ì´íŠ¸
    train_stats = trainer.train_step(n_epochs=10, batch_size=64)

    # 3. ìš´ì˜ ê´€ë¦¬ (ìë™ ì²˜ë¦¬)
    ops_result = ops_manager.on_training_step(
        step=step,
        metrics={
            "loss": train_stats["policy_loss"],
            "reward": avg_reward
        },
        model_state=trainer.network.state_dict()
    )

    # 4. Entropy coefficient ìë™ ì¡°ì •
    trainer.entropy_coef = ops_result["entropy_coef"]

    # 5. ì´ìƒ ê°ì§€ & ë¡¤ë°±
    if ops_result["rollback_path"]:
        trainer.load_checkpoint(ops_result["rollback_path"])
        print(f"ğŸ”„ Rolled back due to: {ops_result['anomaly']}")

    # 6. ë‹¤ìš´ì‹œí”„íŠ¸ í™•ì¸
    if ops_result["should_downshift"]:
        print("ğŸ”» Using downshift mode for LLM")

    # ë¡œê¹…
    if step % 100 == 0:
        print(f"Step {step}:")
        print(f"  Entropy coef: {ops_result['entropy_coef']:.4f}")
        print(f"  Reward: {avg_reward:.3f}")
        print(f"  TTL: {ops_result['current_ttl']}s")
```


## ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ê°œë³„ ì„¤ì • ë³€ê²½

```python
from fragrance_ai.config.operations_config import (
    LearningScheduleConfig,
    CheckpointConfig,
    CircuitBreakerConfig,
    RetuningConfig
)

# í•™ìŠµ ìŠ¤ì¼€ì¤„ ì»¤ìŠ¤í„°ë§ˆì´ì§•
schedule_config = LearningScheduleConfig(
    entropy_coef_start=0.02,  # ì´ˆê¸°ê°’ ì¦ê°€
    entropy_coef_end=0.0005,  # ìµœì¢…ê°’ ê°ì†Œ
    entropy_decay_type="linear",  # cosine -> linear
    reward_norm_window=2000  # 1k -> 2k
)

# ì²´í¬í¬ì¸íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•
checkpoint_config = CheckpointConfig(
    save_interval_steps=1000,  # 500 -> 1000
    loss_spike_threshold=1.5,  # 2.0 -> 1.5 (ë” ë¯¼ê°í•˜ê²Œ)
    enable_auto_rollback=True
)

# ì ìš©
ops_manager = OperationsManager(checkpoint_dir="./checkpoints")
ops_manager.schedule_config = schedule_config
ops_manager.checkpoint_config = checkpoint_config
```


## ëª¨ë‹ˆí„°ë§

### Prometheus ë©”íŠ¸ë¦­ê³¼ ì—°ë™

```python
from fragrance_ai.monitoring.operations_metrics import OperationsMetricsCollector

collector = OperationsMetricsCollector()

# í•™ìŠµ ìŠ¤í…ë§ˆë‹¤ ê¸°ë¡
ops_result = ops_manager.on_training_step(step, metrics, model_state)

# RL ë©”íŠ¸ë¦­ ê¸°ë¡
collector.record_rl_update(
    algorithm="ppo",
    policy_loss=train_stats["policy_loss"],
    value_loss=train_stats["value_loss"],
    reward=avg_reward,
    entropy=ops_result["entropy_coef"]
)

# ì„œí‚·ë¸Œë ˆì´ì»¤ ë©”íŠ¸ë¦­ ê¸°ë¡
if ops_result["should_downshift"]:
    collector.record_circuit_breaker_downgrade("llm", "creative", "balanced")

# ìºì‹œ TTL ë©”íŠ¸ë¦­
collector.set_cache_size("llm", cache_size)
```


## ì£¼ìš” ì´ì 

### 1. í•™ìŠµ ì•ˆì •ì„±
- âœ… Entropy decayë¡œ exploration â†’ exploitation ìì—°ìŠ¤ëŸ¬ìš´ ì „í™˜
- âœ… Reward normalizationìœ¼ë¡œ í•™ìŠµ ì•ˆì •í™”
- âœ… ìë™ ì´ìƒ ê°ì§€ & ë¡¤ë°±

### 2. ì¥ì•  ë³µì›ë ¥
- âœ… Qwen ì¥ì•  ì‹œ ìë™ ë‹¤ìš´ì‹œí”„íŠ¸ (5ë¶„ê°„)
- âœ… TTL ë‹¨ì¶•ìœ¼ë¡œ ì¬ì‹œë„ ë¹ˆë„ ê°ì†Œ
- âœ… ì²´í¬í¬ì¸íŠ¸ ìë™ ì €ì¥ & ë³µêµ¬

### 3. ë°ì´í„° ê¸°ë°˜ ìµœì í™”
- âœ… 100-300ê±´ í”¼ë“œë°±ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ì¬íŠœë‹
- âœ… ì‚¬ìš©ì í‰ê°€ íŒ¨í„´ì— ë§ì¶˜ ë™ì  ì¡°ì •
- âœ… ì§€ì†ì  ê°œì„  (Continuous Learning)


## í…ŒìŠ¤íŠ¸

```bash
# ìš´ì˜ ì„¤ì • í…ŒìŠ¤íŠ¸
cd "C:\Users\user\Desktop\ìƒˆ í´ë” (2)\Newss"
python -m fragrance_ai.config.operations_config
```

**ì˜ˆìƒ ì¶œë ¥:**
```
=== ìš´ì˜ ì„¤ì • ì‹œë®¬ë ˆì´ì…˜ ===

1. í•™ìŠµ ìŠ¤í… ì‹œë®¬ë ˆì´ì…˜
  Step 500: Checkpoint saved
  Step 1000: Checkpoint saved

2. Qwen ì¥ì•  ì‹œë®¬ë ˆì´ì…˜
  Failure 3: Downshift activated!

3. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ë° ì¬íŠœë‹
  âœ“ Retuning triggered after 100 samples
  Suggestions: {'entropy_coef': 0.02, 'learning_rate': 0.0001}

ì™„ë£Œ!
```


## ìš”ì•½

| ê¸°ëŠ¥ | ì„¤ì • ê°’ | íš¨ê³¼ |
|------|---------|------|
| **Entropy Decay** | 0.01 â†’ 0.001 (cosine) | Exploration â†’ Exploitation |
| **Reward Norm** | 1k step ì°½ | í•™ìŠµ ì•ˆì •í™” |
| **Checkpoint** | 500 step ê°„ê²© | ìë™ ì €ì¥ & ë¡¤ë°± |
| **Circuit Breaker** | 3íšŒ ì‹¤íŒ¨ â†’ ë‹¤ìš´ì‹œí”„íŠ¸ | ì¥ì•  ë³µì›ë ¥ |
| **TTL ë‹¨ì¶•** | 1h â†’ 5min | ì¬ì‹œë„ í­ ê°ì†Œ |
| **ì¬íŠœë‹** | 100-300ê±´ í”¼ë“œë°± | ë™ì  ìµœì í™” |

**ëª¨ë“  ê¸°ëŠ¥ì´ ìë™ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤. ë³„ë„ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”!** ğŸš€
