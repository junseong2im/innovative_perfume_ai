#!/usr/bin/env python3
"""
Fragrance AI ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
ê³¼í•™ì ì´ê³  ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì •ì„ ìœ„í•œ ì¢…í•©ì ì¸ ë²¤ì¹˜ë§ˆí¬ ë„êµ¬
"""

import asyncio
import time
import statistics
import numpy as np
from pathlib import Path
import sys
import json
import logging
from datetime import datetime
from typing import List, Dict, Any, Tuple
import argparse
import psutil
import torch
import gc
from dataclasses import dataclass
from contextlib import contextmanager
import matplotlib.pyplot as plt
import seaborn as sns

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from fragrance_ai.core.vector_store import VectorStore
from fragrance_ai.models.embedding import FragranceEmbedding
from fragrance_ai.models.generator import FragranceGenerator
from fragrance_ai.services.search_service import SearchService
from fragrance_ai.services.generation_service import GenerationService
from fragrance_ai.evaluation.mathematical_metrics import MathematicalMetrics
from fragrance_ai.utils.data_loader import DatasetLoader

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    name: str
    mean_time: float
    std_time: float
    min_time: float
    max_time: float
    p95_time: float
    p99_time: float
    throughput: float
    memory_usage_mb: float
    cpu_usage_percent: float
    success_rate: float
    error_count: int
    sample_size: int
    confidence_interval: Tuple[float, float]

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir: str = "./benchmark_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥
        self.results = {}
        
        # ì‹œìŠ¤í…œ ì •ë³´
        self.system_info = self._collect_system_info()
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°ê¸°
        self.metrics_calculator = MathematicalMetrics(confidence_level=0.95)
        
    def _collect_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        return {
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / (1024**3),
            "python_version": sys.version,
            "torch_version": torch.__version__ if torch.cuda.is_available() else "CPU",
            "cuda_available": torch.cuda.is_available(),
            "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "N/A",
            "timestamp": datetime.now().isoformat()
        }
    
    @contextmanager
    def _monitor_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
        process = psutil.Process()
        
        # ì‹œì‘ ìƒíƒœ
        start_memory = process.memory_info().rss / (1024 * 1024)  # MB
        start_cpu_time = process.cpu_times()
        start_time = time.time()
        
        # GC ì‹¤í–‰
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        yield
        
        # ì¢…ë£Œ ìƒíƒœ
        end_time = time.time()
        end_memory = process.memory_info().rss / (1024 * 1024)  # MB
        end_cpu_time = process.cpu_times()
        
        # CPU ì‚¬ìš©ë¥  ê³„ì‚°
        elapsed_time = end_time - start_time
        cpu_time_used = (end_cpu_time.user - start_cpu_time.user) + (end_cpu_time.system - start_cpu_time.system)
        cpu_usage = (cpu_time_used / elapsed_time) * 100 if elapsed_time > 0 else 0
        
        self._current_memory_usage = end_memory - start_memory
        self._current_cpu_usage = cpu_usage
    
    async def benchmark_embedding_model(
        self, 
        model_path: str = None,
        sample_sizes: List[int] = [1, 10, 50, 100, 500],
        iterations: int = 10
    ) -> Dict[str, BenchmarkResult]:
        """ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        logger.info("Starting embedding model benchmark...")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        embedding_model = FragranceEmbedding()
        if model_path:
            embedding_model.load_model(model_path)
        else:
            await embedding_model.initialize()
        
        results = {}
        
        for sample_size in sample_sizes:
            logger.info(f"Benchmarking embedding with sample_size={sample_size}")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            test_texts = [
                f"ìƒí¼í•œ ì‹œíŠ¸ëŸ¬ìŠ¤ í–¥ìˆ˜ {i}ë²ˆì§¸ í…ŒìŠ¤íŠ¸" 
                for i in range(sample_size)
            ]
            
            # ì„±ëŠ¥ ì¸¡ì •
            times = []
            errors = 0
            
            for iteration in range(iterations):
                with self._monitor_resources():
                    start_time = time.perf_counter()
                    
                    try:
                        if sample_size == 1:
                            # ë‹¨ì¼ ì¸ì½”ë”©
                            _ = await embedding_model.encode_query(test_texts[0])
                        else:
                            # ë°°ì¹˜ ì¸ì½”ë”©
                            _ = await embedding_model.encode_batch(test_texts)
                        
                        end_time = time.perf_counter()
                        times.append(end_time - start_time)
                        
                    except Exception as e:
                        logger.error(f"Error in embedding benchmark: {e}")
                        errors += 1
                        times.append(float('inf'))
            
            # í†µê³„ ê³„ì‚°
            valid_times = [t for t in times if t != float('inf')]
            
            if valid_times:
                result = self._calculate_benchmark_stats(
                    name=f"embedding_batch_{sample_size}",
                    times=valid_times,
                    sample_size=sample_size,
                    errors=errors,
                    total_iterations=iterations
                )
                results[f"batch_{sample_size}"] = result
        
        return results
    
    async def benchmark_generation_model(
        self,
        model_path: str = None,
        generation_types: List[str] = ["basic_recipe", "detailed_recipe"],
        iterations: int = 5
    ) -> Dict[str, BenchmarkResult]:
        """ìƒì„± ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        logger.info("Starting generation model benchmark...")
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        generation_service = GenerationService()
        await generation_service.initialize()
        
        results = {}
        
        test_requests = {
            "basic_recipe": {
                "fragrance_family": "floral",
                "mood": "romantic",
                "intensity": "moderate",
                "generation_type": "basic_recipe"
            },
            "detailed_recipe": {
                "fragrance_family": "woody",
                "mood": "sophisticated", 
                "intensity": "strong",
                "generation_type": "detailed_recipe"
            }
        }
        
        for gen_type in generation_types:
            logger.info(f"Benchmarking generation type: {gen_type}")
            
            times = []
            errors = 0
            quality_scores = []
            
            for iteration in range(iterations):
                with self._monitor_resources():
                    start_time = time.perf_counter()
                    
                    try:
                        result = await generation_service.generate_recipe(
                            request_data=test_requests[gen_type],
                            use_cache=False  # ìºì‹œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                        )
                        
                        end_time = time.perf_counter()
                        times.append(end_time - start_time)
                        
                        # í’ˆì§ˆ ì ìˆ˜ ìˆ˜ì§‘
                        quality_scores.append(result.get("quality_score", 0))
                        
                    except Exception as e:
                        logger.error(f"Error in generation benchmark: {e}")
                        errors += 1
                        times.append(float('inf'))
            
            # í†µê³„ ê³„ì‚°
            valid_times = [t for t in times if t != float('inf')]
            
            if valid_times:
                result = self._calculate_benchmark_stats(
                    name=f"generation_{gen_type}",
                    times=valid_times,
                    sample_size=1,  # í•œ ë²ˆì— í•˜ë‚˜ì”© ìƒì„±
                    errors=errors,
                    total_iterations=iterations
                )
                
                # í’ˆì§ˆ ì ìˆ˜ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                if quality_scores:
                    result.metadata = {
                        "avg_quality_score": statistics.mean(quality_scores),
                        "std_quality_score": statistics.stdev(quality_scores) if len(quality_scores) > 1 else 0
                    }
                
                results[gen_type] = result
        
        return results
    
    async def benchmark_search_service(
        self,
        sample_queries: List[str] = None,
        iterations: int = 10
    ) -> Dict[str, BenchmarkResult]:
        """ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        logger.info("Starting search service benchmark...")
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        search_service = SearchService()
        await search_service.initialize()
        
        # ê¸°ë³¸ ì¿¼ë¦¬ ì„¤ì •
        if not sample_queries:
            sample_queries = [
                "ìƒí¼í•œ ì‹œíŠ¸ëŸ¬ìŠ¤ í–¥ìˆ˜",
                "ë¡œë§¨í‹±í•œ í”Œë¡œëŸ´ í–¥ìˆ˜", 
                "ê¹Šì€ ìš°ë”” í–¥ìˆ˜",
                "ì‹ ì„ í•œ ë´„ í–¥ìˆ˜",
                "ì„¸ë ¨ëœ ì €ë… í–¥ìˆ˜"
            ]
        
        results = {}
        
        # ë‹¨ì¼ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
        logger.info("Benchmarking single query search...")
        times = []
        errors = 0
        result_counts = []
        
        for iteration in range(iterations):
            query = sample_queries[iteration % len(sample_queries)]
            
            with self._monitor_resources():
                start_time = time.perf_counter()
                
                try:
                    result = await search_service.semantic_search(
                        query=query,
                        top_k=10,
                        use_cache=False
                    )
                    
                    end_time = time.perf_counter()
                    times.append(end_time - start_time)
                    result_counts.append(len(result.get("results", [])))
                    
                except Exception as e:
                    logger.error(f"Error in search benchmark: {e}")
                    errors += 1
                    times.append(float('inf'))
        
        # í†µê³„ ê³„ì‚°
        valid_times = [t for t in times if t != float('inf')]
        
        if valid_times:
            result = self._calculate_benchmark_stats(
                name="search_single_query",
                times=valid_times,
                sample_size=1,
                errors=errors,
                total_iterations=iterations
            )
            
            result.metadata = {
                "avg_results_returned": statistics.mean(result_counts) if result_counts else 0
            }
            
            results["single_query"] = result
        
        return results
    
    async def benchmark_end_to_end(
        self,
        scenarios: List[Dict[str, Any]] = None,
        iterations: int = 3
    ) -> Dict[str, BenchmarkResult]:
        """End-to-end ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        logger.info("Starting end-to-end benchmark...")
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        search_service = SearchService()
        generation_service = GenerationService()
        await search_service.initialize()
        await generation_service.initialize()
        
        # ê¸°ë³¸ ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •
        if not scenarios:
            scenarios = [
                {
                    "search_query": "ë´„ì— ì–´ìš¸ë¦¬ëŠ” ìƒí¼í•œ í–¥ìˆ˜",
                    "generation_params": {
                        "fragrance_family": "citrus",
                        "mood": "fresh",
                        "intensity": "light"
                    }
                }
            ]
        
        results = {}
        
        for i, scenario in enumerate(scenarios):
            logger.info(f"Benchmarking E2E scenario {i+1}")
            
            times = []
            errors = 0
            
            for iteration in range(iterations):
                with self._monitor_resources():
                    start_time = time.perf_counter()
                    
                    try:
                        # 1. ê²€ìƒ‰
                        search_result = await search_service.semantic_search(
                            query=scenario["search_query"],
                            top_k=3,
                            use_cache=False
                        )
                        
                        # 2. ìƒì„±
                        generation_result = await generation_service.generate_recipe(
                            request_data=scenario["generation_params"],
                            use_cache=False
                        )
                        
                        end_time = time.perf_counter()
                        times.append(end_time - start_time)
                        
                    except Exception as e:
                        logger.error(f"Error in E2E benchmark: {e}")
                        errors += 1
                        times.append(float('inf'))
            
            # í†µê³„ ê³„ì‚°
            valid_times = [t for t in times if t != float('inf')]
            
            if valid_times:
                result = self._calculate_benchmark_stats(
                    name=f"e2e_scenario_{i+1}",
                    times=valid_times,
                    sample_size=1,
                    errors=errors,
                    total_iterations=iterations
                )
                
                results[f"scenario_{i+1}"] = result
        
        return results
    
    def _calculate_benchmark_stats(
        self,
        name: str,
        times: List[float],
        sample_size: int,
        errors: int,
        total_iterations: int
    ) -> BenchmarkResult:
        """ë²¤ì¹˜ë§ˆí¬ í†µê³„ ê³„ì‚°"""
        if not times:
            # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
            return BenchmarkResult(
                name=name,
                mean_time=float('inf'),
                std_time=0,
                min_time=float('inf'),
                max_time=float('inf'),
                p95_time=float('inf'),
                p99_time=float('inf'),
                throughput=0,
                memory_usage_mb=getattr(self, '_current_memory_usage', 0),
                cpu_usage_percent=getattr(self, '_current_cpu_usage', 0),
                success_rate=0,
                error_count=errors,
                sample_size=sample_size,
                confidence_interval=(float('inf'), float('inf'))
            )
        
        # ê¸°ë³¸ í†µê³„
        mean_time = statistics.mean(times)
        std_time = statistics.stdev(times) if len(times) > 1 else 0
        min_time = min(times)
        max_time = max(times)
        
        # ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°
        p95_time = np.percentile(times, 95)
        p99_time = np.percentile(times, 99)
        
        # ì²˜ë¦¬ëŸ‰ ê³„ì‚° (ì´ˆë‹¹ ì²˜ë¦¬ ê±´ìˆ˜)
        throughput = sample_size / mean_time if mean_time > 0 else 0
        
        # ì„±ê³µë¥  ê³„ì‚°
        success_rate = len(times) / total_iterations
        
        # ì‹ ë¢°êµ¬ê°„ ê³„ì‚° (95%)
        if len(times) > 1:
            confidence_interval = self._calculate_confidence_interval(times, 0.95)
        else:
            confidence_interval = (mean_time, mean_time)
        
        return BenchmarkResult(
            name=name,
            mean_time=mean_time,
            std_time=std_time,
            min_time=min_time,
            max_time=max_time,
            p95_time=p95_time,
            p99_time=p99_time,
            throughput=throughput,
            memory_usage_mb=getattr(self, '_current_memory_usage', 0),
            cpu_usage_percent=getattr(self, '_current_cpu_usage', 0),
            success_rate=success_rate,
            error_count=errors,
            sample_size=sample_size,
            confidence_interval=confidence_interval
        )
    
    def _calculate_confidence_interval(
        self, 
        data: List[float], 
        confidence_level: float = 0.95
    ) -> Tuple[float, float]:
        """ì‹ ë¢°êµ¬ê°„ ê³„ì‚°"""
        n = len(data)
        mean = statistics.mean(data)
        std_err = statistics.stdev(data) / (n ** 0.5)
        
        # t-ë¶„í¬ ì„ê³„ê°’ (ê·¼ì‚¬ê°’)
        alpha = 1 - confidence_level
        if n > 30:
            t_critical = 1.96  # ì •ê·œë¶„í¬ ê·¼ì‚¬
        else:
            # ê°„ë‹¨í•œ t-ë¶„í¬ ê·¼ì‚¬ê°’ë“¤
            t_values = {1: 12.706, 2: 4.303, 3: 3.182, 4: 2.776, 5: 2.571,
                       10: 2.228, 15: 2.131, 20: 2.086, 25: 2.060, 30: 2.042}
            t_critical = t_values.get(n, 2.0)
        
        margin_of_error = t_critical * std_err
        
        return (mean - margin_of_error, mean + margin_of_error)
    
    def generate_report(self, all_results: Dict[str, Dict[str, BenchmarkResult]]) -> str:
        """ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_lines = [
            "# Fragrance AI ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸",
            f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## ì‹œìŠ¤í…œ ì •ë³´",
            f"- CPU: {self.system_info['cpu_count']} cores",
            f"- ë©”ëª¨ë¦¬: {self.system_info['memory_total_gb']:.1f} GB", 
            f"- GPU: {self.system_info['gpu_name']}",
            f"- CUDA: {self.system_info['cuda_available']}",
            f"- Python: {self.system_info['python_version']}",
            f"- PyTorch: {self.system_info['torch_version']}",
            "",
            "## ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼",
            ""
        ]
        
        for category, results in all_results.items():
            report_lines.append(f"### {category.replace('_', ' ').title()}")
            report_lines.append("")
            
            # í…Œì´ë¸” í—¤ë”
            report_lines.extend([
                "| í…ŒìŠ¤íŠ¸ | í‰ê·  ì‹œê°„(s) | í‘œì¤€í¸ì°¨(s) | P95(s) | ì²˜ë¦¬ëŸ‰(/s) | ì„±ê³µë¥ (%) | ë©”ëª¨ë¦¬(MB) |",
                "|--------|-------------|------------|---------|-----------|----------|----------|"
            ])
            
            for test_name, result in results.items():
                report_lines.append(
                    f"| {test_name} | {result.mean_time:.3f} | {result.std_time:.3f} | "
                    f"{result.p95_time:.3f} | {result.throughput:.2f} | "
                    f"{result.success_rate*100:.1f} | {result.memory_usage_mb:.1f} |"
                )
            
            report_lines.append("")
        
        # ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­
        report_lines.extend([
            "## ì„±ëŠ¥ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­",
            ""
        ])
        
        # ì„ë² ë”© ëª¨ë¸ ë¶„ì„
        if "embedding" in all_results:
            embedding_results = all_results["embedding"]
            batch_1_time = embedding_results.get("batch_1", BenchmarkResult("", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, (0, 0))).mean_time
            batch_100_time = embedding_results.get("batch_100", BenchmarkResult("", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, (0, 0))).mean_time
            
            if batch_1_time > 0 and batch_100_time > 0:
                efficiency_ratio = (batch_1_time * 100) / batch_100_time
                report_lines.append(f"- **ì„ë² ë”© ë°°ì¹˜ íš¨ìœ¨ì„±**: {efficiency_ratio:.1f}x (ë°°ì¹˜ ì²˜ë¦¬ê°€ {efficiency_ratio:.1f}ë°° ë” íš¨ìœ¨ì )")
                
                if efficiency_ratio < 5:
                    report_lines.append("  - âš ï¸ ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. GPU í™œìš©ì„ í™•ì¸í•´ë³´ì„¸ìš”.")
                elif efficiency_ratio > 20:
                    report_lines.append("  - âœ… ìš°ìˆ˜í•œ ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨ì„±ì„ ë³´ì…ë‹ˆë‹¤.")
        
        # ìƒì„± ëª¨ë¸ ë¶„ì„
        if "generation" in all_results:
            generation_results = all_results["generation"]
            for test_name, result in generation_results.items():
                if result.mean_time > 10:
                    report_lines.append(f"- âš ï¸ {test_name} ìƒì„± ì‹œê°„ì´ ê¸´ í¸ì…ë‹ˆë‹¤ ({result.mean_time:.1f}s). ëª¨ë¸ ìµœì í™”ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.")
                elif result.mean_time < 3:
                    report_lines.append(f"- âœ… {test_name} ìƒì„± ì„±ëŠ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤ ({result.mean_time:.1f}s).")
        
        return "\n".join(report_lines)
    
    def save_results(self, all_results: Dict[str, Dict[str, BenchmarkResult]]):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        # JSON í˜•íƒœë¡œ ì €ì¥
        json_data = {
            "system_info": self.system_info,
            "results": {}
        }
        
        for category, results in all_results.items():
            json_data["results"][category] = {}
            for test_name, result in results.items():
                json_data["results"][category][test_name] = {
                    "mean_time": result.mean_time,
                    "std_time": result.std_time,
                    "min_time": result.min_time,
                    "max_time": result.max_time,
                    "p95_time": result.p95_time,
                    "p99_time": result.p99_time,
                    "throughput": result.throughput,
                    "memory_usage_mb": result.memory_usage_mb,
                    "cpu_usage_percent": result.cpu_usage_percent,
                    "success_rate": result.success_rate,
                    "error_count": result.error_count,
                    "sample_size": result.sample_size,
                    "confidence_interval": result.confidence_interval
                }
        
        # JSON íŒŒì¼ ì €ì¥
        json_file = self.output_dir / f"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report = self.generate_report(all_results)
        report_file = self.output_dir / f"benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        logger.info(f"  - JSON: {json_file}")
        logger.info(f"  - Report: {report_file}")

async def main():
    parser = argparse.ArgumentParser(description="Fragrance AI ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    parser.add_argument("--output-dir", type=str, default="./benchmark_results", 
                       help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--iterations", type=int, default=10, 
                       help="ë°˜ë³µ íšŸìˆ˜")
    parser.add_argument("--skip-embedding", action="store_true", 
                       help="ì„ë² ë”© ë²¤ì¹˜ë§ˆí¬ ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--skip-generation", action="store_true", 
                       help="ìƒì„± ë²¤ì¹˜ë§ˆí¬ ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--skip-search", action="store_true", 
                       help="ê²€ìƒ‰ ë²¤ì¹˜ë§ˆí¬ ê±´ë„ˆë›°ê¸°")
    parser.add_argument("--skip-e2e", action="store_true", 
                       help="E2E ë²¤ì¹˜ë§ˆí¬ ê±´ë„ˆë›°ê¸°")
    
    args = parser.parse_args()
    
    # ë²¤ì¹˜ë§ˆí¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    benchmark = PerformanceBenchmark(args.output_dir)
    all_results = {}
    
    logger.info("Fragrance AI ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
    logger.info(f"ì‹œìŠ¤í…œ ì •ë³´: {benchmark.system_info}")
    
    # ì„ë² ë”© ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
    if not args.skip_embedding:
        try:
            embedding_results = await benchmark.benchmark_embedding_model(
                iterations=args.iterations
            )
            all_results["embedding"] = embedding_results
            logger.info("âœ… ì„ë² ë”© ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
    
    # ìƒì„± ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
    if not args.skip_generation:
        try:
            generation_results = await benchmark.benchmark_generation_model(
                iterations=max(1, args.iterations // 2)  # ìƒì„±ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ì ˆë°˜
            )
            all_results["generation"] = generation_results
            logger.info("âœ… ìƒì„± ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ìƒì„± ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
    
    # ê²€ìƒ‰ ì„œë¹„ìŠ¤ ë²¤ì¹˜ë§ˆí¬
    if not args.skip_search:
        try:
            search_results = await benchmark.benchmark_search_service(
                iterations=args.iterations
            )
            all_results["search"] = search_results
            logger.info("âœ… ê²€ìƒ‰ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
    
    # End-to-End ë²¤ì¹˜ë§ˆí¬
    if not args.skip_e2e:
        try:
            e2e_results = await benchmark.benchmark_end_to_end(
                iterations=max(1, args.iterations // 3)  # E2EëŠ” ë” ì ì€ ë°˜ë³µ
            )
            all_results["end_to_end"] = e2e_results
            logger.info("âœ… E2E ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ E2E ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
    
    # ê²°ê³¼ ì €ì¥ ë° ë¦¬í¬íŠ¸ ìƒì„±
    if all_results:
        benchmark.save_results(all_results)
        logger.info("ğŸ‰ ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    else:
        logger.warning("âš ï¸ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())