#!/usr/bin/env python3
"""
Canary Deployment System
점진적 트래픽 증가와 자동 검증을 통한 안전한 배포

Usage:
    python scripts/canary_deployment.py --version v0.2.0
    python scripts/canary_deployment.py --version v0.2.0 --stages 1,5,25,100
    python scripts/canary_deployment.py --version v0.2.0 --dry-run
"""

import argparse
import time
import json
import subprocess
import requests
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import statistics


# =============================================================================
# Configuration
# =============================================================================

# Canary stages (percentage of traffic)
DEFAULT_STAGES = [1, 5, 25, 100]

# Observation period per stage (seconds)
OBSERVATION_PERIOD = 15 * 60  # 15 minutes

# Performance thresholds
THRESHOLDS = {
    'fast': {
        'p95_latency': 2.5,  # seconds
        'error_rate': 0.005,  # 0.5%
    },
    'balanced': {
        'p95_latency': 3.2,
        'error_rate': 0.005,
    },
    'creative': {
        'p95_latency': 4.5,
        'error_rate': 0.005,
    },
    'llm_brief_schema_failure': 0.0,  # 0% schema failures
    'rl_update_loss_spike': 2.0,  # Max 2x increase from baseline
}

# Colors
RED = '\033[0;31m'
GREEN = '\033[0;32m'
YELLOW = '\033[1;33m'
BLUE = '\033[0;34m'
NC = '\033[0m'


# =============================================================================
# Helper Functions
# =============================================================================

def log_info(msg: str):
    print(f"{BLUE}[INFO]{NC} {msg}")


def log_success(msg: str):
    print(f"{GREEN}[✓]{NC} {msg}")


def log_warning(msg: str):
    print(f"{YELLOW}[⚠]{NC} {msg}")


def log_error(msg: str):
    print(f"{RED}[✗]{NC} {msg}")


def run_command(cmd: List[str]) -> Tuple[int, str, str]:
    """Run command and return (returncode, stdout, stderr)"""
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=30
        )
        return result.returncode, result.stdout, result.stderr
    except Exception as e:
        return 1, "", str(e)


# =============================================================================
# Traffic Management
# =============================================================================

class TrafficController:
    """Control traffic distribution to canary version"""

    def __init__(self, canary_version: str, production_version: str):
        self.canary_version = canary_version
        self.production_version = production_version

    def set_canary_weight(self, percentage: int):
        """
        Set canary traffic weight (0-100%)

        Uses NGINX upstream weight configuration
        """
        log_info(f"Setting canary traffic to {percentage}%...")

        # Update NGINX config
        nginx_config = self._generate_nginx_config(percentage)

        # Write to file
        config_path = Path("nginx/conf.d/upstream.conf")
        config_path.parent.mkdir(parents=True, exist_ok=True)

        with open(config_path, 'w') as f:
            f.write(nginx_config)

        # Reload NGINX
        returncode, stdout, stderr = run_command([
            'docker-compose', '-f', 'docker-compose.production.yml',
            'exec', '-T', 'nginx', 'nginx', '-s', 'reload'
        ])

        if returncode == 0:
            log_success(f"Canary traffic set to {percentage}%")
            return True
        else:
            log_error(f"Failed to reload NGINX: {stderr}")
            return False

    def _generate_nginx_config(self, canary_percentage: int) -> str:
        """Generate NGINX upstream configuration with weighted traffic"""

        production_weight = 100 - canary_percentage
        canary_weight = canary_percentage

        return f"""
# Auto-generated by canary deployment system
# Generated: {datetime.now().isoformat()}

upstream app_backend {{
    # Production version (stable)
    server app:8000 weight={production_weight};

    # Canary version (testing)
    server app-canary:8000 weight={canary_weight};

    # Connection settings
    keepalive 32;
}}

# Canary metadata
# Canary Version: {self.canary_version}
# Production Version: {self.production_version}
# Canary Weight: {canary_percentage}%
"""

    def rollback(self):
        """Rollback to 100% production traffic"""
        log_warning("Rolling back to 100% production traffic...")
        return self.set_canary_weight(0)


# =============================================================================
# Metrics Collection
# =============================================================================

class MetricsCollector:
    """Collect and validate metrics from Prometheus"""

    def __init__(self, prometheus_url: str = "http://localhost:9090"):
        self.prometheus_url = prometheus_url

    def query_prometheus(self, query: str) -> Optional[float]:
        """Query Prometheus and return metric value"""
        try:
            response = requests.get(
                f"{self.prometheus_url}/api/v1/query",
                params={'query': query},
                timeout=10
            )
            response.raise_for_status()

            data = response.json()
            if data['status'] == 'success' and data['data']['result']:
                value = float(data['data']['result'][0]['value'][1])
                return value
            return None

        except Exception as e:
            log_warning(f"Failed to query Prometheus: {e}")
            return None

    def get_p95_latency(self, mode: str = 'all', version: str = 'canary') -> Optional[float]:
        """Get p95 latency for a specific mode"""
        if mode == 'all':
            query = f'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{{version="{version}"}}[5m]))'
        else:
            query = f'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{{version="{version}",mode="{mode}"}}[5m]))'

        return self.query_prometheus(query)

    def get_error_rate(self, version: str = 'canary') -> Optional[float]:
        """Get error rate (5xx errors / total requests)"""
        error_query = f'rate(http_requests_total{{version="{version}",status=~"5.."}}[5m])'
        total_query = f'rate(http_requests_total{{version="{version}"}}[5m])'

        errors = self.query_prometheus(error_query)
        total = self.query_prometheus(total_query)

        if errors is not None and total is not None and total > 0:
            return errors / total
        return None

    def get_llm_brief_schema_failure_rate(self, version: str = 'canary') -> Optional[float]:
        """Get LLM brief schema validation failure rate"""
        failure_query = f'rate(llm_brief_schema_validation_failures_total{{version="{version}"}}[5m])'
        total_query = f'rate(llm_brief_requests_total{{version="{version}"}}[5m])'

        failures = self.query_prometheus(failure_query)
        total = self.query_prometheus(total_query)

        if failures is not None and total is not None and total > 0:
            return failures / total
        return 0.0 if failures == 0 else None

    def get_rl_update_loss(self, version: str = 'canary') -> Optional[float]:
        """Get RL training loss"""
        query = f'rl_training_loss{{version="{version}"}}'
        return self.query_prometheus(query)

    def get_baseline_rl_loss(self) -> Optional[float]:
        """Get baseline RL loss from production"""
        query = 'rl_training_loss{version="production"}'
        return self.query_prometheus(query)


# =============================================================================
# Validation
# =============================================================================

class CanaryValidator:
    """Validate canary deployment metrics"""

    def __init__(self, metrics_collector: MetricsCollector):
        self.collector = metrics_collector

    def validate_stage(self, percentage: int, observation_period: int) -> Tuple[bool, Dict]:
        """
        Validate canary deployment stage

        Returns: (passed, metrics_report)
        """
        log_info(f"Validating {percentage}% canary traffic for {observation_period}s...")

        # Wait for observation period
        start_time = time.time()
        end_time = start_time + observation_period

        metrics_history = []

        while time.time() < end_time:
            remaining = int(end_time - time.time())
            log_info(f"Observing... {remaining}s remaining")

            # Collect metrics
            metrics = self._collect_metrics()
            metrics_history.append(metrics)

            # Display current metrics
            self._display_metrics(metrics)

            # Early failure detection
            if self._detect_critical_failure(metrics):
                log_error("Critical failure detected! Aborting canary deployment.")
                return False, self._generate_report(metrics_history)

            # Wait before next collection (every 30 seconds)
            time.sleep(min(30, remaining))

        # Final validation
        final_metrics = self._aggregate_metrics(metrics_history)
        passed = self._validate_thresholds(final_metrics)

        report = self._generate_report(metrics_history)

        if passed:
            log_success(f"{percentage}% stage validation: PASSED")
        else:
            log_error(f"{percentage}% stage validation: FAILED")

        return passed, report

    def _collect_metrics(self) -> Dict:
        """Collect all metrics at current time"""
        return {
            'timestamp': datetime.now().isoformat(),
            'latency': {
                'fast': self.collector.get_p95_latency('fast', 'canary'),
                'balanced': self.collector.get_p95_latency('balanced', 'canary'),
                'creative': self.collector.get_p95_latency('creative', 'canary'),
            },
            'error_rate': self.collector.get_error_rate('canary'),
            'llm_schema_failure': self.collector.get_llm_brief_schema_failure_rate('canary'),
            'rl_loss': self.collector.get_rl_update_loss('canary'),
            'rl_baseline': self.collector.get_baseline_rl_loss(),
        }

    def _display_metrics(self, metrics: Dict):
        """Display current metrics"""
        print(f"\n{'-' * 80}")
        print(f"Timestamp: {metrics['timestamp']}")

        # Latency
        print(f"\nLatency (p95):")
        for mode, latency in metrics['latency'].items():
            threshold = THRESHOLDS.get(mode, {}).get('p95_latency', 0)
            status = '✓' if latency and latency <= threshold else '✗'
            print(f"  {status} {mode:10s}: {latency:.3f}s (threshold: {threshold}s)")

        # Error rate
        error_rate = metrics.get('error_rate')
        error_threshold = 0.005
        status = '✓' if error_rate and error_rate < error_threshold else '✗'
        print(f"\nError Rate: {status} {error_rate*100:.2f}% (threshold: {error_threshold*100}%)")

        # LLM schema failures
        schema_failure = metrics.get('llm_schema_failure', 0)
        status = '✓' if schema_failure == 0 else '✗'
        print(f"LLM Schema Failures: {status} {schema_failure*100:.2f}% (threshold: 0%)")

        # RL loss
        rl_loss = metrics.get('rl_loss')
        rl_baseline = metrics.get('rl_baseline')
        if rl_loss and rl_baseline:
            spike_ratio = rl_loss / rl_baseline
            status = '✓' if spike_ratio < THRESHOLDS['rl_update_loss_spike'] else '✗'
            print(f"RL Loss: {status} {rl_loss:.4f} (baseline: {rl_baseline:.4f}, ratio: {spike_ratio:.2f}x)")

        print(f"{'-' * 80}\n")

    def _detect_critical_failure(self, metrics: Dict) -> bool:
        """Detect critical failure requiring immediate rollback"""

        # Check error rate
        error_rate = metrics.get('error_rate')
        if error_rate and error_rate > 0.05:  # 5% error rate
            log_error(f"Critical: Error rate too high: {error_rate*100:.2f}%")
            return True

        # Check RL loss spike
        rl_loss = metrics.get('rl_loss')
        rl_baseline = metrics.get('rl_baseline')
        if rl_loss and rl_baseline:
            if rl_loss / rl_baseline > 5.0:  # 5x increase
                log_error(f"Critical: RL loss spike: {rl_loss / rl_baseline:.2f}x")
                return True

        return False

    def _aggregate_metrics(self, metrics_history: List[Dict]) -> Dict:
        """Aggregate metrics over observation period"""
        if not metrics_history:
            return {}

        aggregated = {
            'latency': {},
            'error_rate': None,
            'llm_schema_failure': None,
            'rl_loss': None,
            'rl_baseline': None,
        }

        # Aggregate latency (p95)
        for mode in ['fast', 'balanced', 'creative']:
            values = [m['latency'][mode] for m in metrics_history if m['latency'][mode] is not None]
            if values:
                # Use 95th percentile of collected p95 values
                aggregated['latency'][mode] = statistics.quantiles(values, n=20)[18]  # p95

        # Aggregate error rate (max)
        error_rates = [m['error_rate'] for m in metrics_history if m['error_rate'] is not None]
        if error_rates:
            aggregated['error_rate'] = max(error_rates)

        # LLM schema failure (max)
        schema_failures = [m['llm_schema_failure'] for m in metrics_history if m['llm_schema_failure'] is not None]
        if schema_failures:
            aggregated['llm_schema_failure'] = max(schema_failures)

        # RL loss (average)
        rl_losses = [m['rl_loss'] for m in metrics_history if m['rl_loss'] is not None]
        if rl_losses:
            aggregated['rl_loss'] = statistics.mean(rl_losses)

        # RL baseline (average)
        rl_baselines = [m['rl_baseline'] for m in metrics_history if m['rl_baseline'] is not None]
        if rl_baselines:
            aggregated['rl_baseline'] = statistics.mean(rl_baselines)

        return aggregated

    def _validate_thresholds(self, metrics: Dict) -> bool:
        """Validate metrics against thresholds"""
        passed = True

        # Check latency for each mode
        for mode in ['fast', 'balanced', 'creative']:
            latency = metrics['latency'].get(mode)
            threshold = THRESHOLDS.get(mode, {}).get('p95_latency')

            if latency is None:
                log_warning(f"No latency data for {mode}")
                continue

            if latency > threshold:
                log_error(f"{mode} p95 latency: {latency:.3f}s > {threshold}s (threshold)")
                passed = False
            else:
                log_success(f"{mode} p95 latency: {latency:.3f}s ≤ {threshold}s")

        # Check error rate
        error_rate = metrics.get('error_rate')
        error_threshold = 0.005

        if error_rate is not None:
            if error_rate >= error_threshold:
                log_error(f"Error rate: {error_rate*100:.2f}% ≥ {error_threshold*100}% (threshold)")
                passed = False
            else:
                log_success(f"Error rate: {error_rate*100:.2f}% < {error_threshold*100}%")

        # Check LLM schema failures
        schema_failure = metrics.get('llm_schema_failure', 0)
        if schema_failure > 0:
            log_error(f"LLM schema failures: {schema_failure*100:.2f}% > 0% (threshold)")
            passed = False
        else:
            log_success(f"LLM schema failures: 0%")

        # Check RL loss spike
        rl_loss = metrics.get('rl_loss')
        rl_baseline = metrics.get('rl_baseline')

        if rl_loss and rl_baseline:
            spike_ratio = rl_loss / rl_baseline
            threshold = THRESHOLDS['rl_update_loss_spike']

            if spike_ratio >= threshold:
                log_error(f"RL loss spike: {spike_ratio:.2f}x ≥ {threshold}x (threshold)")
                passed = False
            else:
                log_success(f"RL loss spike: {spike_ratio:.2f}x < {threshold}x")

        return passed

    def _generate_report(self, metrics_history: List[Dict]) -> Dict:
        """Generate validation report"""
        aggregated = self._aggregate_metrics(metrics_history)

        return {
            'observation_count': len(metrics_history),
            'aggregated_metrics': aggregated,
            'thresholds': THRESHOLDS,
            'passed': self._validate_thresholds(aggregated),
            'history': metrics_history
        }


# =============================================================================
# Canary Deployment Orchestrator
# =============================================================================

class CanaryDeployment:
    """Orchestrate canary deployment"""

    def __init__(
        self,
        canary_version: str,
        production_version: str,
        stages: List[int] = None,
        observation_period: int = OBSERVATION_PERIOD,
        dry_run: bool = False
    ):
        self.canary_version = canary_version
        self.production_version = production_version
        self.stages = stages or DEFAULT_STAGES
        self.observation_period = observation_period
        self.dry_run = dry_run

        self.traffic_controller = TrafficController(canary_version, production_version)
        self.metrics_collector = MetricsCollector()
        self.validator = CanaryValidator(self.metrics_collector)

        self.deployment_log = []

    def execute(self) -> bool:
        """
        Execute canary deployment

        Returns: True if successful, False if rolled back
        """
        log_info("=" * 80)
        log_info("CANARY DEPLOYMENT")
        log_info("=" * 80)
        log_info(f"Canary Version: {self.canary_version}")
        log_info(f"Production Version: {self.production_version}")
        log_info(f"Stages: {self.stages}%")
        log_info(f"Observation Period: {self.observation_period}s ({self.observation_period // 60}min)")
        log_info(f"Dry Run: {self.dry_run}")
        log_info("=" * 80)
        log_info("")

        if self.dry_run:
            log_warning("DRY RUN MODE: No actual traffic changes will be made")
            log_info("")

        # Verify canary container is running
        if not self._verify_canary_ready():
            log_error("Canary container not ready. Deployment aborted.")
            return False

        # Execute stages
        for stage_percentage in self.stages:
            log_info("")
            log_info("=" * 80)
            log_info(f"STAGE: {stage_percentage}% CANARY TRAFFIC")
            log_info("=" * 80)
            log_info("")

            # Set traffic weight
            if not self.dry_run:
                if not self.traffic_controller.set_canary_weight(stage_percentage):
                    log_error("Failed to set canary traffic weight. Rolling back...")
                    self._rollback()
                    return False

            # Wait a bit for traffic to stabilize
            log_info("Waiting for traffic to stabilize...")
            time.sleep(30)

            # Validate stage
            passed, report = self.validator.validate_stage(
                stage_percentage,
                self.observation_period
            )

            # Log stage result
            self.deployment_log.append({
                'stage': stage_percentage,
                'timestamp': datetime.now().isoformat(),
                'passed': passed,
                'report': report
            })

            if not passed:
                log_error(f"Stage {stage_percentage}% validation FAILED")
                log_error("Rolling back to production...")
                self._rollback()
                self._save_deployment_log(success=False)
                return False

            log_success(f"Stage {stage_percentage}% validation PASSED")

            # If not final stage, proceed to next
            if stage_percentage < 100:
                log_info(f"Proceeding to next stage...")
            else:
                log_success("All stages completed successfully!")

        # Deployment successful
        log_info("")
        log_success("=" * 80)
        log_success("CANARY DEPLOYMENT: SUCCESS")
        log_success("=" * 80)
        log_info("")
        log_info(f"Canary version {self.canary_version} now serving 100% traffic")

        self._save_deployment_log(success=True)

        return True

    def _verify_canary_ready(self) -> bool:
        """Verify canary container is running and healthy"""
        log_info("Verifying canary container readiness...")

        # Check Docker container
        returncode, stdout, stderr = run_command([
            'docker-compose', '-f', 'docker-compose.production.yml',
            'ps', 'app-canary'
        ])

        if returncode != 0 or 'Up' not in stdout:
            log_error("Canary container (app-canary) is not running")
            return False

        # Check health endpoint
        try:
            response = requests.get('http://localhost:8001/health', timeout=10)
            if response.status_code == 200:
                log_success("Canary container is healthy")
                return True
            else:
                log_error(f"Canary health check failed: HTTP {response.status_code}")
                return False
        except Exception as e:
            log_error(f"Canary health check failed: {e}")
            return False

    def _rollback(self):
        """Rollback to 100% production"""
        if not self.dry_run:
            self.traffic_controller.rollback()
            log_success("Rolled back to 100% production traffic")

    def _save_deployment_log(self, success: bool):
        """Save deployment log to file"""
        log_dir = Path("deployments/canary")
        log_dir.mkdir(parents=True, exist_ok=True)

        log_file = log_dir / f"canary_{self.canary_version}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        log_data = {
            'canary_version': self.canary_version,
            'production_version': self.production_version,
            'stages': self.stages,
            'observation_period': self.observation_period,
            'success': success,
            'start_time': self.deployment_log[0]['timestamp'] if self.deployment_log else None,
            'end_time': datetime.now().isoformat(),
            'log': self.deployment_log
        }

        with open(log_file, 'w') as f:
            json.dump(log_data, f, indent=2)

        log_info(f"Deployment log saved: {log_file}")


# =============================================================================
# CLI
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='Canary deployment with progressive traffic rollout',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        '--version',
        required=True,
        help='Canary version to deploy (e.g., v0.2.0)'
    )
    parser.add_argument(
        '--production-version',
        default='production',
        help='Current production version (default: production)'
    )
    parser.add_argument(
        '--stages',
        default='1,5,25,100',
        help='Canary stages as comma-separated percentages (default: 1,5,25,100)'
    )
    parser.add_argument(
        '--observation-period',
        type=int,
        default=OBSERVATION_PERIOD,
        help=f'Observation period per stage in seconds (default: {OBSERVATION_PERIOD})'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Dry run mode (no actual traffic changes)'
    )

    args = parser.parse_args()

    # Parse stages
    stages = [int(s.strip()) for s in args.stages.split(',')]

    # Validate stages
    if not all(0 < s <= 100 for s in stages):
        log_error("Invalid stages: all values must be between 1 and 100")
        sys.exit(1)

    if stages[-1] != 100:
        log_error("Invalid stages: last stage must be 100")
        sys.exit(1)

    # Create deployment
    deployment = CanaryDeployment(
        canary_version=args.version,
        production_version=args.production_version,
        stages=stages,
        observation_period=args.observation_period,
        dry_run=args.dry_run
    )

    # Execute
    success = deployment.execute()

    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()
