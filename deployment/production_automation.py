# ğŸš€ ì™„ë²½í•œ í”„ë¡œë•ì…˜ ë°°í¬ ìë™í™” ì‹œìŠ¤í…œ
import asyncio
import json
import yaml
import subprocess
import shutil
import zipfile
import tarfile
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any, Union, Callable
from dataclasses import dataclass, field, asdict
from enum import Enum
from pathlib import Path
import aiofiles
import aiohttp
import docker
import kubernetes
from kubernetes import client, config
import boto3
import structlog
from jinja2 import Template, Environment, FileSystemLoader
import hashlib
import tempfile
import os

logger = structlog.get_logger("deployment_automation")


class DeploymentEnvironment(Enum):
    """ë°°í¬ í™˜ê²½"""
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION = "production"
    CANARY = "canary"
    BLUE_GREEN = "blue_green"


class DeploymentStrategy(Enum):
    """ë°°í¬ ì „ëµ"""
    ROLLING_UPDATE = "rolling_update"
    BLUE_GREEN = "blue_green"
    CANARY = "canary"
    RECREATE = "recreate"
    A_B_TESTING = "a_b_testing"


class DeploymentStatus(Enum):
    """ë°°í¬ ìƒíƒœ"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    SUCCESS = "success"
    FAILED = "failed"
    ROLLBACK = "rollback"
    CANCELLED = "cancelled"


@dataclass
class DeploymentConfig:
    """ë°°í¬ ì„¤ì •"""
    environment: DeploymentEnvironment
    strategy: DeploymentStrategy
    image_tag: str
    replicas: int
    resources: Dict[str, Any]
    environment_variables: Dict[str, str] = field(default_factory=dict)
    secrets: Dict[str, str] = field(default_factory=dict)
    health_check: Dict[str, Any] = field(default_factory=dict)
    rollback_on_failure: bool = True
    auto_scaling: Dict[str, Any] = field(default_factory=dict)
    monitoring: Dict[str, Any] = field(default_factory=dict)


@dataclass
class DeploymentResult:
    """ë°°í¬ ê²°ê³¼"""
    deployment_id: str
    status: DeploymentStatus
    environment: str
    strategy: str
    start_time: datetime
    end_time: Optional[datetime] = None
    duration: Optional[float] = None
    success_rate: Optional[float] = None
    error_message: Optional[str] = None
    rollback_performed: bool = False
    metrics: Dict[str, Any] = field(default_factory=dict)
    logs: List[str] = field(default_factory=list)


@dataclass
class HealthCheck:
    """í—¬ìŠ¤ì²´í¬ ì„¤ì •"""
    endpoint: str
    method: str = "GET"
    expected_status: int = 200
    timeout: int = 30
    interval: int = 10
    retries: int = 3
    headers: Dict[str, str] = field(default_factory=dict)


class ContainerBuilder:
    """ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ë¹Œë”"""

    def __init__(self, docker_client: Optional[docker.DockerClient] = None):
        self.docker_client = docker_client or docker.from_env()

    async def build_image(
        self,
        dockerfile_path: str,
        image_name: str,
        tag: str,
        build_args: Dict[str, str] = None,
        context_path: str = "."
    ) -> str:
        """Docker ì´ë¯¸ì§€ ë¹Œë“œ"""
        full_image_name = f"{image_name}:{tag}"

        try:
            logger.info(f"Docker ì´ë¯¸ì§€ ë¹Œë“œ ì‹œì‘: {full_image_name}")

            # ë¹Œë“œ ë¡œê·¸ë¥¼ ìœ„í•œ ì½œë°±
            build_logs = []

            def log_callback(stream):
                if 'stream' in stream:
                    log_line = stream['stream'].strip()
                    if log_line:
                        build_logs.append(log_line)
                        logger.info(f"ë¹Œë“œ: {log_line}")

            # ì´ë¯¸ì§€ ë¹Œë“œ
            image, build_log = self.docker_client.images.build(
                path=context_path,
                dockerfile=dockerfile_path,
                tag=full_image_name,
                buildargs=build_args or {},
                rm=True,
                forcerm=True
            )

            # ë¹Œë“œ ë¡œê·¸ ì²˜ë¦¬
            for log in build_log:
                log_callback(log)

            logger.info(f"Docker ì´ë¯¸ì§€ ë¹Œë“œ ì™„ë£Œ: {full_image_name}")
            return full_image_name

        except Exception as e:
            logger.error(f"Docker ì´ë¯¸ì§€ ë¹Œë“œ ì‹¤íŒ¨: {e}")
            raise

    async def push_image(self, image_name: str, registry_url: str = None) -> bool:
        """ì´ë¯¸ì§€ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— í‘¸ì‹œ"""
        try:
            if registry_url:
                # ë ˆì§€ìŠ¤íŠ¸ë¦¬ íƒœê·¸ ì¶”ê°€
                registry_image = f"{registry_url}/{image_name}"
                self.docker_client.api.tag(image_name, registry_image)
                image_name = registry_image

            logger.info(f"ì´ë¯¸ì§€ í‘¸ì‹œ ì‹œì‘: {image_name}")

            # í‘¸ì‹œ ì‹¤í–‰
            push_log = self.docker_client.images.push(image_name, stream=True, decode=True)

            for log in push_log:
                if 'status' in log:
                    logger.info(f"í‘¸ì‹œ: {log['status']}")

            logger.info(f"ì´ë¯¸ì§€ í‘¸ì‹œ ì™„ë£Œ: {image_name}")
            return True

        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ í‘¸ì‹œ ì‹¤íŒ¨: {e}")
            return False

    def get_image_info(self, image_name: str) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ì •ë³´ ì¡°íšŒ"""
        try:
            image = self.docker_client.images.get(image_name)
            return {
                "id": image.id,
                "tags": image.tags,
                "created": image.attrs.get("Created"),
                "size": image.attrs.get("Size"),
                "architecture": image.attrs.get("Architecture"),
                "os": image.attrs.get("Os")
            }
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}


class KubernetesDeployer:
    """Kubernetes ë°°í¬ì"""

    def __init__(self, kubeconfig_path: str = None):
        if kubeconfig_path:
            config.load_kube_config(config_file=kubeconfig_path)
        else:
            try:
                config.load_incluster_config()
            except:
                config.load_kube_config()

        self.apps_v1 = client.AppsV1Api()
        self.core_v1 = client.CoreV1Api()
        self.autoscaling_v1 = client.AutoscalingV1Api()
        self.networking_v1 = client.NetworkingV1Api()

    async def deploy_application(
        self,
        config: DeploymentConfig,
        namespace: str = "default"
    ) -> Dict[str, Any]:
        """ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬"""
        try:
            deployment_name = f"fragrance-ai-{config.environment.value}"

            # ë°°í¬ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
            deployment_manifest = self._create_deployment_manifest(config, deployment_name, namespace)

            # ì„œë¹„ìŠ¤ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
            service_manifest = self._create_service_manifest(deployment_name, namespace)

            # ConfigMap ìƒì„± (í™˜ê²½ ë³€ìˆ˜ìš©)
            if config.environment_variables:
                configmap_manifest = self._create_configmap_manifest(
                    deployment_name, config.environment_variables, namespace
                )
                await self._apply_configmap(configmap_manifest)

            # Secret ìƒì„±
            if config.secrets:
                secret_manifest = self._create_secret_manifest(
                    deployment_name, config.secrets, namespace
                )
                await self._apply_secret(secret_manifest)

            # ë°°í¬ ì „ëµì— ë”°ë¥¸ ë°°í¬ ì‹¤í–‰
            if config.strategy == DeploymentStrategy.BLUE_GREEN:
                return await self._blue_green_deploy(deployment_manifest, service_manifest, namespace)
            elif config.strategy == DeploymentStrategy.CANARY:
                return await self._canary_deploy(deployment_manifest, service_manifest, namespace)
            else:
                return await self._rolling_update_deploy(deployment_manifest, service_manifest, namespace)

        except Exception as e:
            logger.error(f"Kubernetes ë°°í¬ ì‹¤íŒ¨: {e}")
            raise

    def _create_deployment_manifest(
        self,
        config: DeploymentConfig,
        name: str,
        namespace: str
    ) -> Dict[str, Any]:
        """ë°°í¬ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±"""
        return {
            "apiVersion": "apps/v1",
            "kind": "Deployment",
            "metadata": {
                "name": name,
                "namespace": namespace,
                "labels": {
                    "app": "fragrance-ai",
                    "environment": config.environment.value,
                    "version": config.image_tag
                }
            },
            "spec": {
                "replicas": config.replicas,
                "strategy": {
                    "type": "RollingUpdate",
                    "rollingUpdate": {
                        "maxSurge": 1,
                        "maxUnavailable": 0
                    }
                },
                "selector": {
                    "matchLabels": {
                        "app": "fragrance-ai",
                        "environment": config.environment.value
                    }
                },
                "template": {
                    "metadata": {
                        "labels": {
                            "app": "fragrance-ai",
                            "environment": config.environment.value,
                            "version": config.image_tag
                        }
                    },
                    "spec": {
                        "containers": [{
                            "name": "fragrance-ai",
                            "image": f"fragrance-ai:{config.image_tag}",
                            "ports": [{"containerPort": 8000}],
                            "resources": config.resources,
                            "envFrom": [
                                {"configMapRef": {"name": f"{name}-config"}},
                                {"secretRef": {"name": f"{name}-secrets"}}
                            ],
                            "livenessProbe": {
                                "httpGet": {
                                    "path": "/health",
                                    "port": 8000
                                },
                                "initialDelaySeconds": 30,
                                "periodSeconds": 10
                            },
                            "readinessProbe": {
                                "httpGet": {
                                    "path": "/ready",
                                    "port": 8000
                                },
                                "initialDelaySeconds": 5,
                                "periodSeconds": 5
                            }
                        }]
                    }
                }
            }
        }

    def _create_service_manifest(self, name: str, namespace: str) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±"""
        return {
            "apiVersion": "v1",
            "kind": "Service",
            "metadata": {
                "name": f"{name}-service",
                "namespace": namespace
            },
            "spec": {
                "selector": {
                    "app": "fragrance-ai"
                },
                "ports": [{
                    "port": 80,
                    "targetPort": 8000,
                    "protocol": "TCP"
                }],
                "type": "ClusterIP"
            }
        }

    def _create_configmap_manifest(
        self,
        name: str,
        data: Dict[str, str],
        namespace: str
    ) -> Dict[str, Any]:
        """ConfigMap ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±"""
        return {
            "apiVersion": "v1",
            "kind": "ConfigMap",
            "metadata": {
                "name": f"{name}-config",
                "namespace": namespace
            },
            "data": data
        }

    def _create_secret_manifest(
        self,
        name: str,
        data: Dict[str, str],
        namespace: str
    ) -> Dict[str, Any]:
        """Secret ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±"""
        import base64

        encoded_data = {
            key: base64.b64encode(value.encode()).decode()
            for key, value in data.items()
        }

        return {
            "apiVersion": "v1",
            "kind": "Secret",
            "metadata": {
                "name": f"{name}-secrets",
                "namespace": namespace
            },
            "type": "Opaque",
            "data": encoded_data
        }

    async def _apply_configmap(self, manifest: Dict[str, Any]):
        """ConfigMap ì ìš©"""
        try:
            self.core_v1.create_namespaced_config_map(
                namespace=manifest["metadata"]["namespace"],
                body=manifest
            )
        except client.ApiException as e:
            if e.status == 409:  # Already exists
                self.core_v1.patch_namespaced_config_map(
                    name=manifest["metadata"]["name"],
                    namespace=manifest["metadata"]["namespace"],
                    body=manifest
                )
            else:
                raise

    async def _apply_secret(self, manifest: Dict[str, Any]):
        """Secret ì ìš©"""
        try:
            self.core_v1.create_namespaced_secret(
                namespace=manifest["metadata"]["namespace"],
                body=manifest
            )
        except client.ApiException as e:
            if e.status == 409:  # Already exists
                self.core_v1.patch_namespaced_secret(
                    name=manifest["metadata"]["name"],
                    namespace=manifest["metadata"]["namespace"],
                    body=manifest
                )
            else:
                raise

    async def _rolling_update_deploy(
        self,
        deployment_manifest: Dict[str, Any],
        service_manifest: Dict[str, Any],
        namespace: str
    ) -> Dict[str, Any]:
        """ë¡¤ë§ ì—…ë°ì´íŠ¸ ë°°í¬"""
        deployment_name = deployment_manifest["metadata"]["name"]

        try:
            # ê¸°ì¡´ ë°°í¬ í™•ì¸
            try:
                existing_deployment = self.apps_v1.read_namespaced_deployment(
                    name=deployment_name,
                    namespace=namespace
                )
                # ì—…ë°ì´íŠ¸
                self.apps_v1.patch_namespaced_deployment(
                    name=deployment_name,
                    namespace=namespace,
                    body=deployment_manifest
                )
                logger.info(f"ë°°í¬ ì—…ë°ì´íŠ¸ë¨: {deployment_name}")
            except client.ApiException as e:
                if e.status == 404:
                    # ìƒˆ ë°°í¬ ìƒì„±
                    self.apps_v1.create_namespaced_deployment(
                        namespace=namespace,
                        body=deployment_manifest
                    )
                    logger.info(f"ìƒˆ ë°°í¬ ìƒì„±ë¨: {deployment_name}")
                else:
                    raise

            # ì„œë¹„ìŠ¤ ìƒì„±/ì—…ë°ì´íŠ¸
            try:
                self.core_v1.create_namespaced_service(
                    namespace=namespace,
                    body=service_manifest
                )
            except client.ApiException as e:
                if e.status == 409:
                    pass  # Service already exists
                else:
                    raise

            # ë°°í¬ ì™„ë£Œ ëŒ€ê¸°
            await self._wait_for_deployment_ready(deployment_name, namespace)

            return {
                "status": "success",
                "deployment_name": deployment_name,
                "strategy": "rolling_update"
            }

        except Exception as e:
            logger.error(f"ë¡¤ë§ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            raise

    async def _blue_green_deploy(
        self,
        deployment_manifest: Dict[str, Any],
        service_manifest: Dict[str, Any],
        namespace: str
    ) -> Dict[str, Any]:
        """ë¸”ë£¨-ê·¸ë¦° ë°°í¬"""
        base_name = deployment_manifest["metadata"]["name"]
        green_name = f"{base_name}-green"

        try:
            # ê·¸ë¦° í™˜ê²½ ë°°í¬
            green_manifest = deployment_manifest.copy()
            green_manifest["metadata"]["name"] = green_name
            green_manifest["spec"]["selector"]["matchLabels"]["deployment"] = "green"
            green_manifest["spec"]["template"]["metadata"]["labels"]["deployment"] = "green"

            self.apps_v1.create_namespaced_deployment(
                namespace=namespace,
                body=green_manifest
            )

            # ê·¸ë¦° í™˜ê²½ ì¤€ë¹„ ëŒ€ê¸°
            await self._wait_for_deployment_ready(green_name, namespace)

            # í—¬ìŠ¤ì²´í¬ ìˆ˜í–‰
            if await self._perform_health_check(green_name, namespace):
                # ì„œë¹„ìŠ¤ë¥¼ ê·¸ë¦°ìœ¼ë¡œ ì „í™˜
                service_manifest["spec"]["selector"]["deployment"] = "green"
                self.core_v1.patch_namespaced_service(
                    name=service_manifest["metadata"]["name"],
                    namespace=namespace,
                    body=service_manifest
                )

                # ë¸”ë£¨ í™˜ê²½ ì œê±° (ì„ íƒì )
                try:
                    self.apps_v1.delete_namespaced_deployment(
                        name=base_name,
                        namespace=namespace
                    )
                except client.ApiException:
                    pass

                logger.info(f"ë¸”ë£¨-ê·¸ë¦° ë°°í¬ ì™„ë£Œ: {green_name}")
                return {"status": "success", "deployment_name": green_name, "strategy": "blue_green"}

            else:
                # í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨ ì‹œ ê·¸ë¦° í™˜ê²½ ì œê±°
                self.apps_v1.delete_namespaced_deployment(
                    name=green_name,
                    namespace=namespace
                )
                raise Exception("í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨")

        except Exception as e:
            logger.error(f"ë¸”ë£¨-ê·¸ë¦° ë°°í¬ ì‹¤íŒ¨: {e}")
            raise

    async def _canary_deploy(
        self,
        deployment_manifest: Dict[str, Any],
        service_manifest: Dict[str, Any],
        namespace: str
    ) -> Dict[str, Any]:
        """ì¹´ë‚˜ë¦¬ ë°°í¬"""
        base_name = deployment_manifest["metadata"]["name"]
        canary_name = f"{base_name}-canary"

        try:
            # ì¹´ë‚˜ë¦¬ ë°°í¬ ìƒì„± (10% íŠ¸ë˜í”½)
            canary_manifest = deployment_manifest.copy()
            canary_manifest["metadata"]["name"] = canary_name
            canary_manifest["spec"]["replicas"] = max(1, deployment_manifest["spec"]["replicas"] // 10)
            canary_manifest["spec"]["selector"]["matchLabels"]["deployment"] = "canary"
            canary_manifest["spec"]["template"]["metadata"]["labels"]["deployment"] = "canary"

            self.apps_v1.create_namespaced_deployment(
                namespace=namespace,
                body=canary_manifest
            )

            await self._wait_for_deployment_ready(canary_name, namespace)

            # ì¹´ë‚˜ë¦¬ ëª¨ë‹ˆí„°ë§ (ì‹¤ì œë¡œëŠ” ë©”íŠ¸ë¦­ ìˆ˜ì§‘)
            await asyncio.sleep(60)  # 1ë¶„ê°„ ëª¨ë‹ˆí„°ë§

            # ì„±ê³µ ì‹œ ì „ì²´ ë°°í¬ë¡œ í™•ì¥
            canary_manifest["spec"]["replicas"] = deployment_manifest["spec"]["replicas"]
            self.apps_v1.patch_namespaced_deployment(
                name=canary_name,
                namespace=namespace,
                body=canary_manifest
            )

            # ê¸°ì¡´ ë°°í¬ ì œê±°
            try:
                self.apps_v1.delete_namespaced_deployment(
                    name=base_name,
                    namespace=namespace
                )
            except client.ApiException:
                pass

            logger.info(f"ì¹´ë‚˜ë¦¬ ë°°í¬ ì™„ë£Œ: {canary_name}")
            return {"status": "success", "deployment_name": canary_name, "strategy": "canary"}

        except Exception as e:
            logger.error(f"ì¹´ë‚˜ë¦¬ ë°°í¬ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ì¹´ë‚˜ë¦¬ ì œê±°
            try:
                self.apps_v1.delete_namespaced_deployment(
                    name=canary_name,
                    namespace=namespace
                )
            except:
                pass
            raise

    async def _wait_for_deployment_ready(self, name: str, namespace: str, timeout: int = 600):
        """ë°°í¬ ì¤€ë¹„ ëŒ€ê¸°"""
        start_time = datetime.now()

        while (datetime.now() - start_time).total_seconds() < timeout:
            try:
                deployment = self.apps_v1.read_namespaced_deployment(name=name, namespace=namespace)

                if deployment.status.ready_replicas == deployment.spec.replicas:
                    logger.info(f"ë°°í¬ ì¤€ë¹„ ì™„ë£Œ: {name}")
                    return True

                await asyncio.sleep(5)

            except Exception as e:
                logger.warning(f"ë°°í¬ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(5)

        raise TimeoutError(f"ë°°í¬ ì¤€ë¹„ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: {name}")

    async def _perform_health_check(self, deployment_name: str, namespace: str) -> bool:
        """í—¬ìŠ¤ì²´í¬ ìˆ˜í–‰"""
        import aiohttp

        try:
            # ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
            service_name = f"{deployment_name}-service"

            # Kubernetes ì„œë¹„ìŠ¤ ì •ë³´ ì¡°íšŒ
            service_endpoint = await self._get_service_endpoint(service_name, namespace)

            # ì‹¤ì œ HTTP í—¬ìŠ¤ì²´í¬ ìš”ì²­
            health_url = f"http://{service_endpoint}/health"

            async with aiohttp.ClientSession() as session:
                try:
                    async with session.get(health_url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                        if response.status == 200:
                            health_data = await response.json()

                            # í—¬ìŠ¤ì²´í¬ ìƒì„¸ ê²€ì¦
                            if health_data.get('status') == 'healthy':
                                # ì„¸ë¶€ ì»´í¬ë„ŒíŠ¸ ì²´í¬
                                components = health_data.get('components', {})
                                all_healthy = all(
                                    comp.get('status') == 'healthy'
                                    for comp in components.values()
                                )

                                if all_healthy:
                                    logger.info(f"í—¬ìŠ¤ì²´í¬ ì„±ê³µ: {deployment_name} - ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì •ìƒ")
                                    return True
                                else:
                                    unhealthy = [
                                        name for name, comp in components.items()
                                        if comp.get('status') != 'healthy'
                                    ]
                                    logger.warning(f"ì¼ë¶€ ì»´í¬ë„ŒíŠ¸ ë¹„ì •ìƒ: {unhealthy}")
                                    return False
                            else:
                                logger.warning(f"ì„œë¹„ìŠ¤ ìƒíƒœ ë¹„ì •ìƒ: {health_data.get('status')}")
                                return False
                        else:
                            logger.error(f"í—¬ìŠ¤ì²´í¬ HTTP ì˜¤ë¥˜: {response.status}")
                            return False

                except aiohttp.ClientTimeout:
                    logger.error(f"í—¬ìŠ¤ì²´í¬ íƒ€ì„ì•„ì›ƒ: {deployment_name}")
                    return False
                except aiohttp.ClientError as e:
                    logger.error(f"í—¬ìŠ¤ì²´í¬ ì—°ê²° ì˜¤ë¥˜: {e}")
                    return False

        except Exception as e:
            logger.error(f"í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
            return False

    async def _get_service_endpoint(self, service_name: str, namespace: str) -> str:
        """Kubernetes ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ ì¡°íšŒ"""
        try:
            # kubectlì„ ì‚¬ìš©í•˜ì—¬ ì„œë¹„ìŠ¤ ì •ë³´ ì¡°íšŒ
            cmd = f"kubectl get service {service_name} -n {namespace} -o json"
            result = await asyncio.create_subprocess_shell(
                cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await result.communicate()

            if result.returncode == 0:
                import json
                service_info = json.loads(stdout)

                # ClusterIP ë˜ëŠ” LoadBalancer IP ì¶”ì¶œ
                cluster_ip = service_info['spec'].get('clusterIP')
                port = service_info['spec']['ports'][0]['port']

                # LoadBalancer íƒ€ì…ì¸ ê²½ìš° ì™¸ë¶€ IP ì‚¬ìš©
                if service_info['spec']['type'] == 'LoadBalancer':
                    ingress = service_info.get('status', {}).get('loadBalancer', {}).get('ingress', [])
                    if ingress:
                        external_ip = ingress[0].get('ip') or ingress[0].get('hostname')
                        if external_ip:
                            return f"{external_ip}:{port}"

                # NodePort íƒ€ì…ì¸ ê²½ìš° ë…¸ë“œ IP ì‚¬ìš©
                elif service_info['spec']['type'] == 'NodePort':
                    node_port = service_info['spec']['ports'][0]['nodePort']
                    # ì²« ë²ˆì§¸ ë…¸ë“œ IP ì¡°íšŒ
                    nodes_cmd = "kubectl get nodes -o json"
                    nodes_result = await asyncio.create_subprocess_shell(
                        nodes_cmd,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    nodes_stdout, _ = await nodes_result.communicate()

                    if nodes_result.returncode == 0:
                        nodes_info = json.loads(nodes_stdout)
                        if nodes_info['items']:
                            node_ip = nodes_info['items'][0]['status']['addresses'][0]['address']
                            return f"{node_ip}:{node_port}"

                # ê¸°ë³¸: ClusterIP ì‚¬ìš© (í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ì—ì„œë§Œ ì ‘ê·¼ ê°€ëŠ¥)
                return f"{cluster_ip}:{port}"

            else:
                # ë¡œì»¬ ê°œë°œ í™˜ê²½ì—ì„œëŠ” localhost ì‚¬ìš©
                logger.warning(f"kubectl ì‹¤íŒ¨, localhost ì‚¬ìš©: {stderr.decode()}")
                return "localhost:8000"

        except Exception as e:
            logger.error(f"ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return "localhost:8000"


class CloudDeployer:
    """í´ë¼ìš°ë“œ ë°°í¬ì"""

    def __init__(self, cloud_provider: str = "aws"):
        self.cloud_provider = cloud_provider

        if cloud_provider == "aws":
            self.ec2 = boto3.client('ec2')
            self.ecs = boto3.client('ecs')
            self.ecr = boto3.client('ecr')
            self.s3 = boto3.client('s3')
            self.cloudformation = boto3.client('cloudformation')

    async def deploy_to_ecs(
        self,
        cluster_name: str,
        service_name: str,
        task_definition: Dict[str, Any],
        desired_count: int = 1
    ) -> Dict[str, Any]:
        """AWS ECSì— ë°°í¬"""
        try:
            # íƒœìŠ¤í¬ ì •ì˜ ë“±ë¡
            response = self.ecs.register_task_definition(**task_definition)
            task_def_arn = response['taskDefinition']['taskDefinitionArn']

            # ì„œë¹„ìŠ¤ ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒì„±
            try:
                self.ecs.update_service(
                    cluster=cluster_name,
                    service=service_name,
                    taskDefinition=task_def_arn,
                    desiredCount=desired_count
                )
                logger.info(f"ECS ì„œë¹„ìŠ¤ ì—…ë°ì´íŠ¸ë¨: {service_name}")
            except self.ecs.exceptions.ServiceNotFoundException:
                self.ecs.create_service(
                    cluster=cluster_name,
                    serviceName=service_name,
                    taskDefinition=task_def_arn,
                    desiredCount=desired_count
                )
                logger.info(f"ìƒˆ ECS ì„œë¹„ìŠ¤ ìƒì„±ë¨: {service_name}")

            # ë°°í¬ ì™„ë£Œ ëŒ€ê¸°
            await self._wait_for_ecs_deployment(cluster_name, service_name)

            return {
                "status": "success",
                "cluster": cluster_name,
                "service": service_name,
                "task_definition": task_def_arn
            }

        except Exception as e:
            logger.error(f"ECS ë°°í¬ ì‹¤íŒ¨: {e}")
            raise

    async def _wait_for_ecs_deployment(self, cluster_name: str, service_name: str, timeout: int = 600):
        """ECS ë°°í¬ ì™„ë£Œ ëŒ€ê¸°"""
        start_time = datetime.now()

        while (datetime.now() - start_time).total_seconds() < timeout:
            try:
                response = self.ecs.describe_services(
                    cluster=cluster_name,
                    services=[service_name]
                )

                service = response['services'][0]
                deployments = service['deployments']

                # PRIMARY ë°°í¬ê°€ STEADY ìƒíƒœì¸ì§€ í™•ì¸
                for deployment in deployments:
                    if deployment['status'] == 'PRIMARY':
                        if deployment['rolloutState'] == 'COMPLETED':
                            logger.info(f"ECS ë°°í¬ ì™„ë£Œ: {service_name}")
                            return True

                await asyncio.sleep(10)

            except Exception as e:
                logger.warning(f"ECS ë°°í¬ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(10)

        raise TimeoutError(f"ECS ë°°í¬ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: {service_name}")


class DeploymentPipeline:
    """ë°°í¬ íŒŒì´í”„ë¼ì¸"""

    def __init__(self):
        self.container_builder = ContainerBuilder()
        self.k8s_deployer = KubernetesDeployer()
        self.cloud_deployer = CloudDeployer()
        self.deployment_history: List[DeploymentResult] = []

    async def execute_pipeline(
        self,
        config: DeploymentConfig,
        pipeline_steps: List[str],
        **kwargs
    ) -> DeploymentResult:
        """ë°°í¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        deployment_id = f"deploy-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        start_time = datetime.now()

        result = DeploymentResult(
            deployment_id=deployment_id,
            status=DeploymentStatus.IN_PROGRESS,
            environment=config.environment.value,
            strategy=config.strategy.value,
            start_time=start_time
        )

        try:
            logger.info(f"ë°°í¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {deployment_id}")

            for step in pipeline_steps:
                logger.info(f"íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì‹¤í–‰: {step}")

                if step == "build":
                    await self._build_step(config, result, **kwargs)
                elif step == "test":
                    await self._test_step(config, result, **kwargs)
                elif step == "security_scan":
                    await self._security_scan_step(config, result, **kwargs)
                elif step == "deploy":
                    await self._deploy_step(config, result, **kwargs)
                elif step == "health_check":
                    await self._health_check_step(config, result, **kwargs)
                elif step == "smoke_test":
                    await self._smoke_test_step(config, result, **kwargs)
                elif step == "rollback_check":
                    await self._rollback_check_step(config, result, **kwargs)

            # ì„±ê³µ ì™„ë£Œ
            result.status = DeploymentStatus.SUCCESS
            result.end_time = datetime.now()
            result.duration = (result.end_time - result.start_time).total_seconds()

            logger.info(f"ë°°í¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {deployment_id}")

        except Exception as e:
            # ì‹¤íŒ¨ ì²˜ë¦¬
            result.status = DeploymentStatus.FAILED
            result.error_message = str(e)
            result.end_time = datetime.now()
            result.duration = (result.end_time - result.start_time).total_seconds()

            logger.error(f"ë°°í¬ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {deployment_id} - {e}")

            # ë¡¤ë°± ìˆ˜í–‰
            if config.rollback_on_failure:
                try:
                    await self._perform_rollback(config, result)
                    result.rollback_performed = True
                except Exception as rollback_error:
                    logger.error(f"ë¡¤ë°± ì‹¤íŒ¨: {rollback_error}")

        finally:
            self.deployment_history.append(result)

        return result

    async def _build_step(self, config: DeploymentConfig, result: DeploymentResult, **kwargs):
        """ë¹Œë“œ ë‹¨ê³„"""
        dockerfile_path = kwargs.get("dockerfile_path", "Dockerfile")
        context_path = kwargs.get("context_path", ".")

        image_name = await self.container_builder.build_image(
            dockerfile_path=dockerfile_path,
            image_name="fragrance-ai",
            tag=config.image_tag,
            context_path=context_path
        )

        result.logs.append(f"ì´ë¯¸ì§€ ë¹Œë“œ ì™„ë£Œ: {image_name}")

        # ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— í‘¸ì‹œ
        registry_url = kwargs.get("registry_url")
        if registry_url:
            success = await self.container_builder.push_image(image_name, registry_url)
            if not success:
                raise Exception("ì´ë¯¸ì§€ í‘¸ì‹œ ì‹¤íŒ¨")
            result.logs.append(f"ì´ë¯¸ì§€ í‘¸ì‹œ ì™„ë£Œ: {registry_url}/{image_name}")

    async def _test_step(self, config: DeploymentConfig, result: DeploymentResult, **kwargs):
        """í…ŒìŠ¤íŠ¸ ë‹¨ê³„"""
        test_command = kwargs.get("test_command", "pytest tests/")

        try:
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            process = await asyncio.create_subprocess_shell(
                test_command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            stdout, stderr = await process.communicate()

            if process.returncode != 0:
                raise Exception(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {stderr.decode()}")

            result.logs.append("ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼")

        except Exception as e:
            result.logs.append(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise

    async def _security_scan_step(self, config: DeploymentConfig, result: DeploymentResult, **kwargs):
        """ë³´ì•ˆ ìŠ¤ìº” ë‹¨ê³„"""
        try:
            # Trivyë¥¼ ì‚¬ìš©í•œ ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ë³´ì•ˆ ìŠ¤ìº”
            scan_cmd = f"trivy image --severity HIGH,CRITICAL --format json {config.image_tag}"

            scan_result = await asyncio.create_subprocess_shell(
                scan_cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await scan_result.communicate()

            if scan_result.returncode == 0:
                import json
                scan_data = json.loads(stdout) if stdout else {}

                vulnerabilities = []
                for target in scan_data.get('Results', []):
                    for vuln in target.get('Vulnerabilities', []):
                        if vuln['Severity'] in ['HIGH', 'CRITICAL']:
                            vulnerabilities.append({
                                'id': vuln.get('VulnerabilityID'),
                                'severity': vuln.get('Severity'),
                                'package': vuln.get('PkgName'),
                                'title': vuln.get('Title', 'No title')
                            })

                vulnerabilities_found = len(vulnerabilities)

                if vulnerabilities_found > 0:
                    if any(v['severity'] == 'CRITICAL' for v in vulnerabilities):
                        # CRITICAL ì·¨ì•½ì ì´ ìˆìœ¼ë©´ ë°°í¬ ì¤‘ë‹¨
                        critical_vulns = [v for v in vulnerabilities if v['severity'] == 'CRITICAL']
                        raise Exception(
                            f"CRITICAL ë³´ì•ˆ ì·¨ì•½ì  ë°œê²¬ ({len(critical_vulns)}ê°œ): "
                            f"{', '.join([v['id'] for v in critical_vulns[:3]])}"
                        )
                    else:
                        # HIGH ì·¨ì•½ì ë§Œ ìˆìœ¼ë©´ ê²½ê³ ë§Œ
                        result.logs.append(f"HIGH ë³´ì•ˆ ì·¨ì•½ì  ë°œê²¬ ({vulnerabilities_found}ê°œ)")
                        result.security_scan_results = vulnerabilities
                else:
                    result.logs.append("ë³´ì•ˆ ì·¨ì•½ì  ì—†ìŒ")

            else:
                # Trivyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° Grype ì‚¬ìš©
                logger.warning("Trivy ì‹¤í–‰ ì‹¤íŒ¨, Grypeë¡œ ëŒ€ì²´ ì‹œë„")

                grype_cmd = f"grype {config.image_tag} -o json --fail-on high"
                grype_result = await asyncio.create_subprocess_shell(
                    grype_cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                stdout, stderr = await grype_result.communicate()

                if grype_result.returncode != 0 and grype_result.returncode != 1:
                    # returncode 1ì€ ì·¨ì•½ì  ë°œê²¬, ê·¸ ì™¸ëŠ” ì˜¤ë¥˜
                    logger.warning(f"ë³´ì•ˆ ìŠ¤ìº” ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {stderr.decode()}")
                    result.logs.append("ë³´ì•ˆ ìŠ¤ìº” ìŠ¤í‚µ (ë„êµ¬ ì—†ìŒ)")
                elif grype_result.returncode == 1:
                    result.logs.append("HIGH ì´ìƒ ë³´ì•ˆ ì·¨ì•½ì  ë°œê²¬ - ê²€í†  í•„ìš”")

        except FileNotFoundError:
            # ìŠ¤ìº” ë„êµ¬ê°€ ì—†ëŠ” ê²½ìš°
            logger.warning("ë³´ì•ˆ ìŠ¤ìº” ë„êµ¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            result.logs.append("ë³´ì•ˆ ìŠ¤ìº” ìŠ¤í‚µ (ë„êµ¬ ë¯¸ì„¤ì¹˜)")

        except Exception as e:
            logger.error(f"ë³´ì•ˆ ìŠ¤ìº” ì˜¤ë¥˜: {e}")
            if "CRITICAL" in str(e):
                raise  # CRITICAL ì·¨ì•½ì ì€ ë°°í¬ ì¤‘ë‹¨
            else:
                result.logs.append(f"ë³´ì•ˆ ìŠ¤ìº” ê²½ê³ : {e}")

        result.logs.append("ë³´ì•ˆ ìŠ¤ìº” ì™„ë£Œ")

    async def _deploy_step(self, config: DeploymentConfig, result: DeploymentResult, **kwargs):
        """ë°°í¬ ë‹¨ê³„"""
        deployment_target = kwargs.get("deployment_target", "kubernetes")
        namespace = kwargs.get("namespace", "default")

        if deployment_target == "kubernetes":
            deploy_result = await self.k8s_deployer.deploy_application(config, namespace)
            result.logs.append(f"Kubernetes ë°°í¬ ì™„ë£Œ: {deploy_result}")

        elif deployment_target == "ecs":
            cluster_name = kwargs.get("cluster_name", "fragrance-ai-cluster")
            service_name = kwargs.get("service_name", "fragrance-ai-service")
            task_definition = kwargs.get("task_definition", {})

            deploy_result = await self.cloud_deployer.deploy_to_ecs(
                cluster_name, service_name, task_definition
            )
            result.logs.append(f"ECS ë°°í¬ ì™„ë£Œ: {deploy_result}")

        else:
            raise Exception(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°°í¬ ëŒ€ìƒ: {deployment_target}")

    async def _health_check_step(self, config: DeploymentConfig, result: DeploymentResult, **kwargs):
        """í—¬ìŠ¤ì²´í¬ ë‹¨ê³„"""
        health_check_url = kwargs.get("health_check_url", "http://localhost:8000/health")
        max_retries = kwargs.get("max_retries", 10)
        retry_delay = kwargs.get("retry_delay", 30)

        for attempt in range(max_retries):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(health_check_url, timeout=10) as response:
                        if response.status == 200:
                            result.logs.append("í—¬ìŠ¤ì²´í¬ ì„±ê³µ")
                            return

                        logger.warning(f"í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): HTTP {response.status}")

            except Exception as e:
                logger.warning(f"í—¬ìŠ¤ì²´í¬ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")

            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)

        raise Exception("í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨")

    async def _smoke_test_step(self, config: DeploymentConfig, result: DeploymentResult, **kwargs):
        """ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ë‹¨ê³„"""
        api_endpoint = kwargs.get("api_endpoint", "http://localhost:8000/api/v1")

        try:
            async with aiohttp.ClientSession() as session:
                # ê¸°ë³¸ API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
                test_endpoints = [
                    f"{api_endpoint}/health",
                    f"{api_endpoint}/search/semantic",
                    f"{api_endpoint}/generate/recipe"
                ]

                for endpoint in test_endpoints:
                    async with session.get(endpoint) as response:
                        if response.status not in [200, 404]:  # 404ëŠ” ì¸ì¦ ì—†ì´ëŠ” ì •ìƒ
                            logger.warning(f"ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ê²½ê³ : {endpoint} - HTTP {response.status}")

            result.logs.append("ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

        except Exception as e:
            result.logs.append(f"ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise

    async def _rollback_check_step(self, config: DeploymentConfig, result: DeploymentResult, **kwargs):
        """ë¡¤ë°± í™•ì¸ ë‹¨ê³„"""
        # ë°°í¬ í›„ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§
        monitoring_duration = kwargs.get("monitoring_duration", 300)  # 5ë¶„
        error_threshold = kwargs.get("error_threshold", 0.05)  # 5%

        logger.info(f"ë°°í¬ í›„ ëª¨ë‹ˆí„°ë§ ì‹œì‘: {monitoring_duration}ì´ˆ")

        # ì‹¤ì œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        metrics = await self._collect_real_metrics(config, monitoring_duration)

        error_rate = metrics['error_rate']
        response_time_p95 = metrics['response_time_p95']

        if error_rate > error_threshold:
            raise Exception(f"ì˜¤ë¥˜ìœ¨ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: {error_rate:.2%} > {error_threshold:.2%}")

        if response_time_p95 > 2000:  # 2ì´ˆ ì´ˆê³¼
            logger.warning(f"ì‘ë‹µì‹œê°„ì´ ë†’ìŠµë‹ˆë‹¤: {response_time_p95}ms")

        result.success_rate = (1 - error_rate) * 100
        result.metrics = {
            "error_rate": error_rate,
            "response_time_p95": response_time_p95
        }

        result.logs.append(f"ëª¨ë‹ˆí„°ë§ ì™„ë£Œ - ì„±ê³µë¥ : {result.success_rate:.2f}%")

    async def _perform_rollback(self, config: DeploymentConfig, result: DeploymentResult):
        """ë¡¤ë°± ìˆ˜í–‰"""
        logger.info("ë¡¤ë°± ì‹œì‘")

        # ì‹¤ì œ ë¡¤ë°± êµ¬í˜„
        if len(self.deployment_history) > 1:
            previous_deployment = self.deployment_history[-2]
            if previous_deployment.status == DeploymentStatus.SUCCESS:
                # ì´ì „ ì„±ê³µ ë²„ì „ìœ¼ë¡œ ë¡¤ë°±
                await self._execute_rollback(config, previous_deployment)
                logger.info(f"ì´ì „ ë²„ì „ìœ¼ë¡œ ë¡¤ë°±: {previous_deployment.deployment_id}")
                result.logs.append(f"ë¡¤ë°± ì™„ë£Œ: {previous_deployment.deployment_id}")
            else:
                raise Exception("ë¡¤ë°±í•  ì•ˆì •ì ì¸ ë²„ì „ì´ ì—†ìŠµë‹ˆë‹¤")
        else:
            raise Exception("ë¡¤ë°±í•  ì´ì „ ë²„ì „ì´ ì—†ìŠµë‹ˆë‹¤")

    def get_deployment_status(self, deployment_id: str) -> Optional[DeploymentResult]:
        """ë°°í¬ ìƒíƒœ ì¡°íšŒ"""
        for deployment in self.deployment_history:
            if deployment.deployment_id == deployment_id:
                return deployment
        return None

    async def _collect_real_metrics(self, config: DeploymentConfig, duration: int) -> Dict[str, Any]:
        """ì‹¤ì œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        metrics = {
            'error_rate': 0.0,
            'response_time_p95': 0.0,
            'cpu_usage': 0.0,
            'memory_usage': 0.0,
            'request_rate': 0.0
        }

        # Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        if config.monitoring_config.get('prometheus_url'):
            metrics.update(await self._collect_prometheus_metrics(
                config.monitoring_config['prometheus_url'],
                duration
            ))

        # CloudWatch ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (AWS)
        elif config.monitoring_config.get('cloudwatch_enabled'):
            metrics.update(await self._collect_cloudwatch_metrics(
                config.monitoring_config,
                duration
            ))

        # Datadog ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        elif config.monitoring_config.get('datadog_api_key'):
            metrics.update(await self._collect_datadog_metrics(
                config.monitoring_config,
                duration
            ))

        # ë¡œì»¬ í—¬ìŠ¤ì²´í¬ í´ë°±
        else:
            metrics.update(await self._collect_health_check_metrics(
                config.health_check_url,
                duration
            ))

        return metrics

    async def _collect_prometheus_metrics(self, prometheus_url: str, duration: int) -> Dict[str, Any]:
        """Prometheusì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        import aiohttp
        import statistics

        metrics = {}
        end_time = datetime.now()
        start_time = end_time - timedelta(seconds=duration)

        queries = {
            'error_rate': 'rate(http_requests_total{status=~"5.."}[5m])/rate(http_requests_total[5m])',
            'response_time_p95': 'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))',
            'cpu_usage': 'avg(rate(container_cpu_usage_seconds_total[5m]))',
            'memory_usage': 'avg(container_memory_usage_bytes)',
            'request_rate': 'rate(http_requests_total[5m])'
        }

        async with aiohttp.ClientSession() as session:
            for metric_name, query in queries.items():
                try:
                    async with session.get(
                        f"{prometheus_url}/api/v1/query",
                        params={'query': query}
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            if data['status'] == 'success' and data['data']['result']:
                                value = float(data['data']['result'][0]['value'][1])
                                metrics[metric_name] = value
                except Exception as e:
                    logger.warning(f"Failed to collect {metric_name} from Prometheus: {e}")
                    metrics[metric_name] = 0.0

        return metrics

    async def _collect_cloudwatch_metrics(self, config: Dict, duration: int) -> Dict[str, Any]:
        """AWS CloudWatchì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            import boto3
            from datetime import datetime, timedelta

            cloudwatch = boto3.client(
                'cloudwatch',
                region_name=config.get('aws_region', 'us-east-1'),
                aws_access_key_id=config.get('aws_access_key_id'),
                aws_secret_access_key=config.get('aws_secret_access_key')
            )

            end_time = datetime.utcnow()
            start_time = end_time - timedelta(seconds=duration)

            metrics = {}

            # ì˜¤ë¥˜ìœ¨
            error_stats = cloudwatch.get_metric_statistics(
                Namespace='AWS/ELB',
                MetricName='HTTPCode_Target_5XX_Count',
                Dimensions=[{'Name': 'LoadBalancer', 'Value': config.get('load_balancer_name', '')}],
                StartTime=start_time,
                EndTime=end_time,
                Period=60,
                Statistics=['Sum']
            )

            total_stats = cloudwatch.get_metric_statistics(
                Namespace='AWS/ELB',
                MetricName='RequestCount',
                Dimensions=[{'Name': 'LoadBalancer', 'Value': config.get('load_balancer_name', '')}],
                StartTime=start_time,
                EndTime=end_time,
                Period=60,
                Statistics=['Sum']
            )

            error_count = sum(point['Sum'] for point in error_stats.get('Datapoints', []))
            total_count = sum(point['Sum'] for point in total_stats.get('Datapoints', []))

            metrics['error_rate'] = error_count / total_count if total_count > 0 else 0.0

            # ì‘ë‹µ ì‹œê°„
            response_time_stats = cloudwatch.get_metric_statistics(
                Namespace='AWS/ELB',
                MetricName='TargetResponseTime',
                Dimensions=[{'Name': 'LoadBalancer', 'Value': config.get('load_balancer_name', '')}],
                StartTime=start_time,
                EndTime=end_time,
                Period=60,
                Statistics=['Average']
            )

            if response_time_stats['Datapoints']:
                metrics['response_time_p95'] = max(point['Average'] for point in response_time_stats['Datapoints']) * 1000

            return metrics

        except Exception as e:
            logger.error(f"CloudWatch metrics collection failed: {e}")
            return {'error_rate': 0.0, 'response_time_p95': 0.0}

    async def _collect_datadog_metrics(self, config: Dict, duration: int) -> Dict[str, Any]:
        """Datadogì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        import aiohttp

        api_key = config['datadog_api_key']
        app_key = config.get('datadog_app_key')

        headers = {
            'DD-API-KEY': api_key,
            'DD-APPLICATION-KEY': app_key
        } if app_key else {'DD-API-KEY': api_key}

        end_time = int(datetime.now().timestamp())
        start_time = end_time - duration

        metrics = {}

        queries = {
            'error_rate': 'sum:http.requests{status:5*}.as_rate()/sum:http.requests{*}.as_rate()',
            'response_time_p95': 'percentile:http.request.duration{*}:95',
            'cpu_usage': 'avg:system.cpu.user{*}',
            'memory_usage': 'avg:system.mem.used{*}'
        }

        async with aiohttp.ClientSession() as session:
            for metric_name, query in queries.items():
                try:
                    async with session.get(
                        'https://api.datadoghq.com/api/v1/query',
                        params={
                            'from': start_time,
                            'to': end_time,
                            'query': query
                        },
                        headers=headers
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            if data['series']:
                                points = data['series'][0]['pointlist']
                                if points:
                                    metrics[metric_name] = points[-1][1]  # ìµœì‹  ê°’
                except Exception as e:
                    logger.warning(f"Failed to collect {metric_name} from Datadog: {e}")
                    metrics[metric_name] = 0.0

        return metrics

    async def _collect_health_check_metrics(self, health_url: str, duration: int) -> Dict[str, Any]:
        """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        import aiohttp
        import statistics

        metrics = {
            'error_rate': 0.0,
            'response_time_p95': 0.0,
            'cpu_usage': 0.0,
            'memory_usage': 0.0
        }

        response_times = []
        error_count = 0
        total_count = 0

        # duration ë™ì•ˆ ì£¼ê¸°ì ìœ¼ë¡œ í—¬ìŠ¤ì²´í¬
        check_interval = min(5, duration // 10)  # 5ì´ˆ ë˜ëŠ” duration/10 ì¤‘ ì‘ì€ ê°’
        checks = duration // check_interval

        async with aiohttp.ClientSession() as session:
            for _ in range(checks):
                start = datetime.now()
                try:
                    async with session.get(health_url, timeout=10) as resp:
                        response_time = (datetime.now() - start).total_seconds() * 1000
                        response_times.append(response_time)

                        if resp.status >= 500:
                            error_count += 1
                        total_count += 1

                        # í—¬ìŠ¤ì²´í¬ ì‘ë‹µì—ì„œ ì¶”ê°€ ë©”íŠ¸ë¦­ ì¶”ì¶œ
                        if resp.status == 200:
                            try:
                                data = await resp.json()
                                metrics['cpu_usage'] = data.get('cpu_usage', 0.0)
                                metrics['memory_usage'] = data.get('memory_usage', 0.0)
                            except:
                                pass

                except Exception as e:
                    logger.warning(f"Health check failed: {e}")
                    error_count += 1
                    total_count += 1

                await asyncio.sleep(check_interval)

        if total_count > 0:
            metrics['error_rate'] = error_count / total_count

        if response_times:
            response_times.sort()
            p95_index = int(len(response_times) * 0.95)
            metrics['response_time_p95'] = response_times[min(p95_index, len(response_times) - 1)]

        return metrics

    async def _execute_rollback(self, config: DeploymentConfig, previous_deployment: DeploymentResult):
        """ì‹¤ì œ ë¡¤ë°± ì‹¤í–‰"""
        rollback_strategy = config.rollback_strategy

        if rollback_strategy == "blue_green":
            # Blue-Green ë¡¤ë°±: ì´ì „ í™˜ê²½ìœ¼ë¡œ íŠ¸ë˜í”½ ì „í™˜
            await self._switch_traffic_to_previous(config, previous_deployment)

        elif rollback_strategy == "canary":
            # Canary ë¡¤ë°±: ìƒˆ ë²„ì „ ì¸ìŠ¤í„´ìŠ¤ ì œê±°
            await self._remove_canary_instances(config)

        elif rollback_strategy == "rolling":
            # Rolling ë¡¤ë°±: ì´ì „ ë²„ì „ìœ¼ë¡œ ìˆœì°¨ì  ì¬ë°°í¬
            await self._rolling_rollback(config, previous_deployment)

        else:
            # ê¸°ë³¸: ì´ì „ ë²„ì „ ì¬ë°°í¬
            await self._redeploy_previous_version(config, previous_deployment)

    async def _switch_traffic_to_previous(self, config: DeploymentConfig, previous: DeploymentResult):
        """Blue-Green: ì´ì „ í™˜ê²½ìœ¼ë¡œ íŠ¸ë˜í”½ ì „í™˜"""
        load_balancer_config = config.rollback_config.get('load_balancer')

        if load_balancer_config:
            # AWS ELB, Azure Load Balancer, ë˜ëŠ” nginx ì„¤ì • ë³€ê²½
            if load_balancer_config['type'] == 'aws_elb':
                await self._update_aws_elb_target_group(
                    load_balancer_config,
                    previous.deployment_data.get('target_group_arn')
                )
            elif load_balancer_config['type'] == 'nginx':
                await self._update_nginx_upstream(
                    load_balancer_config,
                    previous.deployment_data.get('upstream_servers')
                )

        logger.info(f"Traffic switched to previous deployment: {previous.deployment_id}")

    async def _remove_canary_instances(self, config: DeploymentConfig):
        """Canary ì¸ìŠ¤í„´ìŠ¤ ì œê±°"""
        canary_instances = config.rollback_config.get('canary_instances', [])

        for instance_id in canary_instances:
            # Kubernetes, Docker, ë˜ëŠ” í´ë¼ìš°ë“œ ì¸ìŠ¤í„´ìŠ¤ ì œê±°
            if config.deployment_type == "kubernetes":
                await self._delete_k8s_deployment(instance_id)
            elif config.deployment_type == "docker":
                await self._stop_docker_container(instance_id)
            elif config.deployment_type == "ec2":
                await self._terminate_ec2_instance(instance_id)

        logger.info(f"Removed {len(canary_instances)} canary instances")

    async def _rolling_rollback(self, config: DeploymentConfig, previous: DeploymentResult):
        """ë¡¤ë§ ë¡¤ë°± ì‹¤í–‰"""
        instances = config.rollback_config.get('instances', [])
        batch_size = config.rollback_config.get('batch_size', 1)

        for i in range(0, len(instances), batch_size):
            batch = instances[i:i+batch_size]

            for instance_id in batch:
                await self._rollback_instance(instance_id, previous.deployment_data)

            # ë°°ì¹˜ ê°„ ëŒ€ê¸°
            await asyncio.sleep(config.rollback_config.get('batch_delay', 30))

            # í—¬ìŠ¤ì²´í¬
            await self._verify_instance_health(batch)

        logger.info(f"Rolling rollback completed for {len(instances)} instances")

    async def _redeploy_previous_version(self, config: DeploymentConfig, previous: DeploymentResult):
        """ì´ì „ ë²„ì „ ì¬ë°°í¬"""
        deployment_commands = previous.deployment_data.get('deployment_commands', [])

        for command in deployment_commands:
            # ë°°í¬ ëª…ë ¹ ì‹¤í–‰
            result = await self._execute_deployment_command(command)
            if not result['success']:
                raise Exception(f"Rollback command failed: {command}")

        logger.info(f"Previous version redeployed: {previous.deployment_id}")

    async def _update_aws_elb_target_group(self, load_balancer_config: Dict, target_group_arn: str):
        """AWS ELB íƒ€ê²Ÿ ê·¸ë£¹ ì—…ë°ì´íŠ¸"""
        try:
            import boto3
            elbv2 = boto3.client('elbv2',
                                  region_name=load_balancer_config.get('region', 'us-east-1'))

            # í˜„ì¬ íƒ€ê²Ÿ ê·¸ë£¹ì˜ íƒ€ê²Ÿ í•´ì œ
            current_targets = elbv2.describe_target_health(TargetGroupArn=target_group_arn)
            if current_targets['TargetHealthDescriptions']:
                targets_to_deregister = [
                    {'Id': t['Target']['Id']} for t in current_targets['TargetHealthDescriptions']
                ]
                elbv2.deregister_targets(
                    TargetGroupArn=target_group_arn,
                    Targets=targets_to_deregister
                )

            # ìƒˆ íƒ€ê²Ÿ ë“±ë¡
            new_targets = load_balancer_config.get('new_targets', [])
            if new_targets:
                elbv2.register_targets(
                    TargetGroupArn=target_group_arn,
                    Targets=new_targets
                )

            logger.info(f"AWS ELB íƒ€ê²Ÿ ê·¸ë£¹ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {target_group_arn}")

        except Exception as e:
            logger.error(f"AWS ELB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            raise

    async def _update_nginx_upstream(self, load_balancer_config: Dict, upstream_servers: List[str]):
        """Nginx upstream ì„¤ì • ì—…ë°ì´íŠ¸"""
        import aiohttp

        try:
            nginx_api_url = load_balancer_config.get('nginx_api_url', 'http://nginx-plus-api:8080')
            upstream_name = load_balancer_config.get('upstream_name', 'backend')

            # Nginx Plus APIë¥¼ ì‚¬ìš©í•œ ë™ì  upstream ì—…ë°ì´íŠ¸
            async with aiohttp.ClientSession() as session:
                # í˜„ì¬ ì„œë²„ ëª©ë¡ ì¡°íšŒ
                async with session.get(f"{nginx_api_url}/api/6/http/upstreams/{upstream_name}/servers") as resp:
                    if resp.status == 200:
                        current_servers = await resp.json()

                        # ê¸°ì¡´ ì„œë²„ ì œê±°
                        for server in current_servers:
                            await session.delete(
                                f"{nginx_api_url}/api/6/http/upstreams/{upstream_name}/servers/{server['id']}"
                            )

                # ìƒˆ ì„œë²„ ì¶”ê°€
                for server in upstream_servers:
                    server_config = {
                        "server": server,
                        "weight": 1,
                        "max_fails": 3,
                        "fail_timeout": "10s"
                    }
                    await session.post(
                        f"{nginx_api_url}/api/6/http/upstreams/{upstream_name}/servers",
                        json=server_config
                    )

            logger.info(f"Nginx upstream ì—…ë°ì´íŠ¸ ì™„ë£Œ: {upstream_name}")

        except Exception as e:
            logger.error(f"Nginx upstream ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            # Fallback: ì„¤ì • íŒŒì¼ ì§ì ‘ ìˆ˜ì •
            await self._update_nginx_config_file(load_balancer_config, upstream_servers)

    async def _update_nginx_config_file(self, load_balancer_config: Dict, upstream_servers: List[str]):
        """Nginx ì„¤ì • íŒŒì¼ ì§ì ‘ ì—…ë°ì´íŠ¸"""
        config_path = load_balancer_config.get('config_path', '/etc/nginx/conf.d/upstream.conf')
        upstream_name = load_balancer_config.get('upstream_name', 'backend')

        upstream_config = f"upstream {upstream_name} {{\n"
        for server in upstream_servers:
            upstream_config += f"    server {server};\n"
        upstream_config += "}\n"

        # SSHë¥¼ í†µí•œ ì›ê²© íŒŒì¼ ì—…ë°ì´íŠ¸
        nginx_host = load_balancer_config.get('nginx_host', 'nginx-server')
        cmd = f"ssh {nginx_host} 'echo \"{upstream_config}\" > {config_path} && nginx -s reload'"

        result = await asyncio.create_subprocess_shell(
            cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await result.communicate()

        if result.returncode != 0:
            raise Exception(f"Nginx config update failed: {stderr.decode()}")

    async def _delete_k8s_deployment(self, instance_id: str):
        """Kubernetes ë””í”Œë¡œì´ë¨¼íŠ¸ ì‚­ì œ"""
        try:
            namespace, deployment_name = instance_id.split('/', 1)
            cmd = f"kubectl delete deployment {deployment_name} -n {namespace}"

            result = await asyncio.create_subprocess_shell(
                cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await result.communicate()

            if result.returncode == 0:
                logger.info(f"K8s deployment ì‚­ì œ ì™„ë£Œ: {instance_id}")
            else:
                raise Exception(f"K8s deployment ì‚­ì œ ì‹¤íŒ¨: {stderr.decode()}")

        except Exception as e:
            logger.error(f"K8s deployment ì‚­ì œ ì˜¤ë¥˜: {e}")
            raise

    async def _stop_docker_container(self, instance_id: str):
        """Docker ì»¨í…Œì´ë„ˆ ì¤‘ì§€"""
        try:
            # ë¡œì»¬ ë˜ëŠ” ì›ê²© Docker ë°ëª¬ì— ì—°ê²°
            cmd = f"docker stop {instance_id} && docker rm {instance_id}"

            result = await asyncio.create_subprocess_shell(
                cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await result.communicate()

            if result.returncode == 0:
                logger.info(f"Docker container ì¤‘ì§€ ì™„ë£Œ: {instance_id}")
            else:
                # ì´ë¯¸ ì¤‘ì§€ëœ ì»¨í…Œì´ë„ˆì¼ ìˆ˜ ìˆìŒ
                logger.warning(f"Docker container ì¤‘ì§€ ê²½ê³ : {stderr.decode()}")

        except Exception as e:
            logger.error(f"Docker container ì¤‘ì§€ ì˜¤ë¥˜: {e}")
            raise

    async def _terminate_ec2_instance(self, instance_id: str):
        """EC2 ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ"""
        try:
            import boto3
            ec2 = boto3.client('ec2')

            # ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ
            response = ec2.terminate_instances(InstanceIds=[instance_id])

            # ì¢…ë£Œ ëŒ€ê¸°
            waiter = ec2.get_waiter('instance_terminated')
            waiter.wait(
                InstanceIds=[instance_id],
                WaiterConfig={
                    'Delay': 5,
                    'MaxAttempts': 60  # ìµœëŒ€ 5ë¶„ ëŒ€ê¸°
                }
            )

            logger.info(f"EC2 instance ì¢…ë£Œ ì™„ë£Œ: {instance_id}")

        except Exception as e:
            logger.error(f"EC2 instance ì¢…ë£Œ ì˜¤ë¥˜: {e}")
            raise

    async def _rollback_instance(self, instance_id: str, deployment_data: Dict):
        """ê°œë³„ ì¸ìŠ¤í„´ìŠ¤ ë¡¤ë°±"""
        instance_type = deployment_data.get('instance_type', 'kubernetes')

        if instance_type == 'kubernetes':
            # K8s Pod ì¬ì‹œì‘ ë˜ëŠ” ì´ë¯¸ì§€ ë³€ê²½
            namespace, pod_name = instance_id.split('/', 1)
            previous_image = deployment_data.get('previous_image')

            cmd = f"kubectl set image deployment/{pod_name} *={previous_image} -n {namespace}"
            await asyncio.create_subprocess_shell(cmd)

        elif instance_type == 'docker':
            # Docker ì»¨í…Œì´ë„ˆ ì¬ìƒì„±
            previous_image = deployment_data.get('previous_image')
            container_config = deployment_data.get('container_config', {})

            # ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì¤‘ì§€
            await self._stop_docker_container(instance_id)

            # ìƒˆ ì»¨í…Œì´ë„ˆ ì‹œì‘
            docker_run_cmd = f"docker run -d --name {instance_id} {previous_image}"
            await asyncio.create_subprocess_shell(docker_run_cmd)

        elif instance_type == 'ec2':
            # EC2 ì¸ìŠ¤í„´ìŠ¤ AMI ë³€ê²½ (ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘)
            await self._launch_ec2_from_ami(
                instance_id,
                deployment_data.get('previous_ami')
            )

    async def _launch_ec2_from_ami(self, instance_name: str, ami_id: str):
        """AMIë¡œë¶€í„° EC2 ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘"""
        try:
            import boto3
            ec2 = boto3.client('ec2')

            # ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘
            response = ec2.run_instances(
                ImageId=ami_id,
                MinCount=1,
                MaxCount=1,
                InstanceType='t2.micro',  # ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
                TagSpecifications=[
                    {
                        'ResourceType': 'instance',
                        'Tags': [
                            {'Key': 'Name', 'Value': instance_name},
                            {'Key': 'Environment', 'Value': 'rollback'}
                        ]
                    }
                ]
            )

            instance_id = response['Instances'][0]['InstanceId']
            logger.info(f"ìƒˆ EC2 ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘: {instance_id}")
            return instance_id

        except Exception as e:
            logger.error(f"EC2 ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘ ì‹¤íŒ¨: {e}")
            raise

    async def _verify_instance_health(self, instances: List[str]):
        """ì¸ìŠ¤í„´ìŠ¤ í—¬ìŠ¤ í™•ì¸"""
        health_checks = []

        for instance_id in instances:
            # ê° ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ì— ë”°ë¥¸ í—¬ìŠ¤ì²´í¬
            if '/' in instance_id:  # Kubernetes
                health_checks.append(self._check_k8s_pod_health(instance_id))
            elif instance_id.startswith('i-'):  # EC2
                health_checks.append(self._check_ec2_health(instance_id))
            else:  # Docker
                health_checks.append(self._check_docker_health(instance_id))

        results = await asyncio.gather(*health_checks, return_exceptions=True)

        for instance_id, result in zip(instances, results):
            if isinstance(result, Exception):
                logger.error(f"í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨ {instance_id}: {result}")
            elif not result:
                logger.warning(f"ì¸ìŠ¤í„´ìŠ¤ ë¹„ì •ìƒ {instance_id}")

    async def _check_k8s_pod_health(self, pod_id: str) -> bool:
        """Kubernetes Pod í—¬ìŠ¤ ì²´í¬"""
        namespace, pod_name = pod_id.split('/', 1)
        cmd = f"kubectl get pod {pod_name} -n {namespace} -o json"

        result = await asyncio.create_subprocess_shell(
            cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await result.communicate()

        if result.returncode == 0:
            import json
            pod_info = json.loads(stdout)
            return pod_info['status']['phase'] == 'Running'
        return False

    async def _check_ec2_health(self, instance_id: str) -> bool:
        """EC2 ì¸ìŠ¤í„´ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
        try:
            import boto3
            ec2 = boto3.client('ec2')

            response = ec2.describe_instance_status(InstanceIds=[instance_id])
            if response['InstanceStatuses']:
                status = response['InstanceStatuses'][0]
                return (status['InstanceStatus']['Status'] == 'ok' and
                        status['SystemStatus']['Status'] == 'ok')
            return False
        except:
            return False

    async def _check_docker_health(self, container_id: str) -> bool:
        """Docker ì»¨í…Œì´ë„ˆ í—¬ìŠ¤ ì²´í¬"""
        cmd = f"docker inspect --format='{{{{.State.Health.Status}}}}' {container_id}"

        result = await asyncio.create_subprocess_shell(
            cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await result.communicate()

        if result.returncode == 0:
            health_status = stdout.decode().strip()
            return health_status in ['healthy', 'starting']
        return False

    async def _execute_deployment_command(self, command: Dict) -> Dict[str, Any]:
        """ë°°í¬ ëª…ë ¹ ì‹¤í–‰"""
        cmd_type = command.get('type')
        cmd_str = command.get('command')

        try:
            if cmd_type == 'shell':
                result = await asyncio.create_subprocess_shell(
                    cmd_str,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                stdout, stderr = await result.communicate()

                return {
                    'success': result.returncode == 0,
                    'stdout': stdout.decode(),
                    'stderr': stderr.decode()
                }

            elif cmd_type == 'kubernetes':
                # kubectl ëª…ë ¹ ì‹¤í–‰
                return await self._execute_kubectl_command(cmd_str)

            elif cmd_type == 'docker':
                # docker ëª…ë ¹ ì‹¤í–‰
                return await self._execute_docker_command(cmd_str)

            else:
                raise ValueError(f"Unknown command type: {cmd_type}")

        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

    async def _execute_kubectl_command(self, command: str) -> Dict[str, Any]:
        """kubectl ëª…ë ¹ ì‹¤í–‰"""
        full_cmd = f"kubectl {command}"
        result = await asyncio.create_subprocess_shell(
            full_cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await result.communicate()

        return {
            'success': result.returncode == 0,
            'stdout': stdout.decode(),
            'stderr': stderr.decode()
        }

    async def _execute_docker_command(self, command: str) -> Dict[str, Any]:
        """docker ëª…ë ¹ ì‹¤í–‰"""
        full_cmd = f"docker {command}"
        result = await asyncio.create_subprocess_shell(
            full_cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await result.communicate()

        return {
            'success': result.returncode == 0,
            'stdout': stdout.decode(),
            'stderr': stderr.decode()
        }

    def get_deployment_history(self, limit: int = 10) -> List[DeploymentResult]:
        """ë°°í¬ ì´ë ¥ ì¡°íšŒ"""
        return self.deployment_history[-limit:]


# ì „ì—­ ë°°í¬ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤
global_pipeline: Optional[DeploymentPipeline] = None


def get_deployment_pipeline() -> DeploymentPipeline:
    """ì „ì—­ ë°°í¬ íŒŒì´í”„ë¼ì¸ ê°€ì ¸ì˜¤ê¸°"""
    global global_pipeline
    if global_pipeline is None:
        global_pipeline = DeploymentPipeline()
    return global_pipeline


# í¸ì˜ í•¨ìˆ˜ë“¤
async def deploy_to_production(
    image_tag: str,
    strategy: DeploymentStrategy = DeploymentStrategy.ROLLING_UPDATE,
    replicas: int = 3
) -> DeploymentResult:
    """í”„ë¡œë•ì…˜ ë°°í¬"""
    config = DeploymentConfig(
        environment=DeploymentEnvironment.PRODUCTION,
        strategy=strategy,
        image_tag=image_tag,
        replicas=replicas,
        resources={
            "requests": {"cpu": "500m", "memory": "1Gi"},
            "limits": {"cpu": "1000m", "memory": "2Gi"}
        }
    )

    pipeline_steps = [
        "build",
        "test",
        "security_scan",
        "deploy",
        "health_check",
        "smoke_test",
        "rollback_check"
    ]

    pipeline = get_deployment_pipeline()
    return await pipeline.execute_pipeline(config, pipeline_steps)


async def deploy_to_staging(image_tag: str) -> DeploymentResult:
    """ìŠ¤í…Œì´ì§• ë°°í¬"""
    config = DeploymentConfig(
        environment=DeploymentEnvironment.STAGING,
        strategy=DeploymentStrategy.RECREATE,
        image_tag=image_tag,
        replicas=1,
        resources={
            "requests": {"cpu": "250m", "memory": "512Mi"},
            "limits": {"cpu": "500m", "memory": "1Gi"}
        }
    )

    pipeline_steps = ["build", "test", "deploy", "health_check"]

    pipeline = get_deployment_pipeline()
    return await pipeline.execute_pipeline(config, pipeline_steps)