# üöÄ ÏôÑÎ≤ΩÌïú ÌîÑÎ°úÎçïÏÖò Î∞∞Ìè¨ ÏûêÎèôÌôî ÏãúÏä§ÌÖú
import asyncio
import json
import yaml
import subprocess
import shutil
import zipfile
import tarfile
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any, Union, Callable
from dataclasses import dataclass, field, asdict
from enum import Enum
from pathlib import Path
import aiofiles
import aiohttp
import docker
import kubernetes
from kubernetes import client, config
import boto3
import structlog
from jinja2 import Template, Environment, FileSystemLoader
import hashlib
import tempfile
import os

logger = structlog.get_logger("deployment_automation")


class DeploymentEnvironment(Enum):
    """Î∞∞Ìè¨ ÌôòÍ≤Ω"""
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION = "production"
    CANARY = "canary"
    BLUE_GREEN = "blue_green"


class DeploymentStrategy(Enum):
    """Î∞∞Ìè¨ Ï†ÑÎûµ"""
    ROLLING_UPDATE = "rolling_update"
    BLUE_GREEN = "blue_green"
    CANARY = "canary"
    RECREATE = "recreate"
    A_B_TESTING = "a_b_testing"


class DeploymentStatus(Enum):
    """Î∞∞Ìè¨ ÏÉÅÌÉú"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    SUCCESS = "success"
    FAILED = "failed"
    ROLLBACK = "rollback"
    CANCELLED = "cancelled"


@dataclass
class DeploymentConfig:
    """Î∞∞Ìè¨ ÏÑ§Ï†ï"""
    environment: DeploymentEnvironment
    strategy: DeploymentStrategy
    image_tag: str
    replicas: int
    resources: Dict[str, Any]
    environment_variables: Dict[str, str] = field(default_factory=dict)
    secrets: Dict[str, str] = field(default_factory=dict)
    health_check: Dict[str, Any] = field(default_factory=dict)
    rollback_on_failure: bool = True
    auto_scaling: Dict[str, Any] = field(default_factory=dict)
    monitoring: Dict[str, Any] = field(default_factory=dict)


@dataclass
class DeploymentResult:
    """Î∞∞Ìè¨ Í≤∞Í≥º"""
    deployment_id: str
    status: DeploymentStatus
    environment: str
    strategy: str
    start_time: datetime
    end_time: Optional[datetime] = None
    duration: Optional[float] = None
    success_rate: Optional[float] = None
    error_message: Optional[str] = None
    rollback_performed: bool = False
    metrics: Dict[str, Any] = field(default_factory=dict)
    logs: List[str] = field(default_factory=list)


@dataclass
class HealthCheck:
    """Ìó¨Ïä§Ï≤¥ÌÅ¨ ÏÑ§Ï†ï"""
    endpoint: str
    method: str = "GET"
    expected_status: int = 200
    timeout: int = 30
    interval: int = 10
    retries: int = 3
    headers: Dict[str, str] = field(default_factory=dict)


class ContainerBuilder:
    """Ïª®ÌÖåÏù¥ÎÑà Ïù¥ÎØ∏ÏßÄ ÎπåÎçî"""

    def __init__(self, docker_client: Optional[docker.DockerClient] = None):
        self.docker_client = docker_client or docker.from_env()

    async def build_image(
        self,
        dockerfile_path: str,
        image_name: str,
        tag: str,
        build_args: Dict[str, str] = None,
        context_path: str = "."
    ) -> str:
        """Docker Ïù¥ÎØ∏ÏßÄ ÎπåÎìú"""
        full_image_name = f"{image_name}:{tag}"

        try:
            logger.info(f"Docker Ïù¥ÎØ∏ÏßÄ ÎπåÎìú ÏãúÏûë: {full_image_name}")

            # ÎπåÎìú Î°úÍ∑∏Î•º ÏúÑÌïú ÏΩúÎ∞±
            build_logs = []

            def log_callback(stream):
                if 'stream' in stream:
                    log_line = stream['stream'].strip()
                    if log_line:
                        build_logs.append(log_line)
                        logger.info(f"ÎπåÎìú: {log_line}")

            # Ïù¥ÎØ∏ÏßÄ ÎπåÎìú
            image, build_log = self.docker_client.images.build(
                path=context_path,
                dockerfile=dockerfile_path,
                tag=full_image_name,
                buildargs=build_args or {},
                rm=True,
                forcerm=True
            )

            # ÎπåÎìú Î°úÍ∑∏ Ï≤òÎ¶¨
            for log in build_log:
                log_callback(log)

            logger.info(f"Docker Ïù¥ÎØ∏ÏßÄ ÎπåÎìú ÏôÑÎ£å: {full_image_name}")
            return full_image_name

        except Exception as e:
            logger.error(f"Docker Ïù¥ÎØ∏ÏßÄ ÎπåÎìú Ïã§Ìå®: {e}")
            raise

    async def push_image(self, image_name: str, registry_url: str = None) -> bool:
        """Ïù¥ÎØ∏ÏßÄ Î†àÏßÄÏä§Ìä∏Î¶¨Ïóê Ìë∏Ïãú"""
        try:
            if registry_url:
                # Î†àÏßÄÏä§Ìä∏Î¶¨ ÌÉúÍ∑∏ Ï∂îÍ∞Ä
                registry_image = f"{registry_url}/{image_name}"
                self.docker_client.api.tag(image_name, registry_image)
                image_name = registry_image

            logger.info(f"Ïù¥ÎØ∏ÏßÄ Ìë∏Ïãú ÏãúÏûë: {image_name}")

            # Ìë∏Ïãú Ïã§Ìñâ
            push_log = self.docker_client.images.push(image_name, stream=True, decode=True)

            for log in push_log:
                if 'status' in log:
                    logger.info(f"Ìë∏Ïãú: {log['status']}")

            logger.info(f"Ïù¥ÎØ∏ÏßÄ Ìë∏Ïãú ÏôÑÎ£å: {image_name}")
            return True

        except Exception as e:
            logger.error(f"Ïù¥ÎØ∏ÏßÄ Ìë∏Ïãú Ïã§Ìå®: {e}")
            return False

    def get_image_info(self, image_name: str) -> Dict[str, Any]:
        """Ïù¥ÎØ∏ÏßÄ Ï†ïÎ≥¥ Ï°∞Ìöå"""
        try:
            image = self.docker_client.images.get(image_name)
            return {
                "id": image.id,
                "tags": image.tags,
                "created": image.attrs.get("Created"),
                "size": image.attrs.get("Size"),
                "architecture": image.attrs.get("Architecture"),
                "os": image.attrs.get("Os")
            }
        except Exception as e:
            logger.error(f"Ïù¥ÎØ∏ÏßÄ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {}


class KubernetesDeployer:
    """Kubernetes Î∞∞Ìè¨Ïûê"""

    def __init__(self, kubeconfig_path: str = None):
        if kubeconfig_path:
            config.load_kube_config(config_file=kubeconfig_path)
        else:
            try:
                config.load_incluster_config()
            except:
                config.load_kube_config()

        self.apps_v1 = client.AppsV1Api()
        self.core_v1 = client.CoreV1Api()
        self.autoscaling_v1 = client.AutoscalingV1Api()
        self.networking_v1 = client.NetworkingV1Api()

    async def deploy_application(
        self,
        config: DeploymentConfig,
        namespace: str = "default"
    ) -> Dict[str, Any]:
        """Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò Î∞∞Ìè¨"""
        try:
            deployment_name = f"fragrance-ai-{config.environment.value}"

            # Î∞∞Ìè¨ Îß§ÎãàÌéòÏä§Ìä∏ ÏÉùÏÑ±
            deployment_manifest = self._create_deployment_manifest(config, deployment_name, namespace)

            # ÏÑúÎπÑÏä§ Îß§ÎãàÌéòÏä§Ìä∏ ÏÉùÏÑ±
            service_manifest = self._create_service_manifest(deployment_name, namespace)

            # ConfigMap ÏÉùÏÑ± (ÌôòÍ≤Ω Î≥ÄÏàòÏö©)
            if config.environment_variables:
                configmap_manifest = self._create_configmap_manifest(
                    deployment_name, config.environment_variables, namespace
                )
                await self._apply_configmap(configmap_manifest)

            # Secret ÏÉùÏÑ±
            if config.secrets:
                secret_manifest = self._create_secret_manifest(
                    deployment_name, config.secrets, namespace
                )
                await self._apply_secret(secret_manifest)

            # Î∞∞Ìè¨ Ï†ÑÎûµÏóê Îî∞Î•∏ Î∞∞Ìè¨ Ïã§Ìñâ
            if config.strategy == DeploymentStrategy.BLUE_GREEN:
                return await self._blue_green_deploy(deployment_manifest, service_manifest, namespace)
            elif config.strategy == DeploymentStrategy.CANARY:
                return await self._canary_deploy(deployment_manifest, service_manifest, namespace)
            else:
                return await self._rolling_update_deploy(deployment_manifest, service_manifest, namespace)

        except Exception as e:
            logger.error(f"Kubernetes Î∞∞Ìè¨ Ïã§Ìå®: {e}")
            raise

    def _create_deployment_manifest(
        self,
        config: DeploymentConfig,
        name: str,
        namespace: str
    ) -> Dict[str, Any]:
        """Î∞∞Ìè¨ Îß§ÎãàÌéòÏä§Ìä∏ ÏÉùÏÑ±"""
        return {
            "apiVersion": "apps/v1",
            "kind": "Deployment",
            "metadata": {
                "name": name,
                "namespace": namespace,
                "labels": {
                    "app": "fragrance-ai",
                    "environment": config.environment.value,
                    "version": config.image_tag
                }
            },
            "spec": {
                "replicas": config.replicas,
                "strategy": {
                    "type": "RollingUpdate",
                    "rollingUpdate": {
                        "maxSurge": 1,
                        "maxUnavailable": 0
                    }
                },
                "selector": {
                    "matchLabels": {
                        "app": "fragrance-ai",
                        "environment": config.environment.value
                    }
                },
                "template": {
                    "metadata": {
                        "labels": {
                            "app": "fragrance-ai",
                            "environment": config.environment.value,
                            "version": config.image_tag
                        }
                    },
                    "spec": {
                        "containers": [{
                            "name": "fragrance-ai",
                            "image": f"fragrance-ai:{config.image_tag}",
                            "ports": [{"containerPort": 8000}],
                            "resources": config.resources,
                            "envFrom": [
                                {"configMapRef": {"name": f"{name}-config"}},
                                {"secretRef": {"name": f"{name}-secrets"}}
                            ],
                            "livenessProbe": {
                                "httpGet": {
                                    "path": "/health",
                                    "port": 8000
                                },
                                "initialDelaySeconds": 30,
                                "periodSeconds": 10
                            },
                            "readinessProbe": {
                                "httpGet": {
                                    "path": "/ready",
                                    "port": 8000
                                },
                                "initialDelaySeconds": 5,
                                "periodSeconds": 5
                            }
                        }]
                    }
                }
            }
        }

    def _create_service_manifest(self, name: str, namespace: str) -> Dict[str, Any]:
        """ÏÑúÎπÑÏä§ Îß§ÎãàÌéòÏä§Ìä∏ ÏÉùÏÑ±"""
        return {
            "apiVersion": "v1",
            "kind": "Service",
            "metadata": {
                "name": f"{name}-service",
                "namespace": namespace
            },
            "spec": {
                "selector": {
                    "app": "fragrance-ai"
                },
                "ports": [{
                    "port": 80,
                    "targetPort": 8000,
                    "protocol": "TCP"
                }],
                "type": "ClusterIP"
            }
        }

    def _create_configmap_manifest(
        self,
        name: str,
        data: Dict[str, str],
        namespace: str
    ) -> Dict[str, Any]:
        """ConfigMap Îß§ÎãàÌéòÏä§Ìä∏ ÏÉùÏÑ±"""
        return {
            "apiVersion": "v1",
            "kind": "ConfigMap",
            "metadata": {
                "name": f"{name}-config",
                "namespace": namespace
            },
            "data": data
        }

    def _create_secret_manifest(
        self,
        name: str,
        data: Dict[str, str],
        namespace: str
    ) -> Dict[str, Any]:
        """Secret Îß§ÎãàÌéòÏä§Ìä∏ ÏÉùÏÑ±"""
        import base64

        encoded_data = {
            key: base64.b64encode(value.encode()).decode()
            for key, value in data.items()
        }

        return {
            "apiVersion": "v1",
            "kind": "Secret",
            "metadata": {
                "name": f"{name}-secrets",
                "namespace": namespace
            },
            "type": "Opaque",
            "data": encoded_data
        }

    async def _apply_configmap(self, manifest: Dict[str, Any]):
        """ConfigMap Ï†ÅÏö©"""
        try:
            self.core_v1.create_namespaced_config_map(
                namespace=manifest["metadata"]["namespace"],
                body=manifest
            )
        except client.ApiException as e:
            if e.status == 409:  # Already exists
                self.core_v1.patch_namespaced_config_map(
                    name=manifest["metadata"]["name"],
                    namespace=manifest["metadata"]["namespace"],
                    body=manifest
                )
            else:
                raise

    async def _apply_secret(self, manifest: Dict[str, Any]):
        """Secret Ï†ÅÏö©"""
        try:
            self.core_v1.create_namespaced_secret(
                namespace=manifest["metadata"]["namespace"],
                body=manifest
            )
        except client.ApiException as e:
            if e.status == 409:  # Already exists
                self.core_v1.patch_namespaced_secret(
                    name=manifest["metadata"]["name"],
                    namespace=manifest["metadata"]["namespace"],
                    body=manifest
                )
            else:
                raise

    async def _rolling_update_deploy(
        self,
        deployment_manifest: Dict[str, Any],
        service_manifest: Dict[str, Any],
        namespace: str
    ) -> Dict[str, Any]:
        """Î°§ÎßÅ ÏóÖÎç∞Ïù¥Ìä∏ Î∞∞Ìè¨"""
        deployment_name = deployment_manifest["metadata"]["name"]

        try:
            # Í∏∞Ï°¥ Î∞∞Ìè¨ ÌôïÏù∏
            try:
                existing_deployment = self.apps_v1.read_namespaced_deployment(
                    name=deployment_name,
                    namespace=namespace
                )
                # ÏóÖÎç∞Ïù¥Ìä∏
                self.apps_v1.patch_namespaced_deployment(
                    name=deployment_name,
                    namespace=namespace,
                    body=deployment_manifest
                )
                logger.info(f"Î∞∞Ìè¨ ÏóÖÎç∞Ïù¥Ìä∏Îê®: {deployment_name}")
            except client.ApiException as e:
                if e.status == 404:
                    # ÏÉà Î∞∞Ìè¨ ÏÉùÏÑ±
                    self.apps_v1.create_namespaced_deployment(
                        namespace=namespace,
                        body=deployment_manifest
                    )
                    logger.info(f"ÏÉà Î∞∞Ìè¨ ÏÉùÏÑ±Îê®: {deployment_name}")
                else:
                    raise

            # ÏÑúÎπÑÏä§ ÏÉùÏÑ±/ÏóÖÎç∞Ïù¥Ìä∏
            try:
                self.core_v1.create_namespaced_service(
                    namespace=namespace,
                    body=service_manifest
                )
            except client.ApiException as e:
                if e.status == 409:
                    pass  # Service already exists
                else:
                    raise

            # Î∞∞Ìè¨ ÏôÑÎ£å ÎåÄÍ∏∞
            await self._wait_for_deployment_ready(deployment_name, namespace)

            return {
                "status": "success",
                "deployment_name": deployment_name,
                "strategy": "rolling_update"
            }

        except Exception as e:
            logger.error(f"Î°§ÎßÅ ÏóÖÎç∞Ïù¥Ìä∏ Ïã§Ìå®: {e}")
            raise

    async def _blue_green_deploy(
        self,
        deployment_manifest: Dict[str, Any],
        service_manifest: Dict[str, Any],
        namespace: str
    ) -> Dict[str, Any]:
        """Î∏îÎ£®-Í∑∏Î¶∞ Î∞∞Ìè¨"""
        base_name = deployment_manifest["metadata"]["name"]
        green_name = f"{base_name}-green"

        try:
            # Í∑∏Î¶∞ ÌôòÍ≤Ω Î∞∞Ìè¨
            green_manifest = deployment_manifest.copy()
            green_manifest["metadata"]["name"] = green_name
            green_manifest["spec"]["selector"]["matchLabels"]["deployment"] = "green"
            green_manifest["spec"]["template"]["metadata"]["labels"]["deployment"] = "green"

            self.apps_v1.create_namespaced_deployment(
                namespace=namespace,
                body=green_manifest
            )

            # Í∑∏Î¶∞ ÌôòÍ≤Ω Ï§ÄÎπÑ ÎåÄÍ∏∞
            await self._wait_for_deployment_ready(green_name, namespace)

            # Ìó¨Ïä§Ï≤¥ÌÅ¨ ÏàòÌñâ
            if await self._perform_health_check(green_name, namespace):
                # ÏÑúÎπÑÏä§Î•º Í∑∏Î¶∞ÏúºÎ°ú Ï†ÑÌôò
                service_manifest["spec"]["selector"]["deployment"] = "green"
                self.core_v1.patch_namespaced_service(
                    name=service_manifest["metadata"]["name"],
                    namespace=namespace,
                    body=service_manifest
                )

                # Î∏îÎ£® ÌôòÍ≤Ω Ï†úÍ±∞ (ÏÑ†ÌÉùÏ†Å)
                try:
                    self.apps_v1.delete_namespaced_deployment(
                        name=base_name,
                        namespace=namespace
                    )
                except client.ApiException:
                    pass

                logger.info(f"Î∏îÎ£®-Í∑∏Î¶∞ Î∞∞Ìè¨ ÏôÑÎ£å: {green_name}")
                return {"status": "success", "deployment_name": green_name, "strategy": "blue_green"}

            else:
                # Ìó¨Ïä§Ï≤¥ÌÅ¨ Ïã§Ìå® Ïãú Í∑∏Î¶∞ ÌôòÍ≤Ω Ï†úÍ±∞
                self.apps_v1.delete_namespaced_deployment(
                    name=green_name,
                    namespace=namespace
                )
                raise Exception("Ìó¨Ïä§Ï≤¥ÌÅ¨ Ïã§Ìå®")

        except Exception as e:
            logger.error(f"Î∏îÎ£®-Í∑∏Î¶∞ Î∞∞Ìè¨ Ïã§Ìå®: {e}")
            raise

    async def _canary_deploy(
        self,
        deployment_manifest: Dict[str, Any],
        service_manifest: Dict[str, Any],
        namespace: str
    ) -> Dict[str, Any]:
        """Ïπ¥ÎÇòÎ¶¨ Î∞∞Ìè¨"""
        base_name = deployment_manifest["metadata"]["name"]
        canary_name = f"{base_name}-canary"

        try:
            # Ïπ¥ÎÇòÎ¶¨ Î∞∞Ìè¨ ÏÉùÏÑ± (10% Ìä∏ÎûòÌîΩ)
            canary_manifest = deployment_manifest.copy()
            canary_manifest["metadata"]["name"] = canary_name
            canary_manifest["spec"]["replicas"] = max(1, deployment_manifest["spec"]["replicas"] // 10)
            canary_manifest["spec"]["selector"]["matchLabels"]["deployment"] = "canary"
            canary_manifest["spec"]["template"]["metadata"]["labels"]["deployment"] = "canary"

            self.apps_v1.create_namespaced_deployment(
                namespace=namespace,
                body=canary_manifest
            )

            await self._wait_for_deployment_ready(canary_name, namespace)

            # Ïπ¥ÎÇòÎ¶¨ Î™®ÎãàÌÑ∞ÎßÅ (Ïã§Ï†úÎ°úÎäî Î©îÌä∏Î¶≠ ÏàòÏßë)
            await asyncio.sleep(60)  # 1Î∂ÑÍ∞Ñ Î™®ÎãàÌÑ∞ÎßÅ

            # ÏÑ±Í≥µ Ïãú Ï†ÑÏ≤¥ Î∞∞Ìè¨Î°ú ÌôïÏû•
            canary_manifest["spec"]["replicas"] = deployment_manifest["spec"]["replicas"]
            self.apps_v1.patch_namespaced_deployment(
                name=canary_name,
                namespace=namespace,
                body=canary_manifest
            )

            # Í∏∞Ï°¥ Î∞∞Ìè¨ Ï†úÍ±∞
            try:
                self.apps_v1.delete_namespaced_deployment(
                    name=base_name,
                    namespace=namespace
                )
            except client.ApiException:
                pass

            logger.info(f"Ïπ¥ÎÇòÎ¶¨ Î∞∞Ìè¨ ÏôÑÎ£å: {canary_name}")
            return {"status": "success", "deployment_name": canary_name, "strategy": "canary"}

        except Exception as e:
            logger.error(f"Ïπ¥ÎÇòÎ¶¨ Î∞∞Ìè¨ Ïã§Ìå®: {e}")
            # Ïã§Ìå® Ïãú Ïπ¥ÎÇòÎ¶¨ Ï†úÍ±∞
            try:
                self.apps_v1.delete_namespaced_deployment(
                    name=canary_name,
                    namespace=namespace
                )
            except:
                pass
            raise

    async def _wait_for_deployment_ready(self, name: str, namespace: str, timeout: int = 600):
        """Î∞∞Ìè¨ Ï§ÄÎπÑ ÎåÄÍ∏∞"""
        start_time = datetime.now()

        while (datetime.now() - start_time).total_seconds() < timeout:
            try:
                deployment = self.apps_v1.read_namespaced_deployment(name=name, namespace=namespace)

                if deployment.status.ready_replicas == deployment.spec.replicas:
                    logger.info(f"Î∞∞Ìè¨ Ï§ÄÎπÑ ÏôÑÎ£å: {name}")
                    return True

                await asyncio.sleep(5)

            except Exception as e:
                logger.warning(f"Î∞∞Ìè¨ ÏÉÅÌÉú ÌôïÏù∏ Ï§ë Ïò§Î•ò: {e}")
                await asyncio.sleep(5)

        raise TimeoutError(f"Î∞∞Ìè¨ Ï§ÄÎπÑ ÎåÄÍ∏∞ ÏãúÍ∞Ñ Ï¥àÍ≥º: {name}")

    async def _perform_health_check(self, deployment_name: str, namespace: str) -> bool:
        """Ìó¨Ïä§Ï≤¥ÌÅ¨ ÏàòÌñâ"""
        import aiohttp

        try:
            # ÏÑúÎπÑÏä§ ÏóîÎìúÌè¨Ïù∏Ìä∏ Í∞ÄÏ†∏Ïò§Í∏∞
            service_name = f"{deployment_name}-service"

            # Kubernetes ÏÑúÎπÑÏä§ Ï†ïÎ≥¥ Ï°∞Ìöå
            service_endpoint = await self._get_service_endpoint(service_name, namespace)

            # Ïã§Ï†ú HTTP Ìó¨Ïä§Ï≤¥ÌÅ¨ ÏöîÏ≤≠
            health_url = f"http://{service_endpoint}/health"

            async with aiohttp.ClientSession() as session:
                try:
                    async with session.get(health_url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                        if response.status == 200:
                            health_data = await response.json()

                            # Ìó¨Ïä§Ï≤¥ÌÅ¨ ÏÉÅÏÑ∏ Í≤ÄÏ¶ù
                            if health_data.get('status') == 'healthy':
                                # ÏÑ∏Î∂Ä Ïª¥Ìè¨ÎÑåÌä∏ Ï≤¥ÌÅ¨
                                components = health_data.get('components', {})
                                all_healthy = all(
                                    comp.get('status') == 'healthy'
                                    for comp in components.values()
                                )

                                if all_healthy:
                                    logger.info(f"Ìó¨Ïä§Ï≤¥ÌÅ¨ ÏÑ±Í≥µ: {deployment_name} - Î™®Îì† Ïª¥Ìè¨ÎÑåÌä∏ Ï†ïÏÉÅ")
                                    return True
                                else:
                                    unhealthy = [
                                        name for name, comp in components.items()
                                        if comp.get('status') != 'healthy'
                                    ]
                                    logger.warning(f"ÏùºÎ∂Ä Ïª¥Ìè¨ÎÑåÌä∏ ÎπÑÏ†ïÏÉÅ: {unhealthy}")
                                    return False
                            else:
                                logger.warning(f"ÏÑúÎπÑÏä§ ÏÉÅÌÉú ÎπÑÏ†ïÏÉÅ: {health_data.get('status')}")
                                return False
                        else:
                            logger.error(f"Ìó¨Ïä§Ï≤¥ÌÅ¨ HTTP Ïò§Î•ò: {response.status}")
                            return False

                except aiohttp.ClientTimeout:
                    logger.error(f"Ìó¨Ïä§Ï≤¥ÌÅ¨ ÌÉÄÏûÑÏïÑÏõÉ: {deployment_name}")
                    return False
                except aiohttp.ClientError as e:
                    logger.error(f"Ìó¨Ïä§Ï≤¥ÌÅ¨ Ïó∞Í≤∞ Ïò§Î•ò: {e}")
                    return False

        except Exception as e:
            logger.error(f"Ìó¨Ïä§Ï≤¥ÌÅ¨ Ïã§Ìå®: {e}")
            return False

    async def _get_service_endpoint(self, service_name: str, namespace: str) -> str:
        """Kubernetes ÏÑúÎπÑÏä§ ÏóîÎìúÌè¨Ïù∏Ìä∏ Ï°∞Ìöå"""
        try:
            # kubectlÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏÑúÎπÑÏä§ Ï†ïÎ≥¥ Ï°∞Ìöå
            cmd = f"kubectl get service {service_name} -n {namespace} -o json"
            result = await asyncio.create_subprocess_shell(
                cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await result.communicate()

            if result.returncode == 0:
                import json
                service_info = json.loads(stdout)

                # ClusterIP ÎòêÎäî LoadBalancer IP Ï∂îÏ∂ú
                cluster_ip = service_info['spec'].get('clusterIP')
                port = service_info['spec']['ports'][0]['port']

                # LoadBalancer ÌÉÄÏûÖÏù∏ Í≤ΩÏö∞ Ïô∏Î∂Ä IP ÏÇ¨Ïö©
                if service_info['spec']['type'] == 'LoadBalancer':
                    ingress = service_info.get('status', {}).get('loadBalancer', {}).get('ingress', [])
                    if ingress:
                        external_ip = ingress[0].get('ip') or ingress[0].get('hostname')
                        if external_ip:
                            return f"{external_ip}:{port}"

                # NodePort ÌÉÄÏûÖÏù∏ Í≤ΩÏö∞ ÎÖ∏Îìú IP ÏÇ¨Ïö©
                elif service_info['spec']['type'] == 'NodePort':
                    node_port = service_info['spec']['ports'][0]['nodePort']
                    # Ï≤´ Î≤àÏß∏ ÎÖ∏Îìú IP Ï°∞Ìöå
                    nodes_cmd = "kubectl get nodes -o json"
                    nodes_result = await asyncio.create_subprocess_shell(
                        nodes_cmd,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    nodes_stdout, _ = await nodes_result.communicate()

                    if nodes_result.returncode == 0:
                        nodes_info = json.loads(nodes_stdout)
                        if nodes_info['items']:
                            node_ip = nodes_info['items'][0]['status']['addresses'][0]['address']
                            return f"{node_ip}:{node_port}"

                # Í∏∞Î≥∏: ClusterIP ÏÇ¨Ïö© (ÌÅ¥Îü¨Ïä§ÌÑ∞ ÎÇ¥Î∂ÄÏóêÏÑúÎßå Ï†ëÍ∑º Í∞ÄÎä•)
                return f"{cluster_ip}:{port}"

            else:
                # Î°úÏª¨ Í∞úÎ∞ú ÌôòÍ≤ΩÏóêÏÑúÎäî localhost ÏÇ¨Ïö©
                logger.warning(f"kubectl Ïã§Ìå®, localhost ÏÇ¨Ïö©: {stderr.decode()}")
                return "localhost:8000"

        except Exception as e:
            logger.error(f"ÏÑúÎπÑÏä§ ÏóîÎìúÌè¨Ïù∏Ìä∏ Ï°∞Ìöå Ïã§Ìå®: {e}")
            # Í∏∞Î≥∏Í∞í Î∞òÌôò
            return "localhost:8000"


class CloudDeployer:
    """ÌÅ¥ÎùºÏö∞Îìú Î∞∞Ìè¨Ïûê"""

    def __init__(self, cloud_provider: str = "aws"):
        self.cloud_provider = cloud_provider

        if cloud_provider == "aws":
            self.ec2 = boto3.client('ec2')
            self.ecs = boto3.client('ecs')
            self.ecr = boto3.client('ecr')
            self.s3 = boto3.client('s3')
            self.cloudformation = boto3.client('cloudformation')

    async def deploy_to_ecs(
        self,
        cluster_name: str,
        service_name: str,
        task_definition: Dict[str, Any],
        desired_count: int = 1
    ) -> Dict[str, Any]:
        """AWS ECSÏóê Î∞∞Ìè¨"""
        try:
            # ÌÉúÏä§ÌÅ¨ Ï†ïÏùò Îì±Î°ù
            response = self.ecs.register_task_definition(**task_definition)
            task_def_arn = response['taskDefinition']['taskDefinitionArn']

            # ÏÑúÎπÑÏä§ ÏóÖÎç∞Ïù¥Ìä∏ ÎòêÎäî ÏÉùÏÑ±
            try:
                self.ecs.update_service(
                    cluster=cluster_name,
                    service=service_name,
                    taskDefinition=task_def_arn,
                    desiredCount=desired_count
                )
                logger.info(f"ECS ÏÑúÎπÑÏä§ ÏóÖÎç∞Ïù¥Ìä∏Îê®: {service_name}")
            except self.ecs.exceptions.ServiceNotFoundException:
                self.ecs.create_service(
                    cluster=cluster_name,
                    serviceName=service_name,
                    taskDefinition=task_def_arn,
                    desiredCount=desired_count
                )
                logger.info(f"ÏÉà ECS ÏÑúÎπÑÏä§ ÏÉùÏÑ±Îê®: {service_name}")

            # Î∞∞Ìè¨ ÏôÑÎ£å ÎåÄÍ∏∞
            await self._wait_for_ecs_deployment(cluster_name, service_name)

            return {
                "status": "success",
                "cluster": cluster_name,
                "service": service_name,
                "task_definition": task_def_arn
            }

        except Exception as e:
            logger.error(f"ECS Î∞∞Ìè¨ Ïã§Ìå®: {e}")
            raise

    async def _wait_for_ecs_deployment(self, cluster_name: str, service_name: str, timeout: int = 600):
        """ECS Î∞∞Ìè¨ ÏôÑÎ£å ÎåÄÍ∏∞"""
        start_time = datetime.now()

        while (datetime.now() - start_time).total_seconds() < timeout:
            try:
                response = self.ecs.describe_services(
                    cluster=cluster_name,
                    services=[service_name]
                )

                service = response['services'][0]
                deployments = service['deployments']

                # PRIMARY Î∞∞Ìè¨Í∞Ä STEADY ÏÉÅÌÉúÏù∏ÏßÄ ÌôïÏù∏
                for deployment in deployments:
                    if deployment['status'] == 'PRIMARY':
                        if deployment['rolloutState'] == 'COMPLETED':
                            logger.info(f"ECS Î∞∞Ìè¨ ÏôÑÎ£å: {service_name}")
                            return True

                await asyncio.sleep(10)

            except Exception as e:
                logger.warning(f"ECS Î∞∞Ìè¨ ÏÉÅÌÉú ÌôïÏù∏ Ï§ë Ïò§Î•ò: {e}")
                await asyncio.sleep(10)

        raise TimeoutError(f"ECS Î∞∞Ìè¨ ÎåÄÍ∏∞ ÏãúÍ∞Ñ Ï¥àÍ≥º: {service_name}")


class DeploymentPipeline:
    """Î∞∞Ìè¨ ÌååÏù¥ÌîÑÎùºÏù∏"""

    def __init__(self):
        self.container_builder = ContainerBuilder()
        self.k8s_deployer = KubernetesDeployer()
        self.cloud_deployer = CloudDeployer()
        self.deployment_history: List[DeploymentResult] = []

    async def execute_pipeline(
        self,
        config: DeploymentConfig,
        pipeline_steps: List[str],
        **kwargs
    ) -> DeploymentResult:
        """Î∞∞Ìè¨ ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ"""
        deployment_id = f"deploy-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        start_time = datetime.now()

        result = DeploymentResult(
            deployment_id=deployment_id,
            status=DeploymentStatus.IN_PROGRESS,
            environment=config.environment.value,
            strategy=config.strategy.value,
            start_time=start_time
        )

        try:
            logger.info(f"Î∞∞Ìè¨ ÌååÏù¥ÌîÑÎùºÏù∏ ÏãúÏûë: {deployment_id}")

            for step in pipeline_steps:
                logger.info(f"ÌååÏù¥ÌîÑÎùºÏù∏ Îã®Í≥Ñ Ïã§Ìñâ: {step}")

                if step == "build":
                    await self._build_step(config, result, **kwargs)
                elif step == "test":
                    await self._test_step(config, result, **kwargs)
                elif step == "security_scan":
                    await self._security_scan_step(config, result, **kwargs)
                elif step == "deploy":
                    await self._deploy_step(config, result, **kwargs)
                elif step == "health_check":
                    await self._health_check_step(config, result, **kwargs)
                elif step == "smoke_test":
                    await self._smoke_test_step(config, result, **kwargs)
                elif step == "rollback_check":
                    await self._rollback_check_step(config, result, **kwargs)

            # ÏÑ±Í≥µ ÏôÑÎ£å
            result.status = DeploymentStatus.SUCCESS
            result.end_time = datetime.now()
            result.duration = (result.end_time - result.start_time).total_seconds()

            logger.info(f"Î∞∞Ìè¨ ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÎ£å: {deployment_id}")

        except Exception as e:
            # Ïã§Ìå® Ï≤òÎ¶¨
            result.status = DeploymentStatus.FAILED
            result.error_message = str(e)
            result.end_time = datetime.now()
            result.duration = (result.end_time - result.start_time).total_seconds()

            logger.error(f"Î∞∞Ìè¨ ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìå®: {deployment_id} - {e}")

            # Î°§Î∞± ÏàòÌñâ
            if config.rollback_on_failure:
                try:
                    await self._perform_rollback(config, result)
                    result.rollback_performed = True
                except Exception as rollback_error:
                    logger.error(f"Î°§Î∞± Ïã§Ìå®: {rollback_error}")

        finally:
            self.deployment_history.append(result)

        return result

    async def _build_step(self, config: DeploymentConfig, result: DeploymentResult, **kwargs):
        """ÎπåÎìú Îã®Í≥Ñ"""
        dockerfile_path = kwargs.get("dockerfile_path", "Dockerfile")
        context_path = kwargs.get("context_path", ".")

        image_name = await self.container_builder.build_image(
            dockerfile_path=dockerfile_path,
            image_name="fragrance-ai",
            tag=config.image_tag,
            context_path=context_path
        )

        result.logs.append(f"Ïù¥ÎØ∏ÏßÄ ÎπåÎìú ÏôÑÎ£å: {image_name}")

        # Î†àÏßÄÏä§Ìä∏Î¶¨Ïóê Ìë∏Ïãú
        registry_url = kwargs.get("registry_url")
        if registry_url:
            success = await self.container_builder.push_image(image_name, registry_url)
            if not success:
                raise Exception("Ïù¥ÎØ∏ÏßÄ Ìë∏Ïãú Ïã§Ìå®")
            result.logs.append(f"Ïù¥ÎØ∏ÏßÄ Ìë∏Ïãú ÏôÑÎ£å: {registry_url}/{image_name}")

    async def _test_step(self, config: DeploymentConfig, result: DeploymentResult, **kwargs):
        """ÌÖåÏä§Ìä∏ Îã®Í≥Ñ"""
        test_command = kwargs.get("test_command", "pytest tests/")

        try:
            # ÌÖåÏä§Ìä∏ Ïã§Ìñâ
            process = await asyncio.create_subprocess_shell(
                test_command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            stdout, stderr = await process.communicate()

            if process.returncode != 0:
                raise Exception(f"ÌÖåÏä§Ìä∏ Ïã§Ìå®: {stderr.decode()}")

            result.logs.append("Î™®Îì† ÌÖåÏä§Ìä∏ ÌÜµÍ≥º")

        except Exception as e:
            result.logs.append(f"ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
            raise

    async def _security_scan_step(self, config: DeploymentConfig, result: DeploymentResult, **kwargs):
        """Î≥¥Ïïà Ïä§Ï∫î Îã®Í≥Ñ"""
        try:
            # TrivyÎ•º ÏÇ¨Ïö©Ìïú Ïª®ÌÖåÏù¥ÎÑà Ïù¥ÎØ∏ÏßÄ Î≥¥Ïïà Ïä§Ï∫î
            scan_cmd = f"trivy image --severity HIGH,CRITICAL --format json {config.image_tag}"

            scan_result = await asyncio.create_subprocess_shell(
                scan_cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await scan_result.communicate()

            if scan_result.returncode == 0:
                import json
                scan_data = json.loads(stdout) if stdout else {}

                vulnerabilities = []
                for target in scan_data.get('Results', []):
                    for vuln in target.get('Vulnerabilities', []):
                        if vuln['Severity'] in ['HIGH', 'CRITICAL']:
                            vulnerabilities.append({
                                'id': vuln.get('VulnerabilityID'),
                                'severity': vuln.get('Severity'),
                                'package': vuln.get('PkgName'),
                                'title': vuln.get('Title', 'No title')
                            })

                vulnerabilities_found = len(vulnerabilities)

                if vulnerabilities_found > 0:
                    if any(v['severity'] == 'CRITICAL' for v in vulnerabilities):
                        # CRITICAL Ï∑®ÏïΩÏ†êÏù¥ ÏûàÏúºÎ©¥ Î∞∞Ìè¨ Ï§ëÎã®
                        critical_vulns = [v for v in vulnerabilities if v['severity'] == 'CRITICAL']
                        raise Exception(
                            f"CRITICAL Î≥¥Ïïà Ï∑®ÏïΩÏ†ê Î∞úÍ≤¨ ({len(critical_vulns)}Í∞ú): "
                            f"{', '.join([v['id'] for v in critical_vulns[:3]])}"
                        )
                    else:
                        # HIGH Ï∑®ÏïΩÏ†êÎßå ÏûàÏúºÎ©¥ Í≤ΩÍ≥†Îßå
                        result.logs.append(f"HIGH Î≥¥Ïïà Ï∑®ÏïΩÏ†ê Î∞úÍ≤¨ ({vulnerabilities_found}Í∞ú)")
                        result.security_scan_results = vulnerabilities
                else:
                    result.logs.append("Î≥¥Ïïà Ï∑®ÏïΩÏ†ê ÏóÜÏùå")

            else:
                # TrivyÍ∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ Grype ÏÇ¨Ïö©
                logger.warning("Trivy Ïã§Ìñâ Ïã§Ìå®, GrypeÎ°ú ÎåÄÏ≤¥ ÏãúÎèÑ")

                grype_cmd = f"grype {config.image_tag} -o json --fail-on high"
                grype_result = await asyncio.create_subprocess_shell(
                    grype_cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                stdout, stderr = await grype_result.communicate()

                if grype_result.returncode != 0 and grype_result.returncode != 1:
                    # returncode 1ÏùÄ Ï∑®ÏïΩÏ†ê Î∞úÍ≤¨, Í∑∏ Ïô∏Îäî Ïò§Î•ò
                    logger.warning(f"Î≥¥Ïïà Ïä§Ï∫î ÎèÑÍµ¨ Ïã§Ìñâ Ïã§Ìå®: {stderr.decode()}")
                    result.logs.append("Î≥¥Ïïà Ïä§Ï∫î Ïä§ÌÇµ (ÎèÑÍµ¨ ÏóÜÏùå)")
                elif grype_result.returncode == 1:
                    result.logs.append("HIGH Ïù¥ÏÉÅ Î≥¥Ïïà Ï∑®ÏïΩÏ†ê Î∞úÍ≤¨ - Í≤ÄÌÜ† ÌïÑÏöî")

        except FileNotFoundError:
            # Ïä§Ï∫î ÎèÑÍµ¨Í∞Ä ÏóÜÎäî Í≤ΩÏö∞
            logger.warning("Î≥¥Ïïà Ïä§Ï∫î ÎèÑÍµ¨Í∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏùå")
            result.logs.append("Î≥¥Ïïà Ïä§Ï∫î Ïä§ÌÇµ (ÎèÑÍµ¨ ÎØ∏ÏÑ§Ïπò)")

        except Exception as e:
            logger.error(f"Î≥¥Ïïà Ïä§Ï∫î Ïò§Î•ò: {e}")
            if "CRITICAL" in str(e):
                raise  # CRITICAL Ï∑®ÏïΩÏ†êÏùÄ Î∞∞Ìè¨ Ï§ëÎã®
            else:
                result.logs.append(f"Î≥¥Ïïà Ïä§Ï∫î Í≤ΩÍ≥†: {e}")

        result.logs.append("Î≥¥Ïïà Ïä§Ï∫î ÏôÑÎ£å")

    async def _deploy_step(self, config: DeploymentConfig, result: DeploymentResult, **kwargs):
        """Î∞∞Ìè¨ Îã®Í≥Ñ"""
        deployment_target = kwargs.get("deployment_target", "kubernetes")
        namespace = kwargs.get("namespace", "default")

        if deployment_target == "kubernetes":
            deploy_result = await self.k8s_deployer.deploy_application(config, namespace)
            result.logs.append(f"Kubernetes Î∞∞Ìè¨ ÏôÑÎ£å: {deploy_result}")

        elif deployment_target == "ecs":
            cluster_name = kwargs.get("cluster_name", "fragrance-ai-cluster")
            service_name = kwargs.get("service_name", "fragrance-ai-service")
            task_definition = kwargs.get("task_definition", {})

            deploy_result = await self.cloud_deployer.deploy_to_ecs(
                cluster_name, service_name, task_definition
            )
            result.logs.append(f"ECS Î∞∞Ìè¨ ÏôÑÎ£å: {deploy_result}")

        else:
            raise Exception(f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Î∞∞Ìè¨ ÎåÄÏÉÅ: {deployment_target}")

    async def _health_check_step(self, config: DeploymentConfig, result: DeploymentResult, **kwargs):
        """Ìó¨Ïä§Ï≤¥ÌÅ¨ Îã®Í≥Ñ"""
        health_check_url = kwargs.get("health_check_url", "http://localhost:8000/health")
        max_retries = kwargs.get("max_retries", 10)
        retry_delay = kwargs.get("retry_delay", 30)

        for attempt in range(max_retries):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(health_check_url, timeout=10) as response:
                        if response.status == 200:
                            result.logs.append("Ìó¨Ïä§Ï≤¥ÌÅ¨ ÏÑ±Í≥µ")
                            return

                        logger.warning(f"Ìó¨Ïä§Ï≤¥ÌÅ¨ Ïã§Ìå® (ÏãúÎèÑ {attempt + 1}): HTTP {response.status}")

            except Exception as e:
                logger.warning(f"Ìó¨Ïä§Ï≤¥ÌÅ¨ Ïò§Î•ò (ÏãúÎèÑ {attempt + 1}): {e}")

            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)

        raise Exception("Ìó¨Ïä§Ï≤¥ÌÅ¨ Ïã§Ìå®")

    async def _smoke_test_step(self, config: DeploymentConfig, result: DeploymentResult, **kwargs):
        """Ïä§Î™®ÌÅ¨ ÌÖåÏä§Ìä∏ Îã®Í≥Ñ"""
        api_endpoint = kwargs.get("api_endpoint", "http://localhost:8000/api/v1")

        try:
            async with aiohttp.ClientSession() as session:
                # Í∏∞Î≥∏ API ÏóîÎìúÌè¨Ïù∏Ìä∏ ÌÖåÏä§Ìä∏
                test_endpoints = [
                    f"{api_endpoint}/health",
                    f"{api_endpoint}/search/semantic",
                    f"{api_endpoint}/generate/recipe"
                ]

                for endpoint in test_endpoints:
                    async with session.get(endpoint) as response:
                        if response.status not in [200, 404]:  # 404Îäî Ïù∏Ï¶ù ÏóÜÏù¥Îäî Ï†ïÏÉÅ
                            logger.warning(f"Ïä§Î™®ÌÅ¨ ÌÖåÏä§Ìä∏ Í≤ΩÍ≥†: {endpoint} - HTTP {response.status}")

            result.logs.append("Ïä§Î™®ÌÅ¨ ÌÖåÏä§Ìä∏ ÏôÑÎ£å")

        except Exception as e:
            result.logs.append(f"Ïä§Î™®ÌÅ¨ ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
            raise

    async def _rollback_check_step(self, config: DeploymentConfig, result: DeploymentResult, **kwargs):
        """Î°§Î∞± ÌôïÏù∏ Îã®Í≥Ñ"""
        # Î∞∞Ìè¨ ÌõÑ Î©îÌä∏Î¶≠ Î™®ÎãàÌÑ∞ÎßÅ
        monitoring_duration = kwargs.get("monitoring_duration", 300)  # 5Î∂Ñ
        error_threshold = kwargs.get("error_threshold", 0.05)  # 5%

        logger.info(f"Î∞∞Ìè¨ ÌõÑ Î™®ÎãàÌÑ∞ÎßÅ ÏãúÏûë: {monitoring_duration}Ï¥à")

        # Ïã§Ï†ú Î©îÌä∏Î¶≠ ÏàòÏßë
        metrics = await self._collect_real_metrics(config, monitoring_duration)

        error_rate = metrics['error_rate']
        response_time_p95 = metrics['response_time_p95']

        if error_rate > error_threshold:
            raise Exception(f"Ïò§Î•òÏú®Ïù¥ ÏûÑÍ≥ÑÍ∞íÏùÑ Ï¥àÍ≥ºÌñàÏäµÎãàÎã§: {error_rate:.2%} > {error_threshold:.2%}")

        if response_time_p95 > 2000:  # 2Ï¥à Ï¥àÍ≥º
            logger.warning(f"ÏùëÎãµÏãúÍ∞ÑÏù¥ ÎÜíÏäµÎãàÎã§: {response_time_p95}ms")

        result.success_rate = (1 - error_rate) * 100
        result.metrics = {
            "error_rate": error_rate,
            "response_time_p95": response_time_p95
        }

        result.logs.append(f"Î™®ÎãàÌÑ∞ÎßÅ ÏôÑÎ£å - ÏÑ±Í≥µÎ•†: {result.success_rate:.2f}%")

    async def _perform_rollback(self, config: DeploymentConfig, result: DeploymentResult):
        """Î°§Î∞± ÏàòÌñâ"""
        logger.info("Î°§Î∞± ÏãúÏûë")

        # Ïã§Ï†ú Î°§Î∞± Íµ¨ÌòÑ
        if len(self.deployment_history) > 1:
            previous_deployment = self.deployment_history[-2]
            if previous_deployment.status == DeploymentStatus.SUCCESS:
                # Ïù¥Ï†Ñ ÏÑ±Í≥µ Î≤ÑÏ†ÑÏúºÎ°ú Î°§Î∞±
                await self._execute_rollback(config, previous_deployment)
                logger.info(f"Ïù¥Ï†Ñ Î≤ÑÏ†ÑÏúºÎ°ú Î°§Î∞±: {previous_deployment.deployment_id}")
                result.logs.append(f"Î°§Î∞± ÏôÑÎ£å: {previous_deployment.deployment_id}")
            else:
                raise Exception("Î°§Î∞±Ìï† ÏïàÏ†ïÏ†ÅÏù∏ Î≤ÑÏ†ÑÏù¥ ÏóÜÏäµÎãàÎã§")
        else:
            raise Exception("Î°§Î∞±Ìï† Ïù¥Ï†Ñ Î≤ÑÏ†ÑÏù¥ ÏóÜÏäµÎãàÎã§")

    def get_deployment_status(self, deployment_id: str) -> Optional[DeploymentResult]:
        """Î∞∞Ìè¨ ÏÉÅÌÉú Ï°∞Ìöå"""
        for deployment in self.deployment_history:
            if deployment.deployment_id == deployment_id:
                return deployment
        return None

    async def _collect_real_metrics(self, config: DeploymentConfig, duration: int) -> Dict[str, Any]:
        """Ïã§Ï†ú Î©îÌä∏Î¶≠ ÏàòÏßë"""
        metrics = {
            'error_rate': 0.0,
            'response_time_p95': 0.0,
            'cpu_usage': 0.0,
            'memory_usage': 0.0,
            'request_rate': 0.0
        }

        # Prometheus Î©îÌä∏Î¶≠ ÏàòÏßë
        if config.monitoring_config.get('prometheus_url'):
            metrics.update(await self._collect_prometheus_metrics(
                config.monitoring_config['prometheus_url'],
                duration
            ))

        # CloudWatch Î©îÌä∏Î¶≠ ÏàòÏßë (AWS)
        elif config.monitoring_config.get('cloudwatch_enabled'):
            metrics.update(await self._collect_cloudwatch_metrics(
                config.monitoring_config,
                duration
            ))

        # Datadog Î©îÌä∏Î¶≠ ÏàòÏßë
        elif config.monitoring_config.get('datadog_api_key'):
            metrics.update(await self._collect_datadog_metrics(
                config.monitoring_config,
                duration
            ))

        # Î°úÏª¨ Ìó¨Ïä§Ï≤¥ÌÅ¨ Ìè¥Î∞±
        else:
            metrics.update(await self._collect_health_check_metrics(
                config.health_check_url,
                duration
            ))

        return metrics

    async def _collect_prometheus_metrics(self, prometheus_url: str, duration: int) -> Dict[str, Any]:
        """PrometheusÏóêÏÑú Î©îÌä∏Î¶≠ ÏàòÏßë"""
        import aiohttp
        import statistics

        metrics = {}
        end_time = datetime.now()
        start_time = end_time - timedelta(seconds=duration)

        queries = {
            'error_rate': 'rate(http_requests_total{status=~"5.."}[5m])/rate(http_requests_total[5m])',
            'response_time_p95': 'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))',
            'cpu_usage': 'avg(rate(container_cpu_usage_seconds_total[5m]))',
            'memory_usage': 'avg(container_memory_usage_bytes)',
            'request_rate': 'rate(http_requests_total[5m])'
        }

        async with aiohttp.ClientSession() as session:
            for metric_name, query in queries.items():
                try:
                    async with session.get(
                        f"{prometheus_url}/api/v1/query",
                        params={'query': query}
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            if data['status'] == 'success' and data['data']['result']:
                                value = float(data['data']['result'][0]['value'][1])
                                metrics[metric_name] = value
                except Exception as e:
                    logger.warning(f"Failed to collect {metric_name} from Prometheus: {e}")
                    metrics[metric_name] = 0.0

        return metrics

    async def _collect_cloudwatch_metrics(self, config: Dict, duration: int) -> Dict[str, Any]:
        """AWS CloudWatchÏóêÏÑú Î©îÌä∏Î¶≠ ÏàòÏßë"""
        try:
            import boto3
            from datetime import datetime, timedelta

            cloudwatch = boto3.client(
                'cloudwatch',
                region_name=config.get('aws_region', 'us-east-1'),
                aws_access_key_id=config.get('aws_access_key_id'),
                aws_secret_access_key=config.get('aws_secret_access_key')
            )

            end_time = datetime.utcnow()
            start_time = end_time - timedelta(seconds=duration)

            metrics = {}

            # Ïò§Î•òÏú®
            error_stats = cloudwatch.get_metric_statistics(
                Namespace='AWS/ELB',
                MetricName='HTTPCode_Target_5XX_Count',
                Dimensions=[{'Name': 'LoadBalancer', 'Value': config.get('load_balancer_name', '')}],
                StartTime=start_time,
                EndTime=end_time,
                Period=60,
                Statistics=['Sum']
            )

            total_stats = cloudwatch.get_metric_statistics(
                Namespace='AWS/ELB',
                MetricName='RequestCount',
                Dimensions=[{'Name': 'LoadBalancer', 'Value': config.get('load_balancer_name', '')}],
                StartTime=start_time,
                EndTime=end_time,
                Period=60,
                Statistics=['Sum']
            )

            error_count = sum(point['Sum'] for point in error_stats.get('Datapoints', []))
            total_count = sum(point['Sum'] for point in total_stats.get('Datapoints', []))

            metrics['error_rate'] = error_count / total_count if total_count > 0 else 0.0

            # ÏùëÎãµ ÏãúÍ∞Ñ
            response_time_stats = cloudwatch.get_metric_statistics(
                Namespace='AWS/ELB',
                MetricName='TargetResponseTime',
                Dimensions=[{'Name': 'LoadBalancer', 'Value': config.get('load_balancer_name', '')}],
                StartTime=start_time,
                EndTime=end_time,
                Period=60,
                Statistics=['Average']
            )

            if response_time_stats['Datapoints']:
                metrics['response_time_p95'] = max(point['Average'] for point in response_time_stats['Datapoints']) * 1000

            return metrics

        except Exception as e:
            logger.error(f"CloudWatch metrics collection failed: {e}")
            return {'error_rate': 0.0, 'response_time_p95': 0.0}

    async def _collect_datadog_metrics(self, config: Dict, duration: int) -> Dict[str, Any]:
        """DatadogÏóêÏÑú Î©îÌä∏Î¶≠ ÏàòÏßë"""
        import aiohttp

        api_key = config['datadog_api_key']
        app_key = config.get('datadog_app_key')

        headers = {
            'DD-API-KEY': api_key,
            'DD-APPLICATION-KEY': app_key
        } if app_key else {'DD-API-KEY': api_key}

        end_time = int(datetime.now().timestamp())
        start_time = end_time - duration

        metrics = {}

        queries = {
            'error_rate': 'sum:http.requests{status:5*}.as_rate()/sum:http.requests{*}.as_rate()',
            'response_time_p95': 'percentile:http.request.duration{*}:95',
            'cpu_usage': 'avg:system.cpu.user{*}',
            'memory_usage': 'avg:system.mem.used{*}'
        }

        async with aiohttp.ClientSession() as session:
            for metric_name, query in queries.items():
                try:
                    async with session.get(
                        'https://api.datadoghq.com/api/v1/query',
                        params={
                            'from': start_time,
                            'to': end_time,
                            'query': query
                        },
                        headers=headers
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            if data['series']:
                                points = data['series'][0]['pointlist']
                                if points:
                                    metrics[metric_name] = points[-1][1]  # ÏµúÏã† Í∞í
                except Exception as e:
                    logger.warning(f"Failed to collect {metric_name} from Datadog: {e}")
                    metrics[metric_name] = 0.0

        return metrics

    async def _collect_health_check_metrics(self, health_url: str, duration: int) -> Dict[str, Any]:
        """Ìó¨Ïä§Ï≤¥ÌÅ¨ ÏóîÎìúÌè¨Ïù∏Ìä∏ÏóêÏÑú Î©îÌä∏Î¶≠ ÏàòÏßë"""
        import aiohttp
        import statistics

        metrics = {
            'error_rate': 0.0,
            'response_time_p95': 0.0,
            'cpu_usage': 0.0,
            'memory_usage': 0.0
        }

        response_times = []
        error_count = 0
        total_count = 0

        # duration ÎèôÏïà Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú Ìó¨Ïä§Ï≤¥ÌÅ¨
        check_interval = min(5, duration // 10)  # 5Ï¥à ÎòêÎäî duration/10 Ï§ë ÏûëÏùÄ Í∞í
        checks = duration // check_interval

        async with aiohttp.ClientSession() as session:
            for _ in range(checks):
                start = datetime.now()
                try:
                    async with session.get(health_url, timeout=10) as resp:
                        response_time = (datetime.now() - start).total_seconds() * 1000
                        response_times.append(response_time)

                        if resp.status >= 500:
                            error_count += 1
                        total_count += 1

                        # Ìó¨Ïä§Ï≤¥ÌÅ¨ ÏùëÎãµÏóêÏÑú Ï∂îÍ∞Ä Î©îÌä∏Î¶≠ Ï∂îÏ∂ú
                        if resp.status == 200:
                            try:
                                data = await resp.json()
                                metrics['cpu_usage'] = data.get('cpu_usage', 0.0)
                                metrics['memory_usage'] = data.get('memory_usage', 0.0)
                            except:
                                pass

                except Exception as e:
                    logger.warning(f"Health check failed: {e}")
                    error_count += 1
                    total_count += 1

                await asyncio.sleep(check_interval)

        if total_count > 0:
            metrics['error_rate'] = error_count / total_count

        if response_times:
            response_times.sort()
            p95_index = int(len(response_times) * 0.95)
            metrics['response_time_p95'] = response_times[min(p95_index, len(response_times) - 1)]

        return metrics

    async def _execute_rollback(self, config: DeploymentConfig, previous_deployment: DeploymentResult):
        """Ïã§Ï†ú Î°§Î∞± Ïã§Ìñâ"""
        rollback_strategy = config.rollback_strategy

        if rollback_strategy == "blue_green":
            # Blue-Green Î°§Î∞±: Ïù¥Ï†Ñ ÌôòÍ≤ΩÏúºÎ°ú Ìä∏ÎûòÌîΩ Ï†ÑÌôò
            await self._switch_traffic_to_previous(config, previous_deployment)

        elif rollback_strategy == "canary":
            # Canary Î°§Î∞±: ÏÉà Î≤ÑÏ†Ñ Ïù∏Ïä§ÌÑ¥Ïä§ Ï†úÍ±∞
            await self._remove_canary_instances(config)

        elif rollback_strategy == "rolling":
            # Rolling Î°§Î∞±: Ïù¥Ï†Ñ Î≤ÑÏ†ÑÏúºÎ°ú ÏàúÏ∞®Ï†Å Ïû¨Î∞∞Ìè¨
            await self._rolling_rollback(config, previous_deployment)

        else:
            # Í∏∞Î≥∏: Ïù¥Ï†Ñ Î≤ÑÏ†Ñ Ïû¨Î∞∞Ìè¨
            await self._redeploy_previous_version(config, previous_deployment)

    async def _switch_traffic_to_previous(self, config: DeploymentConfig, previous: DeploymentResult):
        """Blue-Green: Ïù¥Ï†Ñ ÌôòÍ≤ΩÏúºÎ°ú Ìä∏ÎûòÌîΩ Ï†ÑÌôò"""
        load_balancer_config = config.rollback_config.get('load_balancer')

        if load_balancer_config:
            # AWS ELB, Azure Load Balancer, ÎòêÎäî nginx ÏÑ§Ï†ï Î≥ÄÍ≤Ω
            if load_balancer_config['type'] == 'aws_elb':
                await self._update_aws_elb_target_group(
                    load_balancer_config,
                    previous.deployment_data.get('target_group_arn')
                )
            elif load_balancer_config['type'] == 'nginx':
                await self._update_nginx_upstream(
                    load_balancer_config,
                    previous.deployment_data.get('upstream_servers')
                )

        logger.info(f"Traffic switched to previous deployment: {previous.deployment_id}")

    async def _remove_canary_instances(self, config: DeploymentConfig):
        """Canary Ïù∏Ïä§ÌÑ¥Ïä§ Ï†úÍ±∞"""
        canary_instances = config.rollback_config.get('canary_instances', [])

        for instance_id in canary_instances:
            # Kubernetes, Docker, ÎòêÎäî ÌÅ¥ÎùºÏö∞Îìú Ïù∏Ïä§ÌÑ¥Ïä§ Ï†úÍ±∞
            if config.deployment_type == "kubernetes":
                await self._delete_k8s_deployment(instance_id)
            elif config.deployment_type == "docker":
                await self._stop_docker_container(instance_id)
            elif config.deployment_type == "ec2":
                await self._terminate_ec2_instance(instance_id)

        logger.info(f"Removed {len(canary_instances)} canary instances")

    async def _rolling_rollback(self, config: DeploymentConfig, previous: DeploymentResult):
        """Î°§ÎßÅ Î°§Î∞± Ïã§Ìñâ"""
        instances = config.rollback_config.get('instances', [])
        batch_size = config.rollback_config.get('batch_size', 1)

        for i in range(0, len(instances), batch_size):
            batch = instances[i:i+batch_size]

            for instance_id in batch:
                await self._rollback_instance(instance_id, previous.deployment_data)

            # Î∞∞Ïπò Í∞Ñ ÎåÄÍ∏∞
            await asyncio.sleep(config.rollback_config.get('batch_delay', 30))

            # Ìó¨Ïä§Ï≤¥ÌÅ¨
            await self._verify_instance_health(batch)

        logger.info(f"Rolling rollback completed for {len(instances)} instances")

    async def _redeploy_previous_version(self, config: DeploymentConfig, previous: DeploymentResult):
        """Ïù¥Ï†Ñ Î≤ÑÏ†Ñ Ïû¨Î∞∞Ìè¨"""
        deployment_commands = previous.deployment_data.get('deployment_commands', [])

        for command in deployment_commands:
            # Î∞∞Ìè¨ Î™ÖÎ†π Ïã§Ìñâ
            result = await self._execute_deployment_command(command)
            if not result['success']:
                raise Exception(f"Rollback command failed: {command}")

        logger.info(f"Previous version redeployed: {previous.deployment_id}")

    async def _update_aws_elb_target_group(self, load_balancer_config: Dict, target_group_arn: str):
        """AWS ELB ÌÉÄÍ≤ü Í∑∏Î£π ÏóÖÎç∞Ïù¥Ìä∏"""
        try:
            import boto3
            elbv2 = boto3.client('elbv2',
                                  region_name=load_balancer_config.get('region', 'us-east-1'))

            # ÌòÑÏû¨ ÌÉÄÍ≤ü Í∑∏Î£πÏùò ÌÉÄÍ≤ü Ìï¥Ï†ú
            current_targets = elbv2.describe_target_health(TargetGroupArn=target_group_arn)
            if current_targets['TargetHealthDescriptions']:
                targets_to_deregister = [
                    {'Id': t['Target']['Id']} for t in current_targets['TargetHealthDescriptions']
                ]
                elbv2.deregister_targets(
                    TargetGroupArn=target_group_arn,
                    Targets=targets_to_deregister
                )

            # ÏÉà ÌÉÄÍ≤ü Îì±Î°ù
            new_targets = load_balancer_config.get('new_targets', [])
            if new_targets:
                elbv2.register_targets(
                    TargetGroupArn=target_group_arn,
                    Targets=new_targets
                )

            logger.info(f"AWS ELB ÌÉÄÍ≤ü Í∑∏Î£π ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å: {target_group_arn}")

        except Exception as e:
            logger.error(f"AWS ELB ÏóÖÎç∞Ïù¥Ìä∏ Ïã§Ìå®: {e}")
            raise

    async def _update_nginx_upstream(self, load_balancer_config: Dict, upstream_servers: List[str]):
        """Nginx upstream ÏÑ§Ï†ï ÏóÖÎç∞Ïù¥Ìä∏"""
        import aiohttp

        try:
            nginx_api_url = load_balancer_config.get('nginx_api_url', 'http://nginx-plus-api:8080')
            upstream_name = load_balancer_config.get('upstream_name', 'backend')

            # Nginx Plus APIÎ•º ÏÇ¨Ïö©Ìïú ÎèôÏ†Å upstream ÏóÖÎç∞Ïù¥Ìä∏
            async with aiohttp.ClientSession() as session:
                # ÌòÑÏû¨ ÏÑúÎ≤Ñ Î™©Î°ù Ï°∞Ìöå
                async with session.get(f"{nginx_api_url}/api/6/http/upstreams/{upstream_name}/servers") as resp:
                    if resp.status == 200:
                        current_servers = await resp.json()

                        # Í∏∞Ï°¥ ÏÑúÎ≤Ñ Ï†úÍ±∞
                        for server in current_servers:
                            await session.delete(
                                f"{nginx_api_url}/api/6/http/upstreams/{upstream_name}/servers/{server['id']}"
                            )

                # ÏÉà ÏÑúÎ≤Ñ Ï∂îÍ∞Ä
                for server in upstream_servers:
                    server_config = {
                        "server": server,
                        "weight": 1,
                        "max_fails": 3,
                        "fail_timeout": "10s"
                    }
                    await session.post(
                        f"{nginx_api_url}/api/6/http/upstreams/{upstream_name}/servers",
                        json=server_config
                    )

            logger.info(f"Nginx upstream ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å: {upstream_name}")

        except Exception as e:
            logger.error(f"Nginx upstream ÏóÖÎç∞Ïù¥Ìä∏ Ïã§Ìå®: {e}")
            # Fallback: ÏÑ§Ï†ï ÌååÏùº ÏßÅÏ†ë ÏàòÏ†ï
            await self._update_nginx_config_file(load_balancer_config, upstream_servers)

    async def _update_nginx_config_file(self, load_balancer_config: Dict, upstream_servers: List[str]):
        """Nginx ÏÑ§Ï†ï ÌååÏùº ÏßÅÏ†ë ÏóÖÎç∞Ïù¥Ìä∏"""
        config_path = load_balancer_config.get('config_path', '/etc/nginx/conf.d/upstream.conf')
        upstream_name = load_balancer_config.get('upstream_name', 'backend')

        upstream_config = f"upstream {upstream_name} {{\n"
        for server in upstream_servers:
            upstream_config += f"    server {server};\n"
        upstream_config += "}\n"

        # SSHÎ•º ÌÜµÌïú ÏõêÍ≤© ÌååÏùº ÏóÖÎç∞Ïù¥Ìä∏
        nginx_host = load_balancer_config.get('nginx_host', 'nginx-server')
        cmd = f"ssh {nginx_host} 'echo \"{upstream_config}\" > {config_path} && nginx -s reload'"

        result = await asyncio.create_subprocess_shell(
            cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await result.communicate()

        if result.returncode != 0:
            raise Exception(f"Nginx config update failed: {stderr.decode()}")

    async def _delete_k8s_deployment(self, instance_id: str):
        """Kubernetes ÎîîÌîåÎ°úÏù¥Î®ºÌä∏ ÏÇ≠Ï†ú"""
        try:
            namespace, deployment_name = instance_id.split('/', 1)
            cmd = f"kubectl delete deployment {deployment_name} -n {namespace}"

            result = await asyncio.create_subprocess_shell(
                cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await result.communicate()

            if result.returncode == 0:
                logger.info(f"K8s deployment ÏÇ≠Ï†ú ÏôÑÎ£å: {instance_id}")
            else:
                raise Exception(f"K8s deployment ÏÇ≠Ï†ú Ïã§Ìå®: {stderr.decode()}")

        except Exception as e:
            logger.error(f"K8s deployment ÏÇ≠Ï†ú Ïò§Î•ò: {e}")
            raise

    async def _stop_docker_container(self, instance_id: str):
        """Docker Ïª®ÌÖåÏù¥ÎÑà Ï§ëÏßÄ"""
        try:
            # Î°úÏª¨ ÎòêÎäî ÏõêÍ≤© Docker Îç∞Î™¨Ïóê Ïó∞Í≤∞
            cmd = f"docker stop {instance_id} && docker rm {instance_id}"

            result = await asyncio.create_subprocess_shell(
                cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await result.communicate()

            if result.returncode == 0:
                logger.info(f"Docker container Ï§ëÏßÄ ÏôÑÎ£å: {instance_id}")
            else:
                # Ïù¥ÎØ∏ Ï§ëÏßÄÎêú Ïª®ÌÖåÏù¥ÎÑàÏùº Ïàò ÏûàÏùå
                logger.warning(f"Docker container Ï§ëÏßÄ Í≤ΩÍ≥†: {stderr.decode()}")

        except Exception as e:
            logger.error(f"Docker container Ï§ëÏßÄ Ïò§Î•ò: {e}")
            raise

    async def _terminate_ec2_instance(self, instance_id: str):
        """EC2 Ïù∏Ïä§ÌÑ¥Ïä§ Ï¢ÖÎ£å"""
        try:
            import boto3
            ec2 = boto3.client('ec2')

            # Ïù∏Ïä§ÌÑ¥Ïä§ Ï¢ÖÎ£å
            response = ec2.terminate_instances(InstanceIds=[instance_id])

            # Ï¢ÖÎ£å ÎåÄÍ∏∞
            waiter = ec2.get_waiter('instance_terminated')
            waiter.wait(
                InstanceIds=[instance_id],
                WaiterConfig={
                    'Delay': 5,
                    'MaxAttempts': 60  # ÏµúÎåÄ 5Î∂Ñ ÎåÄÍ∏∞
                }
            )

            logger.info(f"EC2 instance Ï¢ÖÎ£å ÏôÑÎ£å: {instance_id}")

        except Exception as e:
            logger.error(f"EC2 instance Ï¢ÖÎ£å Ïò§Î•ò: {e}")
            raise

    async def _rollback_instance(self, instance_id: str, deployment_data: Dict):
        """Í∞úÎ≥Ñ Ïù∏Ïä§ÌÑ¥Ïä§ Î°§Î∞±"""
        instance_type = deployment_data.get('instance_type', 'kubernetes')

        if instance_type == 'kubernetes':
            # K8s Pod Ïû¨ÏãúÏûë ÎòêÎäî Ïù¥ÎØ∏ÏßÄ Î≥ÄÍ≤Ω
            namespace, pod_name = instance_id.split('/', 1)
            previous_image = deployment_data.get('previous_image')

            cmd = f"kubectl set image deployment/{pod_name} *={previous_image} -n {namespace}"
            await asyncio.create_subprocess_shell(cmd)

        elif instance_type == 'docker':
            # Docker Ïª®ÌÖåÏù¥ÎÑà Ïû¨ÏÉùÏÑ±
            previous_image = deployment_data.get('previous_image')
            container_config = deployment_data.get('container_config', {})

            # Í∏∞Ï°¥ Ïª®ÌÖåÏù¥ÎÑà Ï§ëÏßÄ
            await self._stop_docker_container(instance_id)

            # ÏÉà Ïª®ÌÖåÏù¥ÎÑà ÏãúÏûë
            docker_run_cmd = f"docker run -d --name {instance_id} {previous_image}"
            await asyncio.create_subprocess_shell(docker_run_cmd)

        elif instance_type == 'ec2':
            # EC2 Ïù∏Ïä§ÌÑ¥Ïä§ AMI Î≥ÄÍ≤Ω (ÏÉà Ïù∏Ïä§ÌÑ¥Ïä§ ÏãúÏûë)
            await self._launch_ec2_from_ami(
                instance_id,
                deployment_data.get('previous_ami')
            )

    async def _launch_ec2_from_ami(self, instance_name: str, ami_id: str):
        """AMIÎ°úÎ∂ÄÌÑ∞ EC2 Ïù∏Ïä§ÌÑ¥Ïä§ ÏãúÏûë"""
        try:
            import boto3
            ec2 = boto3.client('ec2')

            # ÏÉà Ïù∏Ïä§ÌÑ¥Ïä§ ÏãúÏûë
            response = ec2.run_instances(
                ImageId=ami_id,
                MinCount=1,
                MaxCount=1,
                InstanceType='t2.micro',  # ÏÑ§Ï†ïÏóêÏÑú Í∞ÄÏ†∏Ïò¨ Ïàò ÏûàÏùå
                TagSpecifications=[
                    {
                        'ResourceType': 'instance',
                        'Tags': [
                            {'Key': 'Name', 'Value': instance_name},
                            {'Key': 'Environment', 'Value': 'rollback'}
                        ]
                    }
                ]
            )

            instance_id = response['Instances'][0]['InstanceId']
            logger.info(f"ÏÉà EC2 Ïù∏Ïä§ÌÑ¥Ïä§ ÏãúÏûë: {instance_id}")
            return instance_id

        except Exception as e:
            logger.error(f"EC2 Ïù∏Ïä§ÌÑ¥Ïä§ ÏãúÏûë Ïã§Ìå®: {e}")
            raise

    async def _verify_instance_health(self, instances: List[str]):
        """Ïù∏Ïä§ÌÑ¥Ïä§ Ìó¨Ïä§ ÌôïÏù∏"""
        health_checks = []

        for instance_id in instances:
            # Í∞Å Ïù∏Ïä§ÌÑ¥Ïä§ ÌÉÄÏûÖÏóê Îî∞Î•∏ Ìó¨Ïä§Ï≤¥ÌÅ¨
            if '/' in instance_id:  # Kubernetes
                health_checks.append(self._check_k8s_pod_health(instance_id))
            elif instance_id.startswith('i-'):  # EC2
                health_checks.append(self._check_ec2_health(instance_id))
            else:  # Docker
                health_checks.append(self._check_docker_health(instance_id))

        results = await asyncio.gather(*health_checks, return_exceptions=True)

        for instance_id, result in zip(instances, results):
            if isinstance(result, Exception):
                logger.error(f"Ìó¨Ïä§Ï≤¥ÌÅ¨ Ïã§Ìå® {instance_id}: {result}")
            elif not result:
                logger.warning(f"Ïù∏Ïä§ÌÑ¥Ïä§ ÎπÑÏ†ïÏÉÅ {instance_id}")

    async def _check_k8s_pod_health(self, pod_id: str) -> bool:
        """Kubernetes Pod Ìó¨Ïä§ Ï≤¥ÌÅ¨"""
        namespace, pod_name = pod_id.split('/', 1)
        cmd = f"kubectl get pod {pod_name} -n {namespace} -o json"

        result = await asyncio.create_subprocess_shell(
            cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await result.communicate()

        if result.returncode == 0:
            import json
            pod_info = json.loads(stdout)
            return pod_info['status']['phase'] == 'Running'
        return False

    async def _check_ec2_health(self, instance_id: str) -> bool:
        """EC2 Ïù∏Ïä§ÌÑ¥Ïä§ Ìó¨Ïä§ Ï≤¥ÌÅ¨"""
        try:
            import boto3
            ec2 = boto3.client('ec2')

            response = ec2.describe_instance_status(InstanceIds=[instance_id])
            if response['InstanceStatuses']:
                status = response['InstanceStatuses'][0]
                return (status['InstanceStatus']['Status'] == 'ok' and
                        status['SystemStatus']['Status'] == 'ok')
            return False
        except:
            return False

    async def _check_docker_health(self, container_id: str) -> bool:
        """Docker Ïª®ÌÖåÏù¥ÎÑà Ìó¨Ïä§ Ï≤¥ÌÅ¨"""
        cmd = f"docker inspect --format='{{{{.State.Health.Status}}}}' {container_id}"

        result = await asyncio.create_subprocess_shell(
            cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await result.communicate()

        if result.returncode == 0:
            health_status = stdout.decode().strip()
            return health_status in ['healthy', 'starting']
        return False

    async def _execute_deployment_command(self, command: Dict) -> Dict[str, Any]:
        """Î∞∞Ìè¨ Î™ÖÎ†π Ïã§Ìñâ"""
        cmd_type = command.get('type')
        cmd_str = command.get('command')

        try:
            if cmd_type == 'shell':
                result = await asyncio.create_subprocess_shell(
                    cmd_str,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                stdout, stderr = await result.communicate()

                return {
                    'success': result.returncode == 0,
                    'stdout': stdout.decode(),
                    'stderr': stderr.decode()
                }

            elif cmd_type == 'kubernetes':
                # kubectl Î™ÖÎ†π Ïã§Ìñâ
                return await self._execute_kubectl_command(cmd_str)

            elif cmd_type == 'docker':
                # docker Î™ÖÎ†π Ïã§Ìñâ
                return await self._execute_docker_command(cmd_str)

            else:
                raise ValueError(f"Unknown command type: {cmd_type}")

        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

    async def _execute_kubectl_command(self, command: str) -> Dict[str, Any]:
        """kubectl Î™ÖÎ†π Ïã§Ìñâ"""
        full_cmd = f"kubectl {command}"
        result = await asyncio.create_subprocess_shell(
            full_cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await result.communicate()

        return {
            'success': result.returncode == 0,
            'stdout': stdout.decode(),
            'stderr': stderr.decode()
        }

    async def _execute_docker_command(self, command: str) -> Dict[str, Any]:
        """docker Î™ÖÎ†π Ïã§Ìñâ"""
        full_cmd = f"docker {command}"
        result = await asyncio.create_subprocess_shell(
            full_cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await result.communicate()

        return {
            'success': result.returncode == 0,
            'stdout': stdout.decode(),
            'stderr': stderr.decode()
        }

    def get_deployment_history(self, limit: int = 10) -> List[DeploymentResult]:
        """Î∞∞Ìè¨ Ïù¥Î†• Ï°∞Ìöå"""
        return self.deployment_history[-limit:]


# Ï†ÑÏó≠ Î∞∞Ìè¨ ÌååÏù¥ÌîÑÎùºÏù∏ Ïù∏Ïä§ÌÑ¥Ïä§
global_pipeline: Optional[DeploymentPipeline] = None


def get_deployment_pipeline() -> DeploymentPipeline:
    """Ï†ÑÏó≠ Î∞∞Ìè¨ ÌååÏù¥ÌîÑÎùºÏù∏ Í∞ÄÏ†∏Ïò§Í∏∞"""
    global global_pipeline
    if global_pipeline is None:
        global_pipeline = DeploymentPipeline()
    return global_pipeline


# Ìé∏Ïùò Ìï®ÏàòÎì§
async def deploy_to_production(
    image_tag: str,
    strategy: DeploymentStrategy = DeploymentStrategy.ROLLING_UPDATE,
    replicas: int = 3
) -> DeploymentResult:
    """ÌîÑÎ°úÎçïÏÖò Î∞∞Ìè¨"""
    config = DeploymentConfig(
        environment=DeploymentEnvironment.PRODUCTION,
        strategy=strategy,
        image_tag=image_tag,
        replicas=replicas,
        resources={
            "requests": {"cpu": "500m", "memory": "1Gi"},
            "limits": {"cpu": "1000m", "memory": "2Gi"}
        }
    )

    pipeline_steps = [
        "build",
        "test",
        "security_scan",
        "deploy",
        "health_check",
        "smoke_test",
        "rollback_check"
    ]

    pipeline = get_deployment_pipeline()
    return await pipeline.execute_pipeline(config, pipeline_steps)


async def deploy_to_staging(image_tag: str) -> DeploymentResult:
    """Ïä§ÌÖåÏù¥Ïßï Î∞∞Ìè¨"""
    config = DeploymentConfig(
        environment=DeploymentEnvironment.STAGING,
        strategy=DeploymentStrategy.RECREATE,
        image_tag=image_tag,
        replicas=1,
        resources={
            "requests": {"cpu": "250m", "memory": "512Mi"},
            "limits": {"cpu": "500m", "memory": "1Gi"}
        }
    )

    pipeline_steps = ["build", "test", "deploy", "health_check"]

    pipeline = get_deployment_pipeline()
    return await pipeline.execute_pipeline(config, pipeline_steps)