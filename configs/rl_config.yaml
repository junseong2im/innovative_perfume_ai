# Reinforcement Learning Configuration
rl:
  # Algorithm selection: "REINFORCE" or "PPO"
  algorithm: "PPO"

  # Common hyperparameters
  state_dim: 20  # Dimension of state space
  action_dim: 12  # Number of possible actions
  learning_rate: 0.0003  # 3e-4

  # REINFORCE specific
  reinforce:
    baseline: false  # Use baseline for variance reduction

  # PPO specific
  ppo:
    gamma: 0.99  # Discount factor
    gae_lambda: 0.95  # GAE lambda for advantage estimation
    clip_epsilon: 0.2  # PPO clipping parameter
    value_coef: 0.5  # Value loss coefficient
    entropy_coef: 0.01  # Entropy bonus coefficient
    update_epochs: 4  # Number of PPO update epochs
    minibatch_size: 64  # Minibatch size for PPO updates
    max_grad_norm: 0.5  # Gradient clipping max norm

  # Training settings
  training:
    max_episodes: 1000
    max_steps_per_episode: 100
    save_frequency: 100  # Save model every N episodes
    log_frequency: 10  # Log metrics every N episodes

  # Model paths
  model:
    save_path: "models/rl_model"
    load_path: "models/rl_model"
    backup_path: "models/backup"

# Genetic Algorithm Configuration
ga:
  # Population settings
  population_size: 100
  generations: 200
  elite_size: 10  # Number of elite individuals to preserve

  # Genetic operators
  crossover:
    probability: 0.9
    type: "SBX"  # Simulated Binary Crossover
    eta_c: 20  # SBX distribution index

  mutation:
    probability: 0.1
    type: "polynomial"  # Polynomial mutation
    eta_m: 20  # Polynomial mutation distribution index
    sigma: 0.2  # Standard deviation for exponential mutation
    min_concentration: 0.1  # Minimum effective concentration

  # Objective functions
  objectives:
    - name: "quality"
      weight: 1.0
      maximize: true
    - name: "cost"
      weight: -1.0
      maximize: false
    - name: "stability"
      weight: 1.0
      maximize: true
    - name: "creativity"
      weight: 0.5
      maximize: true

  # Constraints
  constraints:
    ifra_limit: true  # Apply IFRA safety limits
    concentration_sum: 100  # Total concentration must sum to 100%
    min_ingredients: 3
    max_ingredients: 15

# Creativity Function Settings
creativity:
  # Entropy calculation
  entropy:
    epsilon: 1e-12  # Small value to prevent log(0)
    min_probability: 0.001  # Minimum probability threshold

  # Category balance
  category_weights:
    top_notes: 0.25  # 20-30% ideal
    heart_notes: 0.40  # 30-50% ideal
    base_notes: 0.35  # 30-50% ideal

  # Novelty bonus
  novelty:
    enabled: true
    comparison_pool_size: 50  # Compare against last N formulations
    distance_threshold: 0.3  # Minimum distance for novelty

# Logging Configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "logs/fragrance_ai.log"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  # Metrics to track
  metrics:
    - "loss"
    - "reward"
    - "entropy"
    - "policy_loss"
    - "value_loss"
    - "ga_fitness"
    - "ifra_violations"
    - "diversity"

# Database Configuration
database:
  path: "data/fragrance_production.db"
  backup_frequency: 1000  # Backup every N operations

# Safety and Validation
safety:
  # NaN prevention
  nan_handling:
    check_frequency: 10  # Check for NaN every N steps
    replace_value: 0.0  # Value to replace NaN with

  # Gradient safety
  gradient:
    clip_norm: 1.0
    check_explosion: true  # Check for gradient explosion
    max_norm: 10.0  # Maximum allowed gradient norm

  # Validation
  validation:
    check_concentrations: true  # Ensure sum = 100%
    check_ifra: true  # Validate IFRA limits
    check_positive: true  # Ensure all values are positive